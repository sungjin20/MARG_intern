2025-02-18 05:39:17,359 - Epoch [1/1000], Step [100/4367], Loss: 0.8575
2025-02-18 05:39:43,014 - Epoch [1/1000], Step [200/4367], Loss: 0.8301
2025-02-18 05:40:08,894 - Epoch [1/1000], Step [300/4367], Loss: 0.5882
2025-02-18 05:40:34,897 - Epoch [1/1000], Step [400/4367], Loss: 0.8243
2025-02-18 05:41:01,006 - Epoch [1/1000], Step [500/4367], Loss: 0.8216
2025-02-18 05:41:27,013 - Epoch [1/1000], Step [600/4367], Loss: 0.8639
2025-02-18 05:41:52,991 - Epoch [1/1000], Step [700/4367], Loss: 0.8665
2025-02-18 05:42:19,013 - Epoch [1/1000], Step [800/4367], Loss: 0.5953
2025-02-18 05:42:44,969 - Epoch [1/1000], Step [900/4367], Loss: 0.6489
2025-02-18 05:43:10,924 - Epoch [1/1000], Step [1000/4367], Loss: 0.8222
2025-02-18 05:43:36,843 - Epoch [1/1000], Step [1100/4367], Loss: 0.5702
2025-02-18 05:44:02,786 - Epoch [1/1000], Step [1200/4367], Loss: 0.3837
2025-02-18 05:44:28,949 - Epoch [1/1000], Step [1300/4367], Loss: 0.4508
2025-02-18 05:44:54,932 - Epoch [1/1000], Step [1400/4367], Loss: 0.6667
2025-02-18 05:45:20,909 - Epoch [1/1000], Step [1500/4367], Loss: 0.3660
2025-02-18 05:45:47,154 - Epoch [1/1000], Step [1600/4367], Loss: 0.5248
2025-02-18 05:46:12,872 - Epoch [1/1000], Step [1700/4367], Loss: 0.3599
2025-02-18 05:46:38,937 - Epoch [1/1000], Step [1800/4367], Loss: 0.8283
2025-02-18 05:47:05,118 - Epoch [1/1000], Step [1900/4367], Loss: 0.5424
2025-02-18 05:47:31,462 - Epoch [1/1000], Step [2000/4367], Loss: 0.5513
2025-02-18 05:47:57,473 - Epoch [1/1000], Step [2100/4367], Loss: 0.5377
2025-02-18 05:48:23,549 - Epoch [1/1000], Step [2200/4367], Loss: 0.5951
2025-02-18 05:48:49,502 - Epoch [1/1000], Step [2300/4367], Loss: 0.7091
2025-02-18 05:49:15,565 - Epoch [1/1000], Step [2400/4367], Loss: 0.5707
2025-02-18 05:49:41,599 - Epoch [1/1000], Step [2500/4367], Loss: 0.6141
2025-02-18 05:50:07,595 - Epoch [1/1000], Step [2600/4367], Loss: 0.3852
2025-02-18 05:50:33,403 - Epoch [1/1000], Step [2700/4367], Loss: 0.2545
2025-02-18 05:50:59,617 - Epoch [1/1000], Step [2800/4367], Loss: 0.5347
2025-02-18 05:51:25,730 - Epoch [1/1000], Step [2900/4367], Loss: 0.5557
2025-02-18 05:51:51,884 - Epoch [1/1000], Step [3000/4367], Loss: 0.3585
2025-02-18 05:52:17,736 - Epoch [1/1000], Step [3100/4367], Loss: 0.2940
2025-02-18 05:52:43,955 - Epoch [1/1000], Step [3200/4367], Loss: 0.4126
2025-02-18 05:53:09,771 - Epoch [1/1000], Step [3300/4367], Loss: 0.2823
2025-02-18 05:53:35,929 - Epoch [1/1000], Step [3400/4367], Loss: 0.2906
2025-02-18 05:54:01,694 - Epoch [1/1000], Step [3500/4367], Loss: 0.6197
2025-02-18 05:54:27,687 - Epoch [1/1000], Step [3600/4367], Loss: 0.6456
2025-02-18 05:54:53,816 - Epoch [1/1000], Step [3700/4367], Loss: 0.5431
2025-02-18 05:55:19,797 - Epoch [1/1000], Step [3800/4367], Loss: 0.4978
2025-02-18 05:55:46,057 - Epoch [1/1000], Step [3900/4367], Loss: 0.6617
2025-02-18 05:56:12,309 - Epoch [1/1000], Step [4000/4367], Loss: 0.8134
2025-02-18 05:56:38,371 - Epoch [1/1000], Step [4100/4367], Loss: 0.4633
2025-02-18 05:57:04,561 - Epoch [1/1000], Step [4200/4367], Loss: 0.3546
2025-02-18 05:57:30,593 - Epoch [1/1000], Step [4300/4367], Loss: 0.4075
2025-02-18 05:57:57,943 - Epoch [1/1000], Validation Step [100/1090], Val Loss: 10.9292
2025-02-18 05:58:05,442 - Epoch [1/1000], Validation Step [200/1090], Val Loss: 11.3864
2025-02-18 05:58:13,130 - Epoch [1/1000], Validation Step [300/1090], Val Loss: 12.1697
2025-02-18 05:58:20,849 - Epoch [1/1000], Validation Step [400/1090], Val Loss: 9.7416
2025-02-18 05:58:28,154 - Epoch [1/1000], Validation Step [500/1090], Val Loss: 10.5427
2025-02-18 05:58:35,725 - Epoch [1/1000], Validation Step [600/1090], Val Loss: 11.6784
2025-02-18 05:58:43,362 - Epoch [1/1000], Validation Step [700/1090], Val Loss: 11.1016
2025-02-18 05:58:50,395 - Epoch [1/1000], Validation Step [800/1090], Val Loss: 10.3256
2025-02-18 05:58:57,293 - Epoch [1/1000], Validation Step [900/1090], Val Loss: 10.7193
2025-02-18 05:59:04,697 - Epoch [1/1000], Validation Step [1000/1090], Val Loss: 0.0000
2025-02-18 05:59:11,645 - Epoch 1/1000, Train Loss: 0.5968, Val Loss: 9.7140, Accuracy: 14.64%
2025-02-18 05:59:40,213 - Epoch [2/1000], Step [100/4367], Loss: 0.8511
2025-02-18 06:00:06,376 - Epoch [2/1000], Step [200/4367], Loss: 0.3756
2025-02-18 06:00:32,488 - Epoch [2/1000], Step [300/4367], Loss: 0.5520
2025-02-18 06:00:58,635 - Epoch [2/1000], Step [400/4367], Loss: 0.3845
2025-02-18 06:01:24,390 - Epoch [2/1000], Step [500/4367], Loss: 0.4259
2025-02-18 06:01:50,351 - Epoch [2/1000], Step [600/4367], Loss: 0.4140
2025-02-18 06:02:16,093 - Epoch [2/1000], Step [700/4367], Loss: 0.2773
2025-02-18 06:02:42,009 - Epoch [2/1000], Step [800/4367], Loss: 0.1801
2025-02-18 06:03:08,009 - Epoch [2/1000], Step [900/4367], Loss: 0.5406
2025-02-18 06:03:34,146 - Epoch [2/1000], Step [1000/4367], Loss: 0.5616
2025-02-18 06:04:00,003 - Epoch [2/1000], Step [1100/4367], Loss: 0.2068
2025-02-18 06:04:26,093 - Epoch [2/1000], Step [1200/4367], Loss: 0.5834
2025-02-18 06:04:52,053 - Epoch [2/1000], Step [1300/4367], Loss: 0.7163
2025-02-18 06:05:18,032 - Epoch [2/1000], Step [1400/4367], Loss: 0.4251
2025-02-18 06:05:44,032 - Epoch [2/1000], Step [1500/4367], Loss: 0.6368
2025-02-18 06:06:09,984 - Epoch [2/1000], Step [1600/4367], Loss: 0.6135
2025-02-18 06:06:36,087 - Epoch [2/1000], Step [1700/4367], Loss: 0.5558
2025-02-18 06:07:02,014 - Epoch [2/1000], Step [1800/4367], Loss: 0.3935
2025-02-18 06:07:27,986 - Epoch [2/1000], Step [1900/4367], Loss: 0.4468
2025-02-18 06:07:53,978 - Epoch [2/1000], Step [2000/4367], Loss: 0.3910
2025-02-18 06:08:19,994 - Epoch [2/1000], Step [2100/4367], Loss: 0.4636
2025-02-18 06:08:45,924 - Epoch [2/1000], Step [2200/4367], Loss: 0.3349
2025-02-18 06:09:11,863 - Epoch [2/1000], Step [2300/4367], Loss: 0.5825
2025-02-18 06:09:37,907 - Epoch [2/1000], Step [2400/4367], Loss: 0.5241
2025-02-18 06:10:03,423 - Epoch [2/1000], Step [2500/4367], Loss: 0.2086
2025-02-18 06:10:29,516 - Epoch [2/1000], Step [2600/4367], Loss: 0.6889
2025-02-18 06:10:55,463 - Epoch [2/1000], Step [2700/4367], Loss: 0.3689
2025-02-18 06:11:21,278 - Epoch [2/1000], Step [2800/4367], Loss: 0.4558
2025-02-18 06:11:47,089 - Epoch [2/1000], Step [2900/4367], Loss: 0.2090
2025-02-18 06:12:12,914 - Epoch [2/1000], Step [3000/4367], Loss: 0.3519
2025-02-18 06:12:38,987 - Epoch [2/1000], Step [3100/4367], Loss: 0.6532
2025-02-18 06:13:05,041 - Epoch [2/1000], Step [3200/4367], Loss: 0.4472
2025-02-18 06:13:31,080 - Epoch [2/1000], Step [3300/4367], Loss: 0.8554
2025-02-18 06:13:57,272 - Epoch [2/1000], Step [3400/4367], Loss: 0.5090
2025-02-18 06:14:23,569 - Epoch [2/1000], Step [3500/4367], Loss: 0.2733
2025-02-18 06:14:49,364 - Epoch [2/1000], Step [3600/4367], Loss: 0.5491
2025-02-18 06:15:15,192 - Epoch [2/1000], Step [3700/4367], Loss: 0.7497
2025-02-18 06:15:41,269 - Epoch [2/1000], Step [3800/4367], Loss: 0.3653
2025-02-18 06:16:07,089 - Epoch [2/1000], Step [3900/4367], Loss: 0.4795
2025-02-18 06:16:32,990 - Epoch [2/1000], Step [4000/4367], Loss: 0.2758
2025-02-18 06:16:59,028 - Epoch [2/1000], Step [4100/4367], Loss: 0.4963
2025-02-18 06:17:25,125 - Epoch [2/1000], Step [4200/4367], Loss: 0.5376
2025-02-18 06:17:51,438 - Epoch [2/1000], Step [4300/4367], Loss: 0.4239
2025-02-18 06:18:18,147 - Epoch [2/1000], Validation Step [100/1090], Val Loss: 0.0000
2025-02-18 06:18:25,509 - Epoch [2/1000], Validation Step [200/1090], Val Loss: 0.0000
2025-02-18 06:18:33,006 - Epoch [2/1000], Validation Step [300/1090], Val Loss: 1.6061
2025-02-18 06:18:40,736 - Epoch [2/1000], Validation Step [400/1090], Val Loss: 0.0888
2025-02-18 06:18:47,999 - Epoch [2/1000], Validation Step [500/1090], Val Loss: 0.0618
2025-02-18 06:18:55,615 - Epoch [2/1000], Validation Step [600/1090], Val Loss: 0.7769
2025-02-18 06:19:03,243 - Epoch [2/1000], Validation Step [700/1090], Val Loss: 0.4769
2025-02-18 06:19:10,278 - Epoch [2/1000], Validation Step [800/1090], Val Loss: 0.8063
2025-02-18 06:19:17,135 - Epoch [2/1000], Validation Step [900/1090], Val Loss: 0.7452
2025-02-18 06:19:24,569 - Epoch [2/1000], Validation Step [1000/1090], Val Loss: 1.9830
2025-02-18 06:19:31,493 - Epoch 2/1000, Train Loss: 0.4374, Val Loss: 0.9308, Accuracy: 63.25%
2025-02-18 06:19:32,053 - Model saved at /data2/personal/sungjin/korean_dialects/classification_second_checkpoint/checkpoint_epoch_2.pth
2025-02-18 06:20:00,276 - Epoch [3/1000], Step [100/4367], Loss: 0.5180
2025-02-18 06:20:26,080 - Epoch [3/1000], Step [200/4367], Loss: 0.3139
2025-02-18 06:20:52,150 - Epoch [3/1000], Step [300/4367], Loss: 0.3579
2025-02-18 06:21:17,989 - Epoch [3/1000], Step [400/4367], Loss: 0.5748
2025-02-18 06:21:44,211 - Epoch [3/1000], Step [500/4367], Loss: 0.4432
2025-02-18 06:22:10,263 - Epoch [3/1000], Step [600/4367], Loss: 0.5524
2025-02-18 06:22:36,013 - Epoch [3/1000], Step [700/4367], Loss: 0.4913
2025-02-18 06:23:01,754 - Epoch [3/1000], Step [800/4367], Loss: 0.4942
2025-02-18 06:23:27,605 - Epoch [3/1000], Step [900/4367], Loss: 0.8018
2025-02-18 06:23:53,846 - Epoch [3/1000], Step [1000/4367], Loss: 0.4292
2025-02-18 06:24:19,724 - Epoch [3/1000], Step [1100/4367], Loss: 0.5063
2025-02-18 06:24:45,527 - Epoch [3/1000], Step [1200/4367], Loss: 0.2281
2025-02-18 06:25:11,964 - Epoch [3/1000], Step [1300/4367], Loss: 0.3591
2025-02-18 06:25:38,712 - Epoch [3/1000], Step [1400/4367], Loss: 0.3729
2025-02-18 06:26:05,552 - Epoch [3/1000], Step [1500/4367], Loss: 0.6498
2025-02-18 06:26:31,653 - Epoch [3/1000], Step [1600/4367], Loss: 0.7090
2025-02-18 06:26:57,356 - Epoch [3/1000], Step [1700/4367], Loss: 0.4010
2025-02-18 06:27:23,385 - Epoch [3/1000], Step [1800/4367], Loss: 0.1632
2025-02-18 06:27:49,492 - Epoch [3/1000], Step [1900/4367], Loss: 0.5929
2025-02-18 06:28:15,465 - Epoch [3/1000], Step [2000/4367], Loss: 0.3868
2025-02-18 06:28:41,445 - Epoch [3/1000], Step [2100/4367], Loss: 1.3475
2025-02-18 06:29:07,427 - Epoch [3/1000], Step [2200/4367], Loss: 1.2431
2025-02-18 06:29:33,580 - Epoch [3/1000], Step [2300/4367], Loss: 0.9810
2025-02-18 06:29:59,556 - Epoch [3/1000], Step [2400/4367], Loss: 0.8577
2025-02-18 06:30:25,604 - Epoch [3/1000], Step [2500/4367], Loss: 1.0495
2025-02-18 06:30:51,141 - Epoch [3/1000], Step [2600/4367], Loss: 0.9424
2025-02-18 06:31:16,842 - Epoch [3/1000], Step [2700/4367], Loss: 0.6468
2025-02-18 06:31:42,815 - Epoch [3/1000], Step [2800/4367], Loss: 0.9747
2025-02-18 06:32:08,895 - Epoch [3/1000], Step [2900/4367], Loss: 0.9237
2025-02-18 06:32:35,021 - Epoch [3/1000], Step [3000/4367], Loss: 0.9805
2025-02-18 06:33:00,787 - Epoch [3/1000], Step [3100/4367], Loss: 1.4404
2025-02-18 06:33:26,834 - Epoch [3/1000], Step [3200/4367], Loss: 1.2861
2025-02-18 06:33:52,692 - Epoch [3/1000], Step [3300/4367], Loss: 1.1579
2025-02-18 06:34:18,496 - Epoch [3/1000], Step [3400/4367], Loss: 1.3036
2025-02-18 06:34:44,264 - Epoch [3/1000], Step [3500/4367], Loss: 1.0466
2025-02-18 06:35:10,173 - Epoch [3/1000], Step [3600/4367], Loss: 1.7719
2025-02-18 06:35:35,855 - Epoch [3/1000], Step [3700/4367], Loss: 1.8149
2025-02-18 06:36:01,552 - Epoch [3/1000], Step [3800/4367], Loss: 1.8276
2025-02-18 06:36:27,404 - Epoch [3/1000], Step [3900/4367], Loss: 1.6213
2025-02-18 06:36:53,178 - Epoch [3/1000], Step [4000/4367], Loss: 1.6259
2025-02-18 06:37:18,570 - Epoch [3/1000], Step [4100/4367], Loss: 1.5270
2025-02-18 06:37:44,847 - Epoch [3/1000], Step [4200/4367], Loss: 1.5161
2025-02-18 06:38:10,950 - Epoch [3/1000], Step [4300/4367], Loss: 1.1736
2025-02-18 06:38:37,800 - Epoch [3/1000], Validation Step [100/1090], Val Loss: 0.0098
2025-02-18 06:38:45,153 - Epoch [3/1000], Validation Step [200/1090], Val Loss: 0.0030
2025-02-18 06:38:52,606 - Epoch [3/1000], Validation Step [300/1090], Val Loss: 3.5117
2025-02-18 06:39:00,276 - Epoch [3/1000], Validation Step [400/1090], Val Loss: 1.8710
2025-02-18 06:39:07,513 - Epoch [3/1000], Validation Step [500/1090], Val Loss: 1.5165
2025-02-18 06:39:15,080 - Epoch [3/1000], Validation Step [600/1090], Val Loss: 1.7491
2025-02-18 06:39:22,724 - Epoch [3/1000], Validation Step [700/1090], Val Loss: 1.6601
2025-02-18 06:39:29,755 - Epoch [3/1000], Validation Step [800/1090], Val Loss: 0.6637
2025-02-18 06:39:36,635 - Epoch [3/1000], Validation Step [900/1090], Val Loss: 1.7948
2025-02-18 06:39:44,019 - Epoch [3/1000], Validation Step [1000/1090], Val Loss: 1.5183
2025-02-18 06:39:50,925 - Epoch 3/1000, Train Loss: 0.9116, Val Loss: 1.2988, Accuracy: 49.67%
2025-02-18 06:40:18,831 - Epoch [4/1000], Step [100/4367], Loss: 0.9522
2025-02-18 06:40:45,027 - Epoch [4/1000], Step [200/4367], Loss: 1.2046
2025-02-18 06:41:10,881 - Epoch [4/1000], Step [300/4367], Loss: 1.2685
2025-02-18 06:41:36,975 - Epoch [4/1000], Step [400/4367], Loss: 0.9324
2025-02-18 06:42:02,995 - Epoch [4/1000], Step [500/4367], Loss: 0.9104
2025-02-18 06:42:29,089 - Epoch [4/1000], Step [600/4367], Loss: 0.9809
2025-02-18 06:42:54,973 - Epoch [4/1000], Step [700/4367], Loss: 1.2565
2025-02-18 06:43:21,205 - Epoch [4/1000], Step [800/4367], Loss: 0.9504
2025-02-18 06:43:47,175 - Epoch [4/1000], Step [900/4367], Loss: 1.1892
2025-02-18 06:44:13,501 - Epoch [4/1000], Step [1000/4367], Loss: 1.0541
2025-02-18 06:44:39,261 - Epoch [4/1000], Step [1100/4367], Loss: 1.2098
2025-02-18 06:45:05,256 - Epoch [4/1000], Step [1200/4367], Loss: 1.0713
2025-02-18 06:45:31,294 - Epoch [4/1000], Step [1300/4367], Loss: 0.7561
2025-02-18 06:45:57,463 - Epoch [4/1000], Step [1400/4367], Loss: 0.9439
2025-02-18 06:46:23,284 - Epoch [4/1000], Step [1500/4367], Loss: 0.9338
2025-02-18 06:46:49,403 - Epoch [4/1000], Step [1600/4367], Loss: 1.0969
2025-02-18 06:47:15,463 - Epoch [4/1000], Step [1700/4367], Loss: 1.3914
2025-02-18 06:47:41,702 - Epoch [4/1000], Step [1800/4367], Loss: 1.6255
2025-02-18 06:48:07,604 - Epoch [4/1000], Step [1900/4367], Loss: 1.0417
2025-02-18 06:48:33,224 - Epoch [4/1000], Step [2000/4367], Loss: 1.2620
2025-02-18 06:48:58,915 - Epoch [4/1000], Step [2100/4367], Loss: 1.3493
2025-02-18 06:49:25,065 - Epoch [4/1000], Step [2200/4367], Loss: 1.7619
2025-02-18 06:49:50,905 - Epoch [4/1000], Step [2300/4367], Loss: 1.2115
2025-02-18 06:50:16,807 - Epoch [4/1000], Step [2400/4367], Loss: 1.0718
2025-02-18 06:50:42,677 - Epoch [4/1000], Step [2500/4367], Loss: 1.0882
2025-02-18 06:51:08,440 - Epoch [4/1000], Step [2600/4367], Loss: 1.0064
2025-02-18 06:51:34,297 - Epoch [4/1000], Step [2700/4367], Loss: 1.2217
2025-02-18 06:52:00,322 - Epoch [4/1000], Step [2800/4367], Loss: 1.0930
2025-02-18 06:52:26,205 - Epoch [4/1000], Step [2900/4367], Loss: 1.0647
2025-02-18 06:52:52,166 - Epoch [4/1000], Step [3000/4367], Loss: 1.0794
2025-02-18 06:53:18,078 - Epoch [4/1000], Step [3100/4367], Loss: 0.8503
2025-02-18 06:53:43,902 - Epoch [4/1000], Step [3200/4367], Loss: 0.8660
2025-02-18 06:54:10,087 - Epoch [4/1000], Step [3300/4367], Loss: 0.9048
2025-02-18 06:54:36,022 - Epoch [4/1000], Step [3400/4367], Loss: 0.9709
2025-02-18 06:55:01,797 - Epoch [4/1000], Step [3500/4367], Loss: 0.9976
2025-02-18 06:55:27,724 - Epoch [4/1000], Step [3600/4367], Loss: 0.9561
2025-02-18 06:55:53,656 - Epoch [4/1000], Step [3700/4367], Loss: 0.7669
2025-02-18 06:56:19,619 - Epoch [4/1000], Step [3800/4367], Loss: 0.6634
2025-02-18 06:56:45,597 - Epoch [4/1000], Step [3900/4367], Loss: 0.6173
2025-02-18 06:57:11,621 - Epoch [4/1000], Step [4000/4367], Loss: 0.5990
2025-02-18 06:57:37,248 - Epoch [4/1000], Step [4100/4367], Loss: 0.9919
2025-02-18 06:58:03,342 - Epoch [4/1000], Step [4200/4367], Loss: 0.5764
2025-02-18 06:58:29,331 - Epoch [4/1000], Step [4300/4367], Loss: 1.0054
2025-02-18 06:58:56,013 - Epoch [4/1000], Validation Step [100/1090], Val Loss: 0.0423
2025-02-18 06:59:03,373 - Epoch [4/1000], Validation Step [200/1090], Val Loss: 0.0125
2025-02-18 06:59:10,797 - Epoch [4/1000], Validation Step [300/1090], Val Loss: 0.8517
2025-02-18 06:59:18,457 - Epoch [4/1000], Validation Step [400/1090], Val Loss: 1.5273
2025-02-18 06:59:25,719 - Epoch [4/1000], Validation Step [500/1090], Val Loss: 1.2417
2025-02-18 06:59:33,305 - Epoch [4/1000], Validation Step [600/1090], Val Loss: 0.4193
2025-02-18 06:59:40,960 - Epoch [4/1000], Validation Step [700/1090], Val Loss: 0.3447
2025-02-18 06:59:48,029 - Epoch [4/1000], Validation Step [800/1090], Val Loss: 1.5597
2025-02-18 06:59:54,881 - Epoch [4/1000], Validation Step [900/1090], Val Loss: 1.7661
2025-02-18 07:00:02,259 - Epoch [4/1000], Validation Step [1000/1090], Val Loss: 3.2666
2025-02-18 07:00:09,161 - Epoch 4/1000, Train Loss: 0.9791, Val Loss: 1.1705, Accuracy: 55.81%
2025-02-18 07:00:09,687 - Model saved at /data2/personal/sungjin/korean_dialects/classification_second_checkpoint/checkpoint_epoch_4.pth
2025-02-18 07:00:37,640 - Epoch [5/1000], Step [100/4367], Loss: 0.5491
2025-02-18 07:01:03,556 - Epoch [5/1000], Step [200/4367], Loss: 0.8039
2025-02-18 07:01:29,712 - Epoch [5/1000], Step [300/4367], Loss: 0.6449
2025-02-18 07:01:55,795 - Epoch [5/1000], Step [400/4367], Loss: 0.6245
2025-02-18 07:02:21,952 - Epoch [5/1000], Step [500/4367], Loss: 0.7381
2025-02-18 07:02:47,965 - Epoch [5/1000], Step [600/4367], Loss: 0.7040
2025-02-18 07:03:13,903 - Epoch [5/1000], Step [700/4367], Loss: 0.6942
2025-02-18 07:03:39,747 - Epoch [5/1000], Step [800/4367], Loss: 0.7216
2025-02-18 07:04:05,709 - Epoch [5/1000], Step [900/4367], Loss: 0.4478
2025-02-18 07:04:31,326 - Epoch [5/1000], Step [1000/4367], Loss: 0.8829
2025-02-18 07:04:57,128 - Epoch [5/1000], Step [1100/4367], Loss: 0.4458
2025-02-18 07:05:23,298 - Epoch [5/1000], Step [1200/4367], Loss: 0.6749
2025-02-18 07:05:49,050 - Epoch [5/1000], Step [1300/4367], Loss: 0.4066
2025-02-18 07:06:14,713 - Epoch [5/1000], Step [1400/4367], Loss: 0.6699
2025-02-18 07:06:40,588 - Epoch [5/1000], Step [1500/4367], Loss: 0.7725
2025-02-18 07:07:06,542 - Epoch [5/1000], Step [1600/4367], Loss: 0.5469
2025-02-18 07:07:32,432 - Epoch [5/1000], Step [1700/4367], Loss: 0.9755
2025-02-18 07:07:57,960 - Epoch [5/1000], Step [1800/4367], Loss: 0.7910
2025-02-18 07:08:23,670 - Epoch [5/1000], Step [1900/4367], Loss: 0.7104
2025-02-18 07:08:49,701 - Epoch [5/1000], Step [2000/4367], Loss: 0.2884
2025-02-18 07:09:15,545 - Epoch [5/1000], Step [2100/4367], Loss: 0.5676
2025-02-18 07:09:41,519 - Epoch [5/1000], Step [2200/4367], Loss: 0.6199
2025-02-18 07:10:07,441 - Epoch [5/1000], Step [2300/4367], Loss: 0.5576
2025-02-18 07:10:33,342 - Epoch [5/1000], Step [2400/4367], Loss: 0.5668
2025-02-18 07:10:59,498 - Epoch [5/1000], Step [2500/4367], Loss: 0.6703
2025-02-18 07:11:25,533 - Epoch [5/1000], Step [2600/4367], Loss: 0.5731
2025-02-18 07:11:51,565 - Epoch [5/1000], Step [2700/4367], Loss: 0.2888
2025-02-18 07:12:17,332 - Epoch [5/1000], Step [2800/4367], Loss: 0.6709
2025-02-18 07:12:43,358 - Epoch [5/1000], Step [2900/4367], Loss: 0.4971
2025-02-18 07:13:09,308 - Epoch [5/1000], Step [3000/4367], Loss: 0.2958
2025-02-18 07:13:35,294 - Epoch [5/1000], Step [3100/4367], Loss: 0.7244
2025-02-18 07:14:01,172 - Epoch [5/1000], Step [3200/4367], Loss: 0.4628
2025-02-18 07:14:26,722 - Epoch [5/1000], Step [3300/4367], Loss: 0.5617
2025-02-18 07:14:52,929 - Epoch [5/1000], Step [3400/4367], Loss: 0.8594
2025-02-18 07:15:19,046 - Epoch [5/1000], Step [3500/4367], Loss: 0.1832
2025-02-18 07:15:45,337 - Epoch [5/1000], Step [3600/4367], Loss: 0.3721
2025-02-18 07:16:11,498 - Epoch [5/1000], Step [3700/4367], Loss: 0.2032
2025-02-18 07:16:37,466 - Epoch [5/1000], Step [3800/4367], Loss: 0.2913
2025-02-18 07:17:03,302 - Epoch [5/1000], Step [3900/4367], Loss: 0.6353
2025-02-18 07:17:28,856 - Epoch [5/1000], Step [4000/4367], Loss: 0.2535
2025-02-18 07:17:55,049 - Epoch [5/1000], Step [4100/4367], Loss: 0.5383
2025-02-18 07:18:21,053 - Epoch [5/1000], Step [4200/4367], Loss: 0.3924
2025-02-18 07:18:46,999 - Epoch [5/1000], Step [4300/4367], Loss: 0.4831
2025-02-18 07:19:13,733 - Epoch [5/1000], Validation Step [100/1090], Val Loss: 0.0030
2025-02-18 07:19:21,112 - Epoch [5/1000], Validation Step [200/1090], Val Loss: 0.0972
2025-02-18 07:19:28,598 - Epoch [5/1000], Validation Step [300/1090], Val Loss: 0.6475
2025-02-18 07:19:36,312 - Epoch [5/1000], Validation Step [400/1090], Val Loss: 0.9140
2025-02-18 07:19:43,573 - Epoch [5/1000], Validation Step [500/1090], Val Loss: 1.3998
2025-02-18 07:19:51,162 - Epoch [5/1000], Validation Step [600/1090], Val Loss: 1.3208
2025-02-18 07:19:58,819 - Epoch [5/1000], Validation Step [700/1090], Val Loss: 1.4450
2025-02-18 07:20:05,849 - Epoch [5/1000], Validation Step [800/1090], Val Loss: 1.1139
2025-02-18 07:20:12,704 - Epoch [5/1000], Validation Step [900/1090], Val Loss: 1.4003
2025-02-18 07:20:20,114 - Epoch [5/1000], Validation Step [1000/1090], Val Loss: 0.0023
2025-02-18 07:20:27,029 - Epoch 5/1000, Train Loss: 0.5598, Val Loss: 0.8854, Accuracy: 70.80%
2025-02-18 07:20:54,967 - Epoch [6/1000], Step [100/4367], Loss: 0.4180
2025-02-18 07:21:20,793 - Epoch [6/1000], Step [200/4367], Loss: 0.2871
2025-02-18 07:21:46,412 - Epoch [6/1000], Step [300/4367], Loss: 0.4497
2025-02-18 07:22:12,341 - Epoch [6/1000], Step [400/4367], Loss: 0.4898
2025-02-18 07:22:38,215 - Epoch [6/1000], Step [500/4367], Loss: 0.5092
2025-02-18 07:23:04,404 - Epoch [6/1000], Step [600/4367], Loss: 0.2741
2025-02-18 07:23:30,415 - Epoch [6/1000], Step [700/4367], Loss: 0.3599
2025-02-18 07:23:56,515 - Epoch [6/1000], Step [800/4367], Loss: 0.5619
2025-02-18 07:24:22,602 - Epoch [6/1000], Step [900/4367], Loss: 0.2641
2025-02-18 07:24:48,342 - Epoch [6/1000], Step [1000/4367], Loss: 0.4944
2025-02-18 07:25:14,193 - Epoch [6/1000], Step [1100/4367], Loss: 0.6330
2025-02-18 07:25:40,411 - Epoch [6/1000], Step [1200/4367], Loss: 0.3492
2025-02-18 07:26:06,495 - Epoch [6/1000], Step [1300/4367], Loss: 0.6533
2025-02-18 07:26:32,485 - Epoch [6/1000], Step [1400/4367], Loss: 0.4755
2025-02-18 07:26:58,224 - Epoch [6/1000], Step [1500/4367], Loss: 0.3711
2025-02-18 07:27:23,777 - Epoch [6/1000], Step [1600/4367], Loss: 0.4761
2025-02-18 07:27:49,628 - Epoch [6/1000], Step [1700/4367], Loss: 0.2878
2025-02-18 07:28:15,505 - Epoch [6/1000], Step [1800/4367], Loss: 0.3695
2025-02-18 07:28:41,307 - Epoch [6/1000], Step [1900/4367], Loss: 0.5015
2025-02-18 07:29:07,065 - Epoch [6/1000], Step [2000/4367], Loss: 0.6089
2025-02-18 07:29:33,248 - Epoch [6/1000], Step [2100/4367], Loss: 0.1770
2025-02-18 07:29:59,073 - Epoch [6/1000], Step [2200/4367], Loss: 0.6032
2025-02-18 07:30:24,761 - Epoch [6/1000], Step [2300/4367], Loss: 0.4706
2025-02-18 07:30:50,741 - Epoch [6/1000], Step [2400/4367], Loss: 0.3822
2025-02-18 07:31:16,585 - Epoch [6/1000], Step [2500/4367], Loss: 0.2385
2025-02-18 07:31:42,366 - Epoch [6/1000], Step [2600/4367], Loss: 0.3073
2025-02-18 07:32:08,141 - Epoch [6/1000], Step [2700/4367], Loss: 0.6550
2025-02-18 07:32:33,769 - Epoch [6/1000], Step [2800/4367], Loss: 0.4798
2025-02-18 07:32:59,615 - Epoch [6/1000], Step [2900/4367], Loss: 0.2196
2025-02-18 07:33:25,737 - Epoch [6/1000], Step [3000/4367], Loss: 0.7035
2025-02-18 07:33:51,438 - Epoch [6/1000], Step [3100/4367], Loss: 0.5098
2025-02-18 07:34:18,959 - Epoch [6/1000], Step [3200/4367], Loss: 0.2805
2025-02-18 07:34:45,089 - Epoch [6/1000], Step [3300/4367], Loss: 0.3428
2025-02-18 07:35:10,902 - Epoch [6/1000], Step [3400/4367], Loss: 0.3910
2025-02-18 07:35:36,968 - Epoch [6/1000], Step [3500/4367], Loss: 0.7172
2025-02-18 07:36:03,376 - Epoch [6/1000], Step [3600/4367], Loss: 0.7394
2025-02-18 07:36:29,142 - Epoch [6/1000], Step [3700/4367], Loss: 0.6281
2025-02-18 07:36:55,268 - Epoch [6/1000], Step [3800/4367], Loss: 0.4907
2025-02-18 07:37:21,326 - Epoch [6/1000], Step [3900/4367], Loss: 0.5458
2025-02-18 07:37:46,964 - Epoch [6/1000], Step [4000/4367], Loss: 0.2864
2025-02-18 07:38:12,935 - Epoch [6/1000], Step [4100/4367], Loss: 0.6423
2025-02-18 07:38:39,105 - Epoch [6/1000], Step [4200/4367], Loss: 0.7061
2025-02-18 07:39:05,389 - Epoch [6/1000], Step [4300/4367], Loss: 0.5419
2025-02-18 07:39:32,368 - Epoch [6/1000], Validation Step [100/1090], Val Loss: 0.0000
2025-02-18 07:39:39,722 - Epoch [6/1000], Validation Step [200/1090], Val Loss: 0.0000
2025-02-18 07:39:47,154 - Epoch [6/1000], Validation Step [300/1090], Val Loss: 1.2272
2025-02-18 07:39:54,833 - Epoch [6/1000], Validation Step [400/1090], Val Loss: 1.1616
2025-02-18 07:40:02,093 - Epoch [6/1000], Validation Step [500/1090], Val Loss: 0.4501
2025-02-18 07:40:09,674 - Epoch [6/1000], Validation Step [600/1090], Val Loss: 0.6396
2025-02-18 07:40:17,325 - Epoch [6/1000], Validation Step [700/1090], Val Loss: 0.4359
2025-02-18 07:40:24,359 - Epoch [6/1000], Validation Step [800/1090], Val Loss: 0.3510
2025-02-18 07:40:31,229 - Epoch [6/1000], Validation Step [900/1090], Val Loss: 0.3657
2025-02-18 07:40:38,616 - Epoch [6/1000], Validation Step [1000/1090], Val Loss: 2.6928
2025-02-18 07:40:45,543 - Epoch 6/1000, Train Loss: 0.4831, Val Loss: 0.9114, Accuracy: 72.46%
2025-02-18 07:40:46,111 - Model saved at /data2/personal/sungjin/korean_dialects/classification_second_checkpoint/checkpoint_epoch_6.pth
2025-02-18 07:41:14,199 - Epoch [7/1000], Step [100/4367], Loss: 0.5750
2025-02-18 07:41:40,136 - Epoch [7/1000], Step [200/4367], Loss: 0.6755
2025-02-18 07:42:06,067 - Epoch [7/1000], Step [300/4367], Loss: 0.3953
2025-02-18 07:42:31,864 - Epoch [7/1000], Step [400/4367], Loss: 0.3705
2025-02-18 07:42:57,645 - Epoch [7/1000], Step [500/4367], Loss: 0.2876
2025-02-18 07:43:23,168 - Epoch [7/1000], Step [600/4367], Loss: 0.3809
2025-02-18 07:43:49,147 - Epoch [7/1000], Step [700/4367], Loss: 0.4337
2025-02-18 07:44:15,055 - Epoch [7/1000], Step [800/4367], Loss: 0.7663
2025-02-18 07:44:41,143 - Epoch [7/1000], Step [900/4367], Loss: 0.4595
2025-02-18 07:45:07,192 - Epoch [7/1000], Step [1000/4367], Loss: 0.6827
2025-02-18 07:45:33,008 - Epoch [7/1000], Step [1100/4367], Loss: 0.4479
2025-02-18 07:45:59,067 - Epoch [7/1000], Step [1200/4367], Loss: 0.3706
2025-02-18 07:46:25,041 - Epoch [7/1000], Step [1300/4367], Loss: 0.4040
2025-02-18 07:46:51,052 - Epoch [7/1000], Step [1400/4367], Loss: 0.2642
2025-02-18 07:47:16,852 - Epoch [7/1000], Step [1500/4367], Loss: 0.2927
2025-02-18 07:47:42,943 - Epoch [7/1000], Step [1600/4367], Loss: 0.5693
2025-02-18 07:48:08,595 - Epoch [7/1000], Step [1700/4367], Loss: 0.5200
2025-02-18 07:48:34,485 - Epoch [7/1000], Step [1800/4367], Loss: 0.5693
2025-02-18 07:49:00,256 - Epoch [7/1000], Step [1900/4367], Loss: 0.7190
2025-02-18 07:49:26,192 - Epoch [7/1000], Step [2000/4367], Loss: 0.3342
2025-02-18 07:49:52,459 - Epoch [7/1000], Step [2100/4367], Loss: 0.3840
2025-02-18 07:50:17,909 - Epoch [7/1000], Step [2200/4367], Loss: 0.4846
2025-02-18 07:50:43,885 - Epoch [7/1000], Step [2300/4367], Loss: 0.3095
2025-02-18 07:51:09,603 - Epoch [7/1000], Step [2400/4367], Loss: 0.7471
2025-02-18 07:51:35,704 - Epoch [7/1000], Step [2500/4367], Loss: 0.4174
2025-02-18 07:52:01,767 - Epoch [7/1000], Step [2600/4367], Loss: 0.4156
2025-02-18 07:52:27,579 - Epoch [7/1000], Step [2700/4367], Loss: 0.6260
2025-02-18 07:52:53,314 - Epoch [7/1000], Step [2800/4367], Loss: 0.4109
2025-02-18 07:53:19,390 - Epoch [7/1000], Step [2900/4367], Loss: 0.5736
2025-02-18 07:53:45,343 - Epoch [7/1000], Step [3000/4367], Loss: 0.5622
2025-02-18 07:54:11,416 - Epoch [7/1000], Step [3100/4367], Loss: 0.5391
2025-02-18 07:54:37,552 - Epoch [7/1000], Step [3200/4367], Loss: 1.7135
2025-02-18 07:55:03,425 - Epoch [7/1000], Step [3300/4367], Loss: 1.4700
2025-02-18 07:55:29,557 - Epoch [7/1000], Step [3400/4367], Loss: 0.9982
2025-02-18 07:55:55,677 - Epoch [7/1000], Step [3500/4367], Loss: 0.9923
2025-02-18 07:56:21,652 - Epoch [7/1000], Step [3600/4367], Loss: 1.2614
2025-02-18 07:56:47,792 - Epoch [7/1000], Step [3700/4367], Loss: 0.8778
2025-02-18 07:57:13,750 - Epoch [7/1000], Step [3800/4367], Loss: 0.8030
2025-02-18 07:57:39,665 - Epoch [7/1000], Step [3900/4367], Loss: 0.7059
2025-02-18 07:58:05,481 - Epoch [7/1000], Step [4000/4367], Loss: 0.7564
2025-02-18 07:58:31,500 - Epoch [7/1000], Step [4100/4367], Loss: 0.8906
2025-02-18 07:58:57,330 - Epoch [7/1000], Step [4200/4367], Loss: 0.5539
2025-02-18 07:59:23,135 - Epoch [7/1000], Step [4300/4367], Loss: 0.7042
2025-02-18 07:59:49,866 - Epoch [7/1000], Validation Step [100/1090], Val Loss: 0.0002
2025-02-18 07:59:57,217 - Epoch [7/1000], Validation Step [200/1090], Val Loss: 0.0007
2025-02-18 08:00:04,645 - Epoch [7/1000], Validation Step [300/1090], Val Loss: 1.2351
2025-02-18 08:00:12,360 - Epoch [7/1000], Validation Step [400/1090], Val Loss: 2.6071
2025-02-18 08:00:19,600 - Epoch [7/1000], Validation Step [500/1090], Val Loss: 0.7692
2025-02-18 08:00:27,135 - Epoch [7/1000], Validation Step [600/1090], Val Loss: 0.3842
2025-02-18 08:00:34,718 - Epoch [7/1000], Validation Step [700/1090], Val Loss: 0.2954
2025-02-18 08:00:41,700 - Epoch [7/1000], Validation Step [800/1090], Val Loss: 1.0390
2025-02-18 08:00:48,504 - Epoch [7/1000], Validation Step [900/1090], Val Loss: 1.3153
2025-02-18 08:00:55,841 - Epoch [7/1000], Validation Step [1000/1090], Val Loss: 1.9846
2025-02-18 08:01:02,691 - Epoch 7/1000, Train Loss: 0.6104, Val Loss: 0.9347, Accuracy: 63.85%
2025-02-18 08:01:30,330 - Epoch [8/1000], Step [100/4367], Loss: 0.7352
2025-02-18 08:01:56,445 - Epoch [8/1000], Step [200/4367], Loss: 0.6631
2025-02-18 08:02:22,301 - Epoch [8/1000], Step [300/4367], Loss: 0.8445
2025-02-18 08:02:48,003 - Epoch [8/1000], Step [400/4367], Loss: 0.9360
2025-02-18 08:03:14,038 - Epoch [8/1000], Step [500/4367], Loss: 0.7806
2025-02-18 08:03:39,983 - Epoch [8/1000], Step [600/4367], Loss: 0.9633
2025-02-18 08:04:05,914 - Epoch [8/1000], Step [700/4367], Loss: 1.0756
2025-02-18 08:04:31,979 - Epoch [8/1000], Step [800/4367], Loss: 1.0429
2025-02-18 08:04:57,853 - Epoch [8/1000], Step [900/4367], Loss: 0.6099
2025-02-18 08:05:23,648 - Epoch [8/1000], Step [1000/4367], Loss: 0.4722
2025-02-18 08:05:49,335 - Epoch [8/1000], Step [1100/4367], Loss: 0.8521
2025-02-18 08:06:15,239 - Epoch [8/1000], Step [1200/4367], Loss: 0.7455
2025-02-18 08:06:41,239 - Epoch [8/1000], Step [1300/4367], Loss: 0.6527
2025-02-18 08:07:07,089 - Epoch [8/1000], Step [1400/4367], Loss: 0.4511
2025-02-18 08:07:32,951 - Epoch [8/1000], Step [1500/4367], Loss: 0.6250
2025-02-18 08:07:58,868 - Epoch [8/1000], Step [1600/4367], Loss: 0.7005
2025-02-18 08:08:24,541 - Epoch [8/1000], Step [1700/4367], Loss: 0.7277
2025-02-18 08:08:50,291 - Epoch [8/1000], Step [1800/4367], Loss: 0.6074
2025-02-18 08:09:16,318 - Epoch [8/1000], Step [1900/4367], Loss: 1.0820
2025-02-18 08:09:42,004 - Epoch [8/1000], Step [2000/4367], Loss: 0.4593
2025-02-18 08:10:07,882 - Epoch [8/1000], Step [2100/4367], Loss: 0.8298
2025-02-18 08:10:33,629 - Epoch [8/1000], Step [2200/4367], Loss: 0.5827
2025-02-18 08:10:59,440 - Epoch [8/1000], Step [2300/4367], Loss: 1.0996
2025-02-18 08:11:25,857 - Epoch [8/1000], Step [2400/4367], Loss: 0.8911
2025-02-18 08:11:51,950 - Epoch [8/1000], Step [2500/4367], Loss: 0.6739
2025-02-18 08:12:17,796 - Epoch [8/1000], Step [2600/4367], Loss: 0.7675
2025-02-18 08:12:43,616 - Epoch [8/1000], Step [2700/4367], Loss: 0.4945
2025-02-18 08:13:09,541 - Epoch [8/1000], Step [2800/4367], Loss: 0.4425
2025-02-18 08:13:35,238 - Epoch [8/1000], Step [2900/4367], Loss: 0.4979
2025-02-18 08:14:01,138 - Epoch [8/1000], Step [3000/4367], Loss: 0.3439
2025-02-18 08:14:26,941 - Epoch [8/1000], Step [3100/4367], Loss: 0.4479
2025-02-18 08:14:53,045 - Epoch [8/1000], Step [3200/4367], Loss: 0.2797
2025-02-18 08:15:18,622 - Epoch [8/1000], Step [3300/4367], Loss: 0.8759
2025-02-18 08:15:44,642 - Epoch [8/1000], Step [3400/4367], Loss: 0.7453
2025-02-18 08:16:10,510 - Epoch [8/1000], Step [3500/4367], Loss: 0.5539
2025-02-18 08:16:36,665 - Epoch [8/1000], Step [3600/4367], Loss: 0.4509
2025-02-18 08:17:02,462 - Epoch [8/1000], Step [3700/4367], Loss: 0.4215
2025-02-18 08:17:28,335 - Epoch [8/1000], Step [3800/4367], Loss: 0.4063
2025-02-18 08:17:54,140 - Epoch [8/1000], Step [3900/4367], Loss: 0.5824
2025-02-18 08:18:19,964 - Epoch [8/1000], Step [4000/4367], Loss: 0.5128
2025-02-18 08:18:45,846 - Epoch [8/1000], Step [4100/4367], Loss: 0.9140
2025-02-18 08:19:11,439 - Epoch [8/1000], Step [4200/4367], Loss: 0.4493
2025-02-18 08:19:36,924 - Epoch [8/1000], Step [4300/4367], Loss: 0.4019
2025-02-18 08:20:04,012 - Epoch [8/1000], Validation Step [100/1090], Val Loss: 0.4633
2025-02-18 08:20:11,375 - Epoch [8/1000], Validation Step [200/1090], Val Loss: 0.1583
2025-02-18 08:20:18,847 - Epoch [8/1000], Validation Step [300/1090], Val Loss: 0.7314
2025-02-18 08:20:26,577 - Epoch [8/1000], Validation Step [400/1090], Val Loss: 0.4833
2025-02-18 08:20:33,855 - Epoch [8/1000], Validation Step [500/1090], Val Loss: 0.4554
2025-02-18 08:20:41,460 - Epoch [8/1000], Validation Step [600/1090], Val Loss: 1.3677
2025-02-18 08:20:49,102 - Epoch [8/1000], Validation Step [700/1090], Val Loss: 1.2052
2025-02-18 08:20:56,134 - Epoch [8/1000], Validation Step [800/1090], Val Loss: 0.3271
2025-02-18 08:21:02,966 - Epoch [8/1000], Validation Step [900/1090], Val Loss: 0.4873
2025-02-18 08:21:10,318 - Epoch [8/1000], Validation Step [1000/1090], Val Loss: 0.2043
2025-02-18 08:21:17,211 - Epoch 8/1000, Train Loss: 0.6404, Val Loss: 0.8626, Accuracy: 67.69%
2025-02-18 08:21:17,785 - Model saved at /data2/personal/sungjin/korean_dialects/classification_second_checkpoint/checkpoint_epoch_8.pth
2025-02-18 08:21:45,980 - Epoch [9/1000], Step [100/4367], Loss: 0.3996
2025-02-18 08:22:11,965 - Epoch [9/1000], Step [200/4367], Loss: 0.3573
2025-02-18 08:22:37,929 - Epoch [9/1000], Step [300/4367], Loss: 0.6106
2025-02-18 08:23:03,952 - Epoch [9/1000], Step [400/4367], Loss: 0.4675
2025-02-18 08:23:30,161 - Epoch [9/1000], Step [500/4367], Loss: 0.4979
2025-02-18 08:23:56,066 - Epoch [9/1000], Step [600/4367], Loss: 0.2739
2025-02-18 08:24:21,937 - Epoch [9/1000], Step [700/4367], Loss: 0.5414
2025-02-18 08:24:48,367 - Epoch [9/1000], Step [800/4367], Loss: 0.6193
2025-02-18 08:25:14,381 - Epoch [9/1000], Step [900/4367], Loss: 0.8521
2025-02-18 08:25:40,073 - Epoch [9/1000], Step [1000/4367], Loss: 0.7783
2025-02-18 08:26:05,849 - Epoch [9/1000], Step [1100/4367], Loss: 0.9322
2025-02-18 08:26:31,412 - Epoch [9/1000], Step [1200/4367], Loss: 0.7749
2025-02-18 08:26:57,395 - Epoch [9/1000], Step [1300/4367], Loss: 0.4942
2025-02-18 08:27:23,545 - Epoch [9/1000], Step [1400/4367], Loss: 0.7256
2025-02-18 08:27:49,550 - Epoch [9/1000], Step [1500/4367], Loss: 0.5681
2025-02-18 08:28:15,480 - Epoch [9/1000], Step [1600/4367], Loss: 0.6863
2025-02-18 08:28:41,295 - Epoch [9/1000], Step [1700/4367], Loss: 0.7224
2025-02-18 08:29:06,973 - Epoch [9/1000], Step [1800/4367], Loss: 0.7865
2025-02-18 08:29:33,299 - Epoch [9/1000], Step [1900/4367], Loss: 0.5741
2025-02-18 08:29:58,991 - Epoch [9/1000], Step [2000/4367], Loss: 0.6529
2025-02-18 08:30:24,765 - Epoch [9/1000], Step [2100/4367], Loss: 0.6625
2025-02-18 08:30:50,850 - Epoch [9/1000], Step [2200/4367], Loss: 0.4842
2025-02-18 08:31:16,850 - Epoch [9/1000], Step [2300/4367], Loss: 0.5373
2025-02-18 08:31:42,913 - Epoch [9/1000], Step [2400/4367], Loss: 0.4572
2025-02-18 08:32:09,039 - Epoch [9/1000], Step [2500/4367], Loss: 0.4171
2025-02-18 08:32:35,001 - Epoch [9/1000], Step [2600/4367], Loss: 0.9430
2025-02-18 08:33:00,844 - Epoch [9/1000], Step [2700/4367], Loss: 0.3321
2025-02-18 08:33:26,682 - Epoch [9/1000], Step [2800/4367], Loss: 0.3758
2025-02-18 08:33:52,587 - Epoch [9/1000], Step [2900/4367], Loss: 0.7917
2025-02-18 08:34:18,209 - Epoch [9/1000], Step [3000/4367], Loss: 0.6575
2025-02-18 08:34:44,298 - Epoch [9/1000], Step [3100/4367], Loss: 0.6925
2025-02-18 08:35:10,326 - Epoch [9/1000], Step [3200/4367], Loss: 0.7247
2025-02-18 08:35:36,362 - Epoch [9/1000], Step [3300/4367], Loss: 0.6655
2025-02-18 08:36:02,044 - Epoch [9/1000], Step [3400/4367], Loss: 0.6077
2025-02-18 08:36:27,957 - Epoch [9/1000], Step [3500/4367], Loss: 0.5193
2025-02-18 08:36:53,810 - Epoch [9/1000], Step [3600/4367], Loss: 0.9921
2025-02-18 08:37:19,850 - Epoch [9/1000], Step [3700/4367], Loss: 0.5022
2025-02-18 08:37:45,245 - Epoch [9/1000], Step [3800/4367], Loss: 0.6373
2025-02-18 08:38:11,246 - Epoch [9/1000], Step [3900/4367], Loss: 0.8833
2025-02-18 08:38:37,266 - Epoch [9/1000], Step [4000/4367], Loss: 0.3087
2025-02-18 08:39:03,300 - Epoch [9/1000], Step [4100/4367], Loss: 0.5236
2025-02-18 08:39:29,135 - Epoch [9/1000], Step [4200/4367], Loss: 0.3136
2025-02-18 08:39:55,270 - Epoch [9/1000], Step [4300/4367], Loss: 0.6445
2025-02-18 08:40:21,921 - Epoch [9/1000], Validation Step [100/1090], Val Loss: 0.0017
2025-02-18 08:40:29,280 - Epoch [9/1000], Validation Step [200/1090], Val Loss: 0.0001
2025-02-18 08:40:36,746 - Epoch [9/1000], Validation Step [300/1090], Val Loss: 0.7351
2025-02-18 08:40:44,490 - Epoch [9/1000], Validation Step [400/1090], Val Loss: 2.5248
2025-02-18 08:40:51,738 - Epoch [9/1000], Validation Step [500/1090], Val Loss: 0.8237
2025-02-18 08:40:59,354 - Epoch [9/1000], Validation Step [600/1090], Val Loss: 0.4979
2025-02-18 08:41:06,989 - Epoch [9/1000], Validation Step [700/1090], Val Loss: 0.3808
2025-02-18 08:41:13,987 - Epoch [9/1000], Validation Step [800/1090], Val Loss: 0.8866
2025-02-18 08:41:20,814 - Epoch [9/1000], Validation Step [900/1090], Val Loss: 1.2510
2025-02-18 08:41:28,183 - Epoch [9/1000], Validation Step [1000/1090], Val Loss: 0.6613
2025-02-18 08:41:35,096 - Epoch 9/1000, Train Loss: 0.6347, Val Loss: 0.6555, Accuracy: 74.64%
2025-02-18 08:42:03,332 - Epoch [10/1000], Step [100/4367], Loss: 0.8826
2025-02-18 08:42:29,078 - Epoch [10/1000], Step [200/4367], Loss: 0.6976
2025-02-18 08:42:54,916 - Epoch [10/1000], Step [300/4367], Loss: 0.6717
2025-02-18 08:43:20,701 - Epoch [10/1000], Step [400/4367], Loss: 0.5857
2025-02-18 08:43:46,601 - Epoch [10/1000], Step [500/4367], Loss: 0.8673
2025-02-18 08:44:12,464 - Epoch [10/1000], Step [600/4367], Loss: 0.6466
2025-02-18 08:44:38,307 - Epoch [10/1000], Step [700/4367], Loss: 1.2595
2025-02-18 08:45:04,615 - Epoch [10/1000], Step [800/4367], Loss: 1.0963
2025-02-18 08:45:30,627 - Epoch [10/1000], Step [900/4367], Loss: 0.5220
2025-02-18 08:45:56,435 - Epoch [10/1000], Step [1000/4367], Loss: 0.4357
2025-02-18 08:46:22,525 - Epoch [10/1000], Step [1100/4367], Loss: 0.8309
2025-02-18 08:46:48,344 - Epoch [10/1000], Step [1200/4367], Loss: 0.8146
2025-02-18 08:47:14,537 - Epoch [10/1000], Step [1300/4367], Loss: 0.7703
2025-02-18 08:47:40,586 - Epoch [10/1000], Step [1400/4367], Loss: 1.2737
2025-02-18 08:48:06,800 - Epoch [10/1000], Step [1500/4367], Loss: 0.8943
2025-02-18 08:48:32,577 - Epoch [10/1000], Step [1600/4367], Loss: 0.8304
2025-02-18 08:48:58,430 - Epoch [10/1000], Step [1700/4367], Loss: 0.6670
2025-02-18 08:49:24,494 - Epoch [10/1000], Step [1800/4367], Loss: 0.4359
2025-02-18 08:49:50,679 - Epoch [10/1000], Step [1900/4367], Loss: 0.8794
2025-02-18 08:50:16,511 - Epoch [10/1000], Step [2000/4367], Loss: 0.5240
2025-02-18 08:50:42,265 - Epoch [10/1000], Step [2100/4367], Loss: 0.9662
2025-02-18 08:51:08,233 - Epoch [10/1000], Step [2200/4367], Loss: 0.4293
2025-02-18 08:51:34,108 - Epoch [10/1000], Step [2300/4367], Loss: 0.9200
2025-02-18 08:52:00,088 - Epoch [10/1000], Step [2400/4367], Loss: 0.6183
2025-02-18 08:52:26,152 - Epoch [10/1000], Step [2500/4367], Loss: 0.5983
2025-02-18 08:52:52,405 - Epoch [10/1000], Step [2600/4367], Loss: 0.5928
2025-02-18 08:53:18,357 - Epoch [10/1000], Step [2700/4367], Loss: 0.7772
2025-02-18 08:53:44,431 - Epoch [10/1000], Step [2800/4367], Loss: 0.6638
2025-02-18 08:54:10,092 - Epoch [10/1000], Step [2900/4367], Loss: 0.4330
2025-02-18 08:54:36,209 - Epoch [10/1000], Step [3000/4367], Loss: 0.6500
2025-02-18 08:55:02,224 - Epoch [10/1000], Step [3100/4367], Loss: 0.7168
2025-02-18 08:55:28,433 - Epoch [10/1000], Step [3200/4367], Loss: 0.6197
2025-02-18 08:55:54,501 - Epoch [10/1000], Step [3300/4367], Loss: 0.7175
2025-02-18 08:56:20,204 - Epoch [10/1000], Step [3400/4367], Loss: 0.5278
2025-02-18 08:56:45,925 - Epoch [10/1000], Step [3500/4367], Loss: 0.3457
2025-02-18 08:57:11,629 - Epoch [10/1000], Step [3600/4367], Loss: 0.6345
2025-02-18 08:57:37,862 - Epoch [10/1000], Step [3700/4367], Loss: 0.7432
2025-02-18 08:58:04,170 - Epoch [10/1000], Step [3800/4367], Loss: 0.6891
2025-02-18 08:58:30,434 - Epoch [10/1000], Step [3900/4367], Loss: 0.6211
2025-02-18 08:58:56,335 - Epoch [10/1000], Step [4000/4367], Loss: 0.5548
2025-02-18 08:59:22,208 - Epoch [10/1000], Step [4100/4367], Loss: 0.5323
2025-02-18 08:59:48,168 - Epoch [10/1000], Step [4200/4367], Loss: 0.3408
2025-02-18 09:00:14,012 - Epoch [10/1000], Step [4300/4367], Loss: 0.4479
2025-02-18 09:00:40,720 - Epoch [10/1000], Validation Step [100/1090], Val Loss: 0.0120
2025-02-18 09:00:48,093 - Epoch [10/1000], Validation Step [200/1090], Val Loss: 0.0003
2025-02-18 09:00:55,549 - Epoch [10/1000], Validation Step [300/1090], Val Loss: 1.3681
2025-02-18 09:01:03,251 - Epoch [10/1000], Validation Step [400/1090], Val Loss: 0.9150
2025-02-18 09:01:10,516 - Epoch [10/1000], Validation Step [500/1090], Val Loss: 0.8347
2025-02-18 09:01:18,070 - Epoch [10/1000], Validation Step [600/1090], Val Loss: 0.9805
2025-02-18 09:01:25,686 - Epoch [10/1000], Validation Step [700/1090], Val Loss: 0.8116
2025-02-18 09:01:32,700 - Epoch [10/1000], Validation Step [800/1090], Val Loss: 0.4187
2025-02-18 09:01:39,550 - Epoch [10/1000], Validation Step [900/1090], Val Loss: 0.6736
2025-02-18 09:01:46,968 - Epoch [10/1000], Validation Step [1000/1090], Val Loss: 0.2305
2025-02-18 09:01:53,901 - Epoch 10/1000, Train Loss: 0.6819, Val Loss: 0.6208, Accuracy: 76.62%
2025-02-18 09:01:54,463 - Model saved at /data2/personal/sungjin/korean_dialects/classification_second_checkpoint/checkpoint_epoch_10.pth
2025-02-18 09:02:22,473 - Epoch [11/1000], Step [100/4367], Loss: 0.4352
2025-02-18 09:02:48,574 - Epoch [11/1000], Step [200/4367], Loss: 0.4784
2025-02-18 09:03:14,522 - Epoch [11/1000], Step [300/4367], Loss: 0.4443
2025-02-18 09:03:40,376 - Epoch [11/1000], Step [400/4367], Loss: 0.6132
2025-02-18 09:04:06,179 - Epoch [11/1000], Step [500/4367], Loss: 0.7021
2025-02-18 09:04:31,986 - Epoch [11/1000], Step [600/4367], Loss: 0.8611
2025-02-18 09:04:58,058 - Epoch [11/1000], Step [700/4367], Loss: 0.6049
2025-02-18 09:05:24,394 - Epoch [11/1000], Step [800/4367], Loss: 0.6549
2025-02-18 09:05:50,302 - Epoch [11/1000], Step [900/4367], Loss: 0.5670
2025-02-18 09:06:16,371 - Epoch [11/1000], Step [1000/4367], Loss: 0.6747
2025-02-18 09:06:42,236 - Epoch [11/1000], Step [1100/4367], Loss: 0.5030
2025-02-18 09:07:07,909 - Epoch [11/1000], Step [1200/4367], Loss: 0.4772
2025-02-18 09:07:34,166 - Epoch [11/1000], Step [1300/4367], Loss: 0.9127
2025-02-18 09:08:00,129 - Epoch [11/1000], Step [1400/4367], Loss: 0.4842
2025-02-18 09:08:26,305 - Epoch [11/1000], Step [1500/4367], Loss: 0.5782
2025-02-18 09:08:52,384 - Epoch [11/1000], Step [1600/4367], Loss: 0.7814
2025-02-18 09:09:18,281 - Epoch [11/1000], Step [1700/4367], Loss: 0.4771
2025-02-18 09:09:44,292 - Epoch [11/1000], Step [1800/4367], Loss: 0.6011
2025-02-18 09:10:09,979 - Epoch [11/1000], Step [1900/4367], Loss: 0.3802
2025-02-18 09:10:35,945 - Epoch [11/1000], Step [2000/4367], Loss: 0.5293
2025-02-18 09:11:01,857 - Epoch [11/1000], Step [2100/4367], Loss: 0.5754
2025-02-18 09:11:27,824 - Epoch [11/1000], Step [2200/4367], Loss: 0.5098
2025-02-18 09:11:53,726 - Epoch [11/1000], Step [2300/4367], Loss: 0.8454
2025-02-18 09:12:19,606 - Epoch [11/1000], Step [2400/4367], Loss: 0.4492
2025-02-18 09:12:45,527 - Epoch [11/1000], Step [2500/4367], Loss: 0.4908
2025-02-18 09:13:11,605 - Epoch [11/1000], Step [2600/4367], Loss: 0.5922
2025-02-18 09:13:37,562 - Epoch [11/1000], Step [2700/4367], Loss: 1.0158
2025-02-18 09:14:03,376 - Epoch [11/1000], Step [2800/4367], Loss: 0.6895
2025-02-18 09:14:29,198 - Epoch [11/1000], Step [2900/4367], Loss: 0.8262
2025-02-18 09:14:54,932 - Epoch [11/1000], Step [3000/4367], Loss: 0.6525
2025-02-18 09:15:20,901 - Epoch [11/1000], Step [3100/4367], Loss: 0.8231
2025-02-18 09:15:46,960 - Epoch [11/1000], Step [3200/4367], Loss: 0.5375
2025-02-18 09:16:12,930 - Epoch [11/1000], Step [3300/4367], Loss: 0.3935
2025-02-18 09:16:38,520 - Epoch [11/1000], Step [3400/4367], Loss: 0.7428
2025-02-18 09:17:04,626 - Epoch [11/1000], Step [3500/4367], Loss: 0.5208
2025-02-18 09:17:30,463 - Epoch [11/1000], Step [3600/4367], Loss: 0.6592
2025-02-18 09:17:56,679 - Epoch [11/1000], Step [3700/4367], Loss: 0.5529
2025-02-18 09:18:22,476 - Epoch [11/1000], Step [3800/4367], Loss: 0.8756
2025-02-18 09:18:48,466 - Epoch [11/1000], Step [3900/4367], Loss: 0.4570
2025-02-18 09:19:14,376 - Epoch [11/1000], Step [4000/4367], Loss: 0.5789
2025-02-18 09:19:40,310 - Epoch [11/1000], Step [4100/4367], Loss: 0.4428
2025-02-18 09:20:06,034 - Epoch [11/1000], Step [4200/4367], Loss: 0.4482
2025-02-18 09:20:31,767 - Epoch [11/1000], Step [4300/4367], Loss: 0.8066
2025-02-18 09:20:58,713 - Epoch [11/1000], Validation Step [100/1090], Val Loss: 0.0000
2025-02-18 09:21:06,060 - Epoch [11/1000], Validation Step [200/1090], Val Loss: 0.0000
2025-02-18 09:21:13,501 - Epoch [11/1000], Validation Step [300/1090], Val Loss: 1.1259
2025-02-18 09:21:21,180 - Epoch [11/1000], Validation Step [400/1090], Val Loss: 3.4948
2025-02-18 09:21:28,383 - Epoch [11/1000], Validation Step [500/1090], Val Loss: 0.5961
2025-02-18 09:21:35,903 - Epoch [11/1000], Validation Step [600/1090], Val Loss: 0.6721
2025-02-18 09:21:43,476 - Epoch [11/1000], Validation Step [700/1090], Val Loss: 0.4826
2025-02-18 09:21:50,455 - Epoch [11/1000], Validation Step [800/1090], Val Loss: 1.7370
2025-02-18 09:21:57,252 - Epoch [11/1000], Validation Step [900/1090], Val Loss: 1.8585
2025-02-18 09:22:04,582 - Epoch [11/1000], Validation Step [1000/1090], Val Loss: 0.7192
2025-02-18 09:22:11,448 - Epoch 11/1000, Train Loss: 0.6025, Val Loss: 0.9238, Accuracy: 66.89%
2025-02-18 09:22:39,661 - Epoch [12/1000], Step [100/4367], Loss: 0.5475
2025-02-18 09:23:05,125 - Epoch [12/1000], Step [200/4367], Loss: 0.7624
2025-02-18 09:23:31,150 - Epoch [12/1000], Step [300/4367], Loss: 0.4068
2025-02-18 09:23:57,193 - Epoch [12/1000], Step [400/4367], Loss: 0.4295
2025-02-18 09:24:23,211 - Epoch [12/1000], Step [500/4367], Loss: 0.4825
2025-02-18 09:24:49,019 - Epoch [12/1000], Step [600/4367], Loss: 0.3339
2025-02-18 09:25:14,807 - Epoch [12/1000], Step [700/4367], Loss: 0.6197
2025-02-18 09:25:40,508 - Epoch [12/1000], Step [800/4367], Loss: 0.6853
2025-02-18 09:26:06,562 - Epoch [12/1000], Step [900/4367], Loss: 0.6619
2025-02-18 09:26:32,422 - Epoch [12/1000], Step [1000/4367], Loss: 0.6217
2025-02-18 09:26:58,072 - Epoch [12/1000], Step [1100/4367], Loss: 0.5135
2025-02-18 09:27:23,969 - Epoch [12/1000], Step [1200/4367], Loss: 0.7219
2025-02-18 09:27:49,941 - Epoch [12/1000], Step [1300/4367], Loss: 0.8547
2025-02-18 09:28:16,113 - Epoch [12/1000], Step [1400/4367], Loss: 0.6652
2025-02-18 09:28:41,796 - Epoch [12/1000], Step [1500/4367], Loss: 0.7503
2025-02-18 09:29:07,580 - Epoch [12/1000], Step [1600/4367], Loss: 1.0024
2025-02-18 09:29:33,232 - Epoch [12/1000], Step [1700/4367], Loss: 0.8278
2025-02-18 09:29:59,089 - Epoch [12/1000], Step [1800/4367], Loss: 0.8825
2025-02-18 09:30:25,091 - Epoch [12/1000], Step [1900/4367], Loss: 0.6935
2025-02-18 09:30:50,885 - Epoch [12/1000], Step [2000/4367], Loss: 0.9540
2025-02-18 09:31:16,924 - Epoch [12/1000], Step [2100/4367], Loss: 0.8014
2025-02-18 09:31:43,007 - Epoch [12/1000], Step [2200/4367], Loss: 0.9890
2025-02-18 09:32:09,237 - Epoch [12/1000], Step [2300/4367], Loss: 0.6340
2025-02-18 09:32:35,049 - Epoch [12/1000], Step [2400/4367], Loss: 0.9602
2025-02-18 09:33:00,927 - Epoch [12/1000], Step [2500/4367], Loss: 1.1131
2025-02-18 09:33:26,803 - Epoch [12/1000], Step [2600/4367], Loss: 0.8717
2025-02-18 09:33:52,797 - Epoch [12/1000], Step [2700/4367], Loss: 0.5192
2025-02-18 09:34:19,013 - Epoch [12/1000], Step [2800/4367], Loss: 0.5232
2025-02-18 09:34:44,789 - Epoch [12/1000], Step [2900/4367], Loss: 1.0428
2025-02-18 09:35:10,711 - Epoch [12/1000], Step [3000/4367], Loss: 0.8042
2025-02-18 09:35:36,708 - Epoch [12/1000], Step [3100/4367], Loss: 0.6886
2025-02-18 09:36:02,403 - Epoch [12/1000], Step [3200/4367], Loss: 0.7504
2025-02-18 09:36:28,468 - Epoch [12/1000], Step [3300/4367], Loss: 1.0877
2025-02-18 09:36:54,296 - Epoch [12/1000], Step [3400/4367], Loss: 0.7148
2025-02-18 09:37:20,165 - Epoch [12/1000], Step [3500/4367], Loss: 0.6531
2025-02-18 09:37:46,001 - Epoch [12/1000], Step [3600/4367], Loss: 0.6223
2025-02-18 09:38:11,839 - Epoch [12/1000], Step [3700/4367], Loss: 0.9479
2025-02-18 09:38:37,912 - Epoch [12/1000], Step [3800/4367], Loss: 0.6519
2025-02-18 09:39:03,642 - Epoch [12/1000], Step [3900/4367], Loss: 0.6023
2025-02-18 09:39:29,255 - Epoch [12/1000], Step [4000/4367], Loss: 0.5568
2025-02-18 09:39:54,984 - Epoch [12/1000], Step [4100/4367], Loss: 0.8196
2025-02-18 09:40:21,010 - Epoch [12/1000], Step [4200/4367], Loss: 0.7724
2025-02-18 09:40:47,041 - Epoch [12/1000], Step [4300/4367], Loss: 0.8061
2025-02-18 09:41:13,812 - Epoch [12/1000], Validation Step [100/1090], Val Loss: 1.1848
2025-02-18 09:41:21,107 - Epoch [12/1000], Validation Step [200/1090], Val Loss: 0.6632
2025-02-18 09:41:28,503 - Epoch [12/1000], Validation Step [300/1090], Val Loss: 1.0195
2025-02-18 09:41:36,135 - Epoch [12/1000], Validation Step [400/1090], Val Loss: 1.5631
2025-02-18 09:41:43,331 - Epoch [12/1000], Validation Step [500/1090], Val Loss: 2.0462
2025-02-18 09:41:50,850 - Epoch [12/1000], Validation Step [600/1090], Val Loss: 0.5814
2025-02-18 09:41:58,432 - Epoch [12/1000], Validation Step [700/1090], Val Loss: 0.5908
2025-02-18 09:42:05,396 - Epoch [12/1000], Validation Step [800/1090], Val Loss: 0.6228
2025-02-18 09:42:12,185 - Epoch [12/1000], Validation Step [900/1090], Val Loss: 0.7382
2025-02-18 09:42:19,527 - Epoch [12/1000], Validation Step [1000/1090], Val Loss: 1.9484
2025-02-18 09:42:26,371 - Epoch 12/1000, Train Loss: 0.7551, Val Loss: 1.3435, Accuracy: 53.22%
2025-02-18 09:42:26,926 - Model saved at /data2/personal/sungjin/korean_dialects/classification_second_checkpoint/checkpoint_epoch_12.pth
2025-02-18 09:42:54,540 - Epoch [13/1000], Step [100/4367], Loss: 1.0269
2025-02-18 09:43:20,163 - Epoch [13/1000], Step [200/4367], Loss: 1.0638
2025-02-18 09:43:45,726 - Epoch [13/1000], Step [300/4367], Loss: 0.9462
2025-02-18 09:44:11,517 - Epoch [13/1000], Step [400/4367], Loss: 0.7618
2025-02-18 09:44:37,647 - Epoch [13/1000], Step [500/4367], Loss: 0.6481
2025-02-18 09:45:03,115 - Epoch [13/1000], Step [600/4367], Loss: 0.6743
2025-02-18 09:45:28,964 - Epoch [13/1000], Step [700/4367], Loss: 0.5900
2025-02-18 09:45:55,108 - Epoch [13/1000], Step [800/4367], Loss: 0.7196
2025-02-18 09:46:20,559 - Epoch [13/1000], Step [900/4367], Loss: 0.6250
2025-02-18 09:46:46,824 - Epoch [13/1000], Step [1000/4367], Loss: 0.8488
2025-02-18 09:47:12,692 - Epoch [13/1000], Step [1100/4367], Loss: 0.6502
2025-02-18 09:47:38,700 - Epoch [13/1000], Step [1200/4367], Loss: 0.8606
2025-02-18 09:48:04,691 - Epoch [13/1000], Step [1300/4367], Loss: 0.6451
2025-02-18 09:48:30,900 - Epoch [13/1000], Step [1400/4367], Loss: 0.4305
2025-02-18 09:48:56,622 - Epoch [13/1000], Step [1500/4367], Loss: 0.9355
2025-02-18 09:49:22,658 - Epoch [13/1000], Step [1600/4367], Loss: 0.7803
2025-02-18 09:49:48,377 - Epoch [13/1000], Step [1700/4367], Loss: 0.6770
2025-02-18 09:50:14,246 - Epoch [13/1000], Step [1800/4367], Loss: 0.5237
2025-02-18 09:50:40,547 - Epoch [13/1000], Step [1900/4367], Loss: 0.5690
2025-02-18 09:51:06,469 - Epoch [13/1000], Step [2000/4367], Loss: 0.6641
2025-02-18 09:51:32,440 - Epoch [13/1000], Step [2100/4367], Loss: 0.4416
2025-02-18 09:51:58,372 - Epoch [13/1000], Step [2200/4367], Loss: 0.8701
2025-02-18 09:52:24,396 - Epoch [13/1000], Step [2300/4367], Loss: 0.5467
2025-02-18 09:52:50,262 - Epoch [13/1000], Step [2400/4367], Loss: 0.7546
2025-02-18 09:53:16,312 - Epoch [13/1000], Step [2500/4367], Loss: 0.6538
2025-02-18 09:53:42,295 - Epoch [13/1000], Step [2600/4367], Loss: 0.5715
2025-02-18 09:54:08,233 - Epoch [13/1000], Step [2700/4367], Loss: 0.6148
2025-02-18 09:54:34,220 - Epoch [13/1000], Step [2800/4367], Loss: 0.5217
2025-02-18 09:55:00,367 - Epoch [13/1000], Step [2900/4367], Loss: 0.8324
2025-02-18 09:55:26,324 - Epoch [13/1000], Step [3000/4367], Loss: 0.6009
2025-02-18 09:55:52,085 - Epoch [13/1000], Step [3100/4367], Loss: 0.5979
2025-02-18 09:56:17,689 - Epoch [13/1000], Step [3200/4367], Loss: 0.6059
2025-02-18 09:56:43,638 - Epoch [13/1000], Step [3300/4367], Loss: 0.7898
2025-02-18 09:57:09,576 - Epoch [13/1000], Step [3400/4367], Loss: 0.4136
2025-02-18 09:57:35,224 - Epoch [13/1000], Step [3500/4367], Loss: 0.5822
2025-02-18 09:58:01,066 - Epoch [13/1000], Step [3600/4367], Loss: 0.5450
2025-02-18 09:58:27,151 - Epoch [13/1000], Step [3700/4367], Loss: 0.6736
2025-02-18 09:58:52,877 - Epoch [13/1000], Step [3800/4367], Loss: 0.5557
2025-02-18 09:59:19,059 - Epoch [13/1000], Step [3900/4367], Loss: 0.7524
2025-02-18 09:59:44,678 - Epoch [13/1000], Step [4000/4367], Loss: 0.5538
2025-02-18 10:00:10,419 - Epoch [13/1000], Step [4100/4367], Loss: 0.3258
2025-02-18 10:00:36,421 - Epoch [13/1000], Step [4200/4367], Loss: 0.6124
2025-02-18 10:01:02,496 - Epoch [13/1000], Step [4300/4367], Loss: 0.9403
2025-02-18 10:01:28,855 - Epoch [13/1000], Validation Step [100/1090], Val Loss: 0.0656
2025-02-18 10:01:36,184 - Epoch [13/1000], Validation Step [200/1090], Val Loss: 0.0040
2025-02-18 10:01:43,579 - Epoch [13/1000], Validation Step [300/1090], Val Loss: 1.0138
2025-02-18 10:01:51,222 - Epoch [13/1000], Validation Step [400/1090], Val Loss: 0.8109
2025-02-18 10:01:58,433 - Epoch [13/1000], Validation Step [500/1090], Val Loss: 0.4744
2025-02-18 10:02:05,960 - Epoch [13/1000], Validation Step [600/1090], Val Loss: 1.1842
2025-02-18 10:02:13,547 - Epoch [13/1000], Validation Step [700/1090], Val Loss: 0.7656
2025-02-18 10:02:20,521 - Epoch [13/1000], Validation Step [800/1090], Val Loss: 0.6933
2025-02-18 10:02:27,329 - Epoch [13/1000], Validation Step [900/1090], Val Loss: 0.9457
2025-02-18 10:02:34,662 - Epoch [13/1000], Validation Step [1000/1090], Val Loss: 0.2358
2025-02-18 10:02:41,523 - Epoch 13/1000, Train Loss: 0.6981, Val Loss: 0.6177, Accuracy: 76.14%
2025-02-18 10:03:09,569 - Epoch [14/1000], Step [100/4367], Loss: 0.4021
2025-02-18 10:03:35,464 - Epoch [14/1000], Step [200/4367], Loss: 0.4134
2025-02-18 10:04:01,381 - Epoch [14/1000], Step [300/4367], Loss: 0.8585
2025-02-18 10:04:27,352 - Epoch [14/1000], Step [400/4367], Loss: 0.7196
2025-02-18 10:04:53,095 - Epoch [14/1000], Step [500/4367], Loss: 0.5576
2025-02-18 10:05:19,113 - Epoch [14/1000], Step [600/4367], Loss: 0.5465
2025-02-18 10:05:45,215 - Epoch [14/1000], Step [700/4367], Loss: 0.4957
2025-02-18 10:06:10,917 - Epoch [14/1000], Step [800/4367], Loss: 0.3974
2025-02-18 10:06:36,817 - Epoch [14/1000], Step [900/4367], Loss: 0.5857
2025-02-18 10:07:02,417 - Epoch [14/1000], Step [1000/4367], Loss: 0.2701
2025-02-18 10:07:28,518 - Epoch [14/1000], Step [1100/4367], Loss: 0.4710
2025-02-18 10:07:54,788 - Epoch [14/1000], Step [1200/4367], Loss: 0.6716
2025-02-18 10:08:20,190 - Epoch [14/1000], Step [1300/4367], Loss: 0.7322
2025-02-18 10:08:45,990 - Epoch [14/1000], Step [1400/4367], Loss: 0.5597
2025-02-18 10:09:11,532 - Epoch [14/1000], Step [1500/4367], Loss: 0.4306
2025-02-18 10:09:37,293 - Epoch [14/1000], Step [1600/4367], Loss: 0.7374
2025-02-18 10:10:03,381 - Epoch [14/1000], Step [1700/4367], Loss: 0.6240
2025-02-18 10:10:29,179 - Epoch [14/1000], Step [1800/4367], Loss: 0.4683
2025-02-18 10:10:55,287 - Epoch [14/1000], Step [1900/4367], Loss: 0.5248
2025-02-18 10:11:21,174 - Epoch [14/1000], Step [2000/4367], Loss: 0.6216
2025-02-18 10:11:47,023 - Epoch [14/1000], Step [2100/4367], Loss: 0.7312
2025-02-18 10:12:12,836 - Epoch [14/1000], Step [2200/4367], Loss: 0.4672
2025-02-18 10:12:38,758 - Epoch [14/1000], Step [2300/4367], Loss: 0.4163
2025-02-18 10:13:04,428 - Epoch [14/1000], Step [2400/4367], Loss: 0.6706
2025-02-18 10:13:30,427 - Epoch [14/1000], Step [2500/4367], Loss: 0.4298
2025-02-18 10:13:56,356 - Epoch [14/1000], Step [2600/4367], Loss: 0.5099
2025-02-18 10:14:22,235 - Epoch [14/1000], Step [2700/4367], Loss: 0.4900
2025-02-18 10:14:48,454 - Epoch [14/1000], Step [2800/4367], Loss: 0.4535
2025-02-18 10:15:14,256 - Epoch [14/1000], Step [2900/4367], Loss: 0.1835
2025-02-18 10:15:39,835 - Epoch [14/1000], Step [3000/4367], Loss: 0.7407
2025-02-18 10:16:05,471 - Epoch [14/1000], Step [3100/4367], Loss: 0.3539
2025-02-18 10:16:31,294 - Epoch [14/1000], Step [3200/4367], Loss: 0.4328
2025-02-18 10:16:57,218 - Epoch [14/1000], Step [3300/4367], Loss: 0.5332
2025-02-18 10:17:23,282 - Epoch [14/1000], Step [3400/4367], Loss: 0.2578
2025-02-18 10:17:49,400 - Epoch [14/1000], Step [3500/4367], Loss: 0.5387
2025-02-18 10:18:15,587 - Epoch [14/1000], Step [3600/4367], Loss: 0.4067
2025-02-18 10:18:41,455 - Epoch [14/1000], Step [3700/4367], Loss: 0.4287
2025-02-18 10:19:07,311 - Epoch [14/1000], Step [3800/4367], Loss: 0.4638
2025-02-18 10:19:33,158 - Epoch [14/1000], Step [3900/4367], Loss: 0.4785
2025-02-18 10:19:59,341 - Epoch [14/1000], Step [4000/4367], Loss: 0.4752
2025-02-18 10:20:25,270 - Epoch [14/1000], Step [4100/4367], Loss: 0.4504
2025-02-18 10:20:51,088 - Epoch [14/1000], Step [4200/4367], Loss: 0.6367
2025-02-18 10:21:17,072 - Epoch [14/1000], Step [4300/4367], Loss: 0.6208
2025-02-18 10:21:44,200 - Epoch [14/1000], Validation Step [100/1090], Val Loss: 0.0071
2025-02-18 10:21:51,557 - Epoch [14/1000], Validation Step [200/1090], Val Loss: 0.0069
2025-02-18 10:21:59,013 - Epoch [14/1000], Validation Step [300/1090], Val Loss: 1.0353
2025-02-18 10:22:06,701 - Epoch [14/1000], Validation Step [400/1090], Val Loss: 0.5070
2025-02-18 10:22:13,949 - Epoch [14/1000], Validation Step [500/1090], Val Loss: 0.9303
2025-02-18 10:22:21,521 - Epoch [14/1000], Validation Step [600/1090], Val Loss: 1.0015
2025-02-18 10:22:29,106 - Epoch [14/1000], Validation Step [700/1090], Val Loss: 0.7107
2025-02-18 10:22:36,097 - Epoch [14/1000], Validation Step [800/1090], Val Loss: 0.1131
2025-02-18 10:22:42,921 - Epoch [14/1000], Validation Step [900/1090], Val Loss: 0.2958
2025-02-18 10:22:50,290 - Epoch [14/1000], Validation Step [1000/1090], Val Loss: 0.3348
2025-02-18 10:22:57,175 - Epoch 14/1000, Train Loss: 0.5314, Val Loss: 0.5750, Accuracy: 79.15%
2025-02-18 10:22:57,691 - Model saved at /data2/personal/sungjin/korean_dialects/classification_second_checkpoint/checkpoint_epoch_14.pth
2025-02-18 10:23:25,900 - Epoch [15/1000], Step [100/4367], Loss: 0.2997
2025-02-18 10:23:51,733 - Epoch [15/1000], Step [200/4367], Loss: 0.6926
2025-02-18 10:24:17,250 - Epoch [15/1000], Step [300/4367], Loss: 1.0076
2025-02-18 10:24:43,338 - Epoch [15/1000], Step [400/4367], Loss: 0.6363
2025-02-18 10:25:09,278 - Epoch [15/1000], Step [500/4367], Loss: 0.5391
2025-02-18 10:25:35,061 - Epoch [15/1000], Step [600/4367], Loss: 0.5967
2025-02-18 10:26:00,816 - Epoch [15/1000], Step [700/4367], Loss: 0.8517
2025-02-18 10:26:26,541 - Epoch [15/1000], Step [800/4367], Loss: 0.5839
2025-02-18 10:26:52,513 - Epoch [15/1000], Step [900/4367], Loss: 0.5878
2025-02-18 10:27:18,368 - Epoch [15/1000], Step [1000/4367], Loss: 0.5960
2025-02-18 10:27:44,293 - Epoch [15/1000], Step [1100/4367], Loss: 0.2682
2025-02-18 10:28:10,290 - Epoch [15/1000], Step [1200/4367], Loss: 0.5005
2025-02-18 10:28:36,288 - Epoch [15/1000], Step [1300/4367], Loss: 0.5504
2025-02-18 10:29:02,163 - Epoch [15/1000], Step [1400/4367], Loss: 0.5277
2025-02-18 10:29:27,923 - Epoch [15/1000], Step [1500/4367], Loss: 0.3622
2025-02-18 10:29:54,195 - Epoch [15/1000], Step [1600/4367], Loss: 0.8153
2025-02-18 10:30:20,006 - Epoch [15/1000], Step [1700/4367], Loss: 0.4965
2025-02-18 10:30:45,731 - Epoch [15/1000], Step [1800/4367], Loss: 0.5814
2025-02-18 10:31:11,901 - Epoch [15/1000], Step [1900/4367], Loss: 0.5730
2025-02-18 10:31:37,722 - Epoch [15/1000], Step [2000/4367], Loss: 0.6204
2025-02-18 10:32:03,782 - Epoch [15/1000], Step [2100/4367], Loss: 0.2770
2025-02-18 10:32:29,994 - Epoch [15/1000], Step [2200/4367], Loss: 0.4633
2025-02-18 10:32:55,970 - Epoch [15/1000], Step [2300/4367], Loss: 0.3234
2025-02-18 10:33:21,969 - Epoch [15/1000], Step [2400/4367], Loss: 0.3964
2025-02-18 10:33:47,651 - Epoch [15/1000], Step [2500/4367], Loss: 0.6725
2025-02-18 10:34:13,730 - Epoch [15/1000], Step [2600/4367], Loss: 0.5286
2025-02-18 10:34:39,930 - Epoch [15/1000], Step [2700/4367], Loss: 0.9344
2025-02-18 10:35:05,902 - Epoch [15/1000], Step [2800/4367], Loss: 0.3198
2025-02-18 10:35:32,193 - Epoch [15/1000], Step [2900/4367], Loss: 0.3935
2025-02-18 10:35:57,901 - Epoch [15/1000], Step [3000/4367], Loss: 0.3742
2025-02-18 10:36:23,739 - Epoch [15/1000], Step [3100/4367], Loss: 0.3692
2025-02-18 10:36:49,862 - Epoch [15/1000], Step [3200/4367], Loss: 0.4063
2025-02-18 10:37:15,734 - Epoch [15/1000], Step [3300/4367], Loss: 0.4542
2025-02-18 10:37:41,549 - Epoch [15/1000], Step [3400/4367], Loss: 0.7502
2025-02-18 10:38:07,351 - Epoch [15/1000], Step [3500/4367], Loss: 0.5201
2025-02-18 10:38:33,101 - Epoch [15/1000], Step [3600/4367], Loss: 0.5906
2025-02-18 10:38:59,046 - Epoch [15/1000], Step [3700/4367], Loss: 0.2689
2025-02-18 10:39:24,871 - Epoch [15/1000], Step [3800/4367], Loss: 0.3909
2025-02-18 10:39:50,737 - Epoch [15/1000], Step [3900/4367], Loss: 0.6118
2025-02-18 10:40:16,619 - Epoch [15/1000], Step [4000/4367], Loss: 0.6336
2025-02-18 10:40:42,832 - Epoch [15/1000], Step [4100/4367], Loss: 0.3082
2025-02-18 10:41:09,027 - Epoch [15/1000], Step [4200/4367], Loss: 0.5136
2025-02-18 10:41:34,830 - Epoch [15/1000], Step [4300/4367], Loss: 0.4967
2025-02-18 10:42:01,638 - Epoch [15/1000], Validation Step [100/1090], Val Loss: 0.0005
2025-02-18 10:42:08,967 - Epoch [15/1000], Validation Step [200/1090], Val Loss: 0.0002
2025-02-18 10:42:16,405 - Epoch [15/1000], Validation Step [300/1090], Val Loss: 0.9216
2025-02-18 10:42:24,084 - Epoch [15/1000], Validation Step [400/1090], Val Loss: 3.6688
2025-02-18 10:42:31,336 - Epoch [15/1000], Validation Step [500/1090], Val Loss: 2.0852
2025-02-18 10:42:38,935 - Epoch [15/1000], Validation Step [600/1090], Val Loss: 0.2544
2025-02-18 10:42:46,568 - Epoch [15/1000], Validation Step [700/1090], Val Loss: 0.1238
2025-02-18 10:42:53,575 - Epoch [15/1000], Validation Step [800/1090], Val Loss: 0.4709
2025-02-18 10:43:00,420 - Epoch [15/1000], Validation Step [900/1090], Val Loss: 0.7551
2025-02-18 10:43:07,822 - Epoch [15/1000], Validation Step [1000/1090], Val Loss: 0.5699
2025-02-18 10:43:14,733 - Epoch 15/1000, Train Loss: 0.5184, Val Loss: 0.8582, Accuracy: 71.28%
2025-02-18 10:43:42,961 - Epoch [16/1000], Step [100/4367], Loss: 0.5221
2025-02-18 10:44:08,995 - Epoch [16/1000], Step [200/4367], Loss: 0.4852
2025-02-18 10:44:34,968 - Epoch [16/1000], Step [300/4367], Loss: 0.4065
2025-02-18 10:45:00,969 - Epoch [16/1000], Step [400/4367], Loss: 0.4450
2025-02-18 10:45:27,258 - Epoch [16/1000], Step [500/4367], Loss: 0.3151
2025-02-18 10:45:53,018 - Epoch [16/1000], Step [600/4367], Loss: 0.5630
2025-02-18 10:46:18,908 - Epoch [16/1000], Step [700/4367], Loss: 0.6764
2025-02-18 10:46:44,859 - Epoch [16/1000], Step [800/4367], Loss: 0.6468
2025-02-18 10:47:10,649 - Epoch [16/1000], Step [900/4367], Loss: 0.3533
2025-02-18 10:47:36,451 - Epoch [16/1000], Step [1000/4367], Loss: 0.5799
2025-02-18 10:48:02,476 - Epoch [16/1000], Step [1100/4367], Loss: 0.6444
2025-02-18 10:48:28,051 - Epoch [16/1000], Step [1200/4367], Loss: 0.7050
2025-02-18 10:48:54,314 - Epoch [16/1000], Step [1300/4367], Loss: 0.4992
2025-02-18 10:49:20,277 - Epoch [16/1000], Step [1400/4367], Loss: 0.6398
2025-02-18 10:49:46,283 - Epoch [16/1000], Step [1500/4367], Loss: 0.7021
2025-02-18 10:50:12,175 - Epoch [16/1000], Step [1600/4367], Loss: 0.5688
2025-02-18 10:50:38,368 - Epoch [16/1000], Step [1700/4367], Loss: 0.7333
2025-02-18 10:51:04,080 - Epoch [16/1000], Step [1800/4367], Loss: 0.6801
2025-02-18 10:51:30,113 - Epoch [16/1000], Step [1900/4367], Loss: 0.6278
2025-02-18 10:51:55,972 - Epoch [16/1000], Step [2000/4367], Loss: 0.8004
2025-02-18 10:52:22,054 - Epoch [16/1000], Step [2100/4367], Loss: 0.6332
2025-02-18 10:52:47,935 - Epoch [16/1000], Step [2200/4367], Loss: 0.7511
2025-02-18 10:53:13,720 - Epoch [16/1000], Step [2300/4367], Loss: 0.6639
2025-02-18 10:53:39,918 - Epoch [16/1000], Step [2400/4367], Loss: 0.5168
2025-02-18 10:54:05,663 - Epoch [16/1000], Step [2500/4367], Loss: 0.5926
2025-02-18 10:54:31,619 - Epoch [16/1000], Step [2600/4367], Loss: 0.4410
2025-02-18 10:54:57,443 - Epoch [16/1000], Step [2700/4367], Loss: 0.5437
2025-02-18 10:55:23,448 - Epoch [16/1000], Step [2800/4367], Loss: 0.4142
2025-02-18 10:55:49,061 - Epoch [16/1000], Step [2900/4367], Loss: 0.4010
2025-02-18 10:56:15,147 - Epoch [16/1000], Step [3000/4367], Loss: 0.8477
2025-02-18 10:56:41,545 - Epoch [16/1000], Step [3100/4367], Loss: 0.4480
2025-02-18 10:57:07,448 - Epoch [16/1000], Step [3200/4367], Loss: 0.3935
2025-02-18 10:57:33,298 - Epoch [16/1000], Step [3300/4367], Loss: 0.7475
2025-02-18 10:57:59,240 - Epoch [16/1000], Step [3400/4367], Loss: 0.1840
2025-02-18 10:58:25,452 - Epoch [16/1000], Step [3500/4367], Loss: 0.4506
2025-02-18 10:58:51,384 - Epoch [16/1000], Step [3600/4367], Loss: 0.5281
2025-02-18 10:59:17,245 - Epoch [16/1000], Step [3700/4367], Loss: 0.6535
2025-02-18 10:59:42,956 - Epoch [16/1000], Step [3800/4367], Loss: 0.3301
2025-02-18 11:00:08,730 - Epoch [16/1000], Step [3900/4367], Loss: 0.3813
2025-02-18 11:00:34,726 - Epoch [16/1000], Step [4000/4367], Loss: 0.4011
2025-02-18 11:01:00,780 - Epoch [16/1000], Step [4100/4367], Loss: 0.5625
2025-02-18 11:01:26,992 - Epoch [16/1000], Step [4200/4367], Loss: 0.4463
2025-02-18 11:01:53,107 - Epoch [16/1000], Step [4300/4367], Loss: 0.4761
2025-02-18 11:02:19,866 - Epoch [16/1000], Validation Step [100/1090], Val Loss: 0.0436
2025-02-18 11:02:27,209 - Epoch [16/1000], Validation Step [200/1090], Val Loss: 0.0028
2025-02-18 11:02:34,656 - Epoch [16/1000], Validation Step [300/1090], Val Loss: 0.6207
2025-02-18 11:02:42,379 - Epoch [16/1000], Validation Step [400/1090], Val Loss: 1.2395
2025-02-18 11:02:49,682 - Epoch [16/1000], Validation Step [500/1090], Val Loss: 0.8404
2025-02-18 11:02:57,268 - Epoch [16/1000], Validation Step [600/1090], Val Loss: 0.3456
2025-02-18 11:03:04,891 - Epoch [16/1000], Validation Step [700/1090], Val Loss: 0.2941
2025-02-18 11:03:11,889 - Epoch [16/1000], Validation Step [800/1090], Val Loss: 0.5976
2025-02-18 11:03:18,731 - Epoch [16/1000], Validation Step [900/1090], Val Loss: 0.8519
2025-02-18 11:03:26,121 - Epoch [16/1000], Validation Step [1000/1090], Val Loss: 0.9836
2025-02-18 11:03:33,020 - Epoch 16/1000, Train Loss: 0.5316, Val Loss: 0.6842, Accuracy: 75.13%
2025-02-18 11:03:33,633 - Model saved at /data2/personal/sungjin/korean_dialects/classification_second_checkpoint/checkpoint_epoch_16.pth
2025-02-18 11:04:01,589 - Epoch [17/1000], Step [100/4367], Loss: 0.3516
2025-02-18 11:04:27,584 - Epoch [17/1000], Step [200/4367], Loss: 0.3433
2025-02-18 11:04:53,298 - Epoch [17/1000], Step [300/4367], Loss: 0.8557
2025-02-18 11:05:18,988 - Epoch [17/1000], Step [400/4367], Loss: 0.6981
2025-02-18 11:05:44,859 - Epoch [17/1000], Step [500/4367], Loss: 0.5571
2025-02-18 11:06:11,025 - Epoch [17/1000], Step [600/4367], Loss: 0.4531
2025-02-18 11:06:37,236 - Epoch [17/1000], Step [700/4367], Loss: 0.4668
2025-02-18 11:07:03,238 - Epoch [17/1000], Step [800/4367], Loss: 0.4972
2025-02-18 11:07:29,205 - Epoch [17/1000], Step [900/4367], Loss: 0.8028
2025-02-18 11:07:55,510 - Epoch [17/1000], Step [1000/4367], Loss: 0.3856
2025-02-18 11:08:21,632 - Epoch [17/1000], Step [1100/4367], Loss: 0.5665
2025-02-18 11:08:47,246 - Epoch [17/1000], Step [1200/4367], Loss: 0.6458
2025-02-18 11:09:13,104 - Epoch [17/1000], Step [1300/4367], Loss: 0.4007
2025-02-18 11:09:39,258 - Epoch [17/1000], Step [1400/4367], Loss: 0.6368
2025-02-18 11:10:05,044 - Epoch [17/1000], Step [1500/4367], Loss: 0.6667
2025-02-18 11:10:31,059 - Epoch [17/1000], Step [1600/4367], Loss: 0.3464
2025-02-18 11:10:56,803 - Epoch [17/1000], Step [1700/4367], Loss: 0.9572
2025-02-18 11:11:22,944 - Epoch [17/1000], Step [1800/4367], Loss: 0.4494
2025-02-18 11:11:48,949 - Epoch [17/1000], Step [1900/4367], Loss: 0.7286
2025-02-18 11:12:15,086 - Epoch [17/1000], Step [2000/4367], Loss: 0.5772
2025-02-18 11:12:41,008 - Epoch [17/1000], Step [2100/4367], Loss: 0.2852
2025-02-18 11:13:07,066 - Epoch [17/1000], Step [2200/4367], Loss: 0.5039
2025-02-18 11:13:32,907 - Epoch [17/1000], Step [2300/4367], Loss: 0.4395
2025-02-18 11:13:58,750 - Epoch [17/1000], Step [2400/4367], Loss: 0.4141
2025-02-18 11:14:24,766 - Epoch [17/1000], Step [2500/4367], Loss: 0.5740
2025-02-18 11:14:50,585 - Epoch [17/1000], Step [2600/4367], Loss: 0.4318
2025-02-18 11:15:16,244 - Epoch [17/1000], Step [2700/4367], Loss: 0.4635
2025-02-18 11:15:42,089 - Epoch [17/1000], Step [2800/4367], Loss: 0.4931
2025-02-18 11:16:07,978 - Epoch [17/1000], Step [2900/4367], Loss: 0.3225
2025-02-18 11:16:33,966 - Epoch [17/1000], Step [3000/4367], Loss: 0.6115
2025-02-18 11:17:00,146 - Epoch [17/1000], Step [3100/4367], Loss: 0.6643
2025-02-18 11:17:25,934 - Epoch [17/1000], Step [3200/4367], Loss: 0.4316
2025-02-18 11:17:51,813 - Epoch [17/1000], Step [3300/4367], Loss: 0.5816
2025-02-18 11:18:17,613 - Epoch [17/1000], Step [3400/4367], Loss: 0.9256
2025-02-18 11:18:43,510 - Epoch [17/1000], Step [3500/4367], Loss: 0.4817
2025-02-18 11:19:09,018 - Epoch [17/1000], Step [3600/4367], Loss: 0.5134
2025-02-18 11:19:34,673 - Epoch [17/1000], Step [3700/4367], Loss: 0.3908
2025-02-18 11:20:00,727 - Epoch [17/1000], Step [3800/4367], Loss: 0.3701
2025-02-18 11:20:26,391 - Epoch [17/1000], Step [3900/4367], Loss: 0.6182
2025-02-18 11:20:52,161 - Epoch [17/1000], Step [4000/4367], Loss: 0.4105
2025-02-18 11:21:17,792 - Epoch [17/1000], Step [4100/4367], Loss: 0.4795
2025-02-18 11:21:43,546 - Epoch [17/1000], Step [4200/4367], Loss: 0.9191
2025-02-18 11:22:09,458 - Epoch [17/1000], Step [4300/4367], Loss: 0.7487
2025-02-18 11:22:36,050 - Epoch [17/1000], Validation Step [100/1090], Val Loss: 0.0000
2025-02-18 11:22:43,353 - Epoch [17/1000], Validation Step [200/1090], Val Loss: 0.0000
2025-02-18 11:22:50,741 - Epoch [17/1000], Validation Step [300/1090], Val Loss: 1.0244
2025-02-18 11:22:58,368 - Epoch [17/1000], Validation Step [400/1090], Val Loss: 1.7349
2025-02-18 11:23:05,552 - Epoch [17/1000], Validation Step [500/1090], Val Loss: 0.7121
2025-02-18 11:23:13,070 - Epoch [17/1000], Validation Step [600/1090], Val Loss: 0.3735
2025-02-18 11:23:20,636 - Epoch [17/1000], Validation Step [700/1090], Val Loss: 0.2550
2025-02-18 11:23:27,594 - Epoch [17/1000], Validation Step [800/1090], Val Loss: 0.4413
2025-02-18 11:23:34,381 - Epoch [17/1000], Validation Step [900/1090], Val Loss: 0.6852
2025-02-18 11:23:41,685 - Epoch [17/1000], Validation Step [1000/1090], Val Loss: 0.7379
2025-02-18 11:23:48,542 - Epoch 17/1000, Train Loss: 0.5145, Val Loss: 0.6153, Accuracy: 78.01%
2025-02-18 11:24:16,368 - Epoch [18/1000], Step [100/4367], Loss: 0.4758
2025-02-18 11:24:42,115 - Epoch [18/1000], Step [200/4367], Loss: 0.6323
2025-02-18 11:25:07,596 - Epoch [18/1000], Step [300/4367], Loss: 0.2454
2025-02-18 11:25:33,417 - Epoch [18/1000], Step [400/4367], Loss: 0.4350
2025-02-18 11:25:59,198 - Epoch [18/1000], Step [500/4367], Loss: 0.9969
2025-02-18 11:26:24,835 - Epoch [18/1000], Step [600/4367], Loss: 0.5676
2025-02-18 11:26:50,727 - Epoch [18/1000], Step [700/4367], Loss: 0.3827
2025-02-18 11:27:16,760 - Epoch [18/1000], Step [800/4367], Loss: 1.3388
2025-02-18 11:27:42,654 - Epoch [18/1000], Step [900/4367], Loss: 0.3183
2025-02-18 11:28:08,594 - Epoch [18/1000], Step [1000/4367], Loss: 0.6735
2025-02-18 11:28:34,790 - Epoch [18/1000], Step [1100/4367], Loss: 0.7637
2025-02-18 11:29:00,588 - Epoch [18/1000], Step [1200/4367], Loss: 0.5916
2025-02-18 11:29:26,877 - Epoch [18/1000], Step [1300/4367], Loss: 0.4625
2025-02-18 11:29:54,261 - Epoch [18/1000], Step [1400/4367], Loss: 0.3739
2025-02-18 11:30:21,240 - Epoch [18/1000], Step [1500/4367], Loss: 0.5172
2025-02-18 11:30:47,416 - Epoch [18/1000], Step [1600/4367], Loss: 0.6347
2025-02-18 11:31:13,324 - Epoch [18/1000], Step [1700/4367], Loss: 0.4762
2025-02-18 11:31:41,135 - Epoch [18/1000], Step [1800/4367], Loss: 0.2341
2025-02-18 11:32:07,411 - Epoch [18/1000], Step [1900/4367], Loss: 0.6664
2025-02-18 11:32:33,571 - Epoch [18/1000], Step [2000/4367], Loss: 0.3858
2025-02-18 11:33:00,061 - Epoch [18/1000], Step [2100/4367], Loss: 0.3996
2025-02-18 11:33:26,056 - Epoch [18/1000], Step [2200/4367], Loss: 0.4978
2025-02-18 11:33:51,825 - Epoch [18/1000], Step [2300/4367], Loss: 0.2400
2025-02-18 11:34:17,876 - Epoch [18/1000], Step [2400/4367], Loss: 0.5149
2025-02-18 11:34:43,699 - Epoch [18/1000], Step [2500/4367], Loss: 0.3942
2025-02-18 11:35:09,504 - Epoch [18/1000], Step [2600/4367], Loss: 0.3687
2025-02-18 11:35:35,801 - Epoch [18/1000], Step [2700/4367], Loss: 0.6626
2025-02-18 11:36:01,388 - Epoch [18/1000], Step [2800/4367], Loss: 0.7370
2025-02-18 11:36:27,404 - Epoch [18/1000], Step [2900/4367], Loss: 0.4025
2025-02-18 11:36:53,462 - Epoch [18/1000], Step [3000/4367], Loss: 0.4514
2025-02-18 11:37:19,368 - Epoch [18/1000], Step [3100/4367], Loss: 0.6316
2025-02-18 11:37:45,503 - Epoch [18/1000], Step [3200/4367], Loss: 0.5945
2025-02-18 11:38:11,685 - Epoch [18/1000], Step [3300/4367], Loss: 0.5442
2025-02-18 11:38:37,858 - Epoch [18/1000], Step [3400/4367], Loss: 0.6025
2025-02-18 11:39:03,894 - Epoch [18/1000], Step [3500/4367], Loss: 0.3879
2025-02-18 11:39:29,740 - Epoch [18/1000], Step [3600/4367], Loss: 0.2634
2025-02-18 11:39:55,518 - Epoch [18/1000], Step [3700/4367], Loss: 0.6302
2025-02-18 11:40:21,620 - Epoch [18/1000], Step [3800/4367], Loss: 0.3941
2025-02-18 11:40:47,691 - Epoch [18/1000], Step [3900/4367], Loss: 0.6676
2025-02-18 11:41:13,897 - Epoch [18/1000], Step [4000/4367], Loss: 0.2611
2025-02-18 11:41:39,899 - Epoch [18/1000], Step [4100/4367], Loss: 0.2646
2025-02-18 11:42:05,961 - Epoch [18/1000], Step [4200/4367], Loss: 0.4432
2025-02-18 11:42:32,332 - Epoch [18/1000], Step [4300/4367], Loss: 0.4558
2025-02-18 11:42:58,823 - Epoch [18/1000], Validation Step [100/1090], Val Loss: 0.0003
2025-02-18 11:43:06,163 - Epoch [18/1000], Validation Step [200/1090], Val Loss: 0.0001
2025-02-18 11:43:13,600 - Epoch [18/1000], Validation Step [300/1090], Val Loss: 0.9082
2025-02-18 11:43:21,297 - Epoch [18/1000], Validation Step [400/1090], Val Loss: 0.2646
2025-02-18 11:43:28,550 - Epoch [18/1000], Validation Step [500/1090], Val Loss: 0.2985
2025-02-18 11:43:36,121 - Epoch [18/1000], Validation Step [600/1090], Val Loss: 0.5569
2025-02-18 11:43:43,747 - Epoch [18/1000], Validation Step [700/1090], Val Loss: 0.4482
2025-02-18 11:43:50,773 - Epoch [18/1000], Validation Step [800/1090], Val Loss: 0.6035
2025-02-18 11:43:57,621 - Epoch [18/1000], Validation Step [900/1090], Val Loss: 0.7198
2025-02-18 11:44:05,015 - Epoch [18/1000], Validation Step [1000/1090], Val Loss: 0.4772
2025-02-18 11:44:11,919 - Epoch 18/1000, Train Loss: 0.4870, Val Loss: 0.5488, Accuracy: 79.72%
2025-02-18 11:44:12,400 - Model saved at /data2/personal/sungjin/korean_dialects/classification_second_checkpoint/checkpoint_epoch_18.pth
2025-02-18 11:44:40,470 - Epoch [19/1000], Step [100/4367], Loss: 0.7193
2025-02-18 11:45:06,177 - Epoch [19/1000], Step [200/4367], Loss: 0.4027
2025-02-18 11:45:32,395 - Epoch [19/1000], Step [300/4367], Loss: 0.5281
2025-02-18 11:45:58,241 - Epoch [19/1000], Step [400/4367], Loss: 0.2279
2025-02-18 11:46:24,092 - Epoch [19/1000], Step [500/4367], Loss: 0.6144
2025-02-18 11:46:50,316 - Epoch [19/1000], Step [600/4367], Loss: 0.6204
2025-02-18 11:47:16,183 - Epoch [19/1000], Step [700/4367], Loss: 0.6367
2025-02-18 11:47:41,926 - Epoch [19/1000], Step [800/4367], Loss: 0.3919
2025-02-18 11:48:07,572 - Epoch [19/1000], Step [900/4367], Loss: 0.6774
2025-02-18 11:48:33,497 - Epoch [19/1000], Step [1000/4367], Loss: 0.3636
2025-02-18 11:48:59,249 - Epoch [19/1000], Step [1100/4367], Loss: 0.7572
2025-02-18 11:49:25,733 - Epoch [19/1000], Step [1200/4367], Loss: 0.4942
2025-02-18 11:49:51,232 - Epoch [19/1000], Step [1300/4367], Loss: 0.4369
2025-02-18 11:50:17,259 - Epoch [19/1000], Step [1400/4367], Loss: 0.4743
2025-02-18 11:50:43,421 - Epoch [19/1000], Step [1500/4367], Loss: 0.4277
2025-02-18 11:51:09,079 - Epoch [19/1000], Step [1600/4367], Loss: 0.5214
2025-02-18 11:51:35,162 - Epoch [19/1000], Step [1700/4367], Loss: 0.6713
2025-02-18 11:52:01,103 - Epoch [19/1000], Step [1800/4367], Loss: 0.6264
2025-02-18 11:52:27,153 - Epoch [19/1000], Step [1900/4367], Loss: 0.5389
2025-02-18 11:52:53,145 - Epoch [19/1000], Step [2000/4367], Loss: 0.7129
2025-02-18 11:53:19,317 - Epoch [19/1000], Step [2100/4367], Loss: 0.3775
2025-02-18 11:53:45,212 - Epoch [19/1000], Step [2200/4367], Loss: 0.5220
2025-02-18 11:54:11,136 - Epoch [19/1000], Step [2300/4367], Loss: 0.4289
2025-02-18 11:54:36,934 - Epoch [19/1000], Step [2400/4367], Loss: 0.6660
2025-02-18 11:55:03,078 - Epoch [19/1000], Step [2500/4367], Loss: 0.5646
2025-02-18 11:55:28,866 - Epoch [19/1000], Step [2600/4367], Loss: 0.2276
2025-02-18 11:55:54,772 - Epoch [19/1000], Step [2700/4367], Loss: 0.7772
2025-02-18 11:56:20,563 - Epoch [19/1000], Step [2800/4367], Loss: 0.2546
2025-02-18 11:56:46,722 - Epoch [19/1000], Step [2900/4367], Loss: 0.4207
2025-02-18 11:57:12,764 - Epoch [19/1000], Step [3000/4367], Loss: 0.2352
2025-02-18 11:57:39,039 - Epoch [19/1000], Step [3100/4367], Loss: 0.3647
2025-02-18 11:58:11,206 - Epoch [19/1000], Step [3200/4367], Loss: 0.3075
2025-02-18 11:58:37,070 - Epoch [19/1000], Step [3300/4367], Loss: 0.1701
2025-02-18 11:59:03,017 - Epoch [19/1000], Step [3400/4367], Loss: 0.2626
2025-02-18 11:59:28,805 - Epoch [19/1000], Step [3500/4367], Loss: 0.4663
2025-02-18 11:59:54,759 - Epoch [19/1000], Step [3600/4367], Loss: 0.3131
2025-02-18 12:00:20,714 - Epoch [19/1000], Step [3700/4367], Loss: 0.5768
2025-02-18 12:00:46,744 - Epoch [19/1000], Step [3800/4367], Loss: 0.3093
2025-02-18 12:01:12,565 - Epoch [19/1000], Step [3900/4367], Loss: 0.5383
2025-02-18 12:01:38,551 - Epoch [19/1000], Step [4000/4367], Loss: 0.4123
2025-02-18 12:02:04,763 - Epoch [19/1000], Step [4100/4367], Loss: 0.3269
2025-02-18 12:02:30,348 - Epoch [19/1000], Step [4200/4367], Loss: 0.5291
2025-02-18 12:02:55,980 - Epoch [19/1000], Step [4300/4367], Loss: 0.6040
2025-02-18 12:03:22,329 - Epoch [19/1000], Validation Step [100/1090], Val Loss: 0.0003
2025-02-18 12:03:29,628 - Epoch [19/1000], Validation Step [200/1090], Val Loss: 0.0000
2025-02-18 12:03:37,025 - Epoch [19/1000], Validation Step [300/1090], Val Loss: 0.8747
2025-02-18 12:03:44,684 - Epoch [19/1000], Validation Step [400/1090], Val Loss: 0.1200
2025-02-18 12:03:51,890 - Epoch [19/1000], Validation Step [500/1090], Val Loss: 0.1723
2025-02-18 12:03:59,426 - Epoch [19/1000], Validation Step [600/1090], Val Loss: 0.7245
2025-02-18 12:04:06,996 - Epoch [19/1000], Validation Step [700/1090], Val Loss: 0.5695
2025-02-18 12:04:13,972 - Epoch [19/1000], Validation Step [800/1090], Val Loss: 0.8916
2025-02-18 12:04:20,776 - Epoch [19/1000], Validation Step [900/1090], Val Loss: 1.1666
2025-02-18 12:04:28,126 - Epoch [19/1000], Validation Step [1000/1090], Val Loss: 0.1877
2025-02-18 12:04:35,019 - Epoch 19/1000, Train Loss: 0.4395, Val Loss: 0.5718, Accuracy: 79.12%
2025-02-18 12:05:02,975 - Epoch [20/1000], Step [100/4367], Loss: 0.7234
2025-02-18 12:05:28,581 - Epoch [20/1000], Step [200/4367], Loss: 0.2754
2025-02-18 12:05:54,407 - Epoch [20/1000], Step [300/4367], Loss: 0.3742
2025-02-18 12:06:20,105 - Epoch [20/1000], Step [400/4367], Loss: 0.2862
2025-02-18 12:06:45,720 - Epoch [20/1000], Step [500/4367], Loss: 0.3915
2025-02-18 12:07:11,559 - Epoch [20/1000], Step [600/4367], Loss: 0.4206
2025-02-18 12:07:37,394 - Epoch [20/1000], Step [700/4367], Loss: 0.2997
2025-02-18 12:08:03,008 - Epoch [20/1000], Step [800/4367], Loss: 0.4888
2025-02-18 12:08:28,958 - Epoch [20/1000], Step [900/4367], Loss: 0.2104
2025-02-18 12:08:55,097 - Epoch [20/1000], Step [1000/4367], Loss: 0.4723
2025-02-18 12:09:20,999 - Epoch [20/1000], Step [1100/4367], Loss: 0.6189
2025-02-18 12:09:47,220 - Epoch [20/1000], Step [1200/4367], Loss: 0.4567
2025-02-18 12:10:13,323 - Epoch [20/1000], Step [1300/4367], Loss: 0.1824
2025-02-18 12:10:39,145 - Epoch [20/1000], Step [1400/4367], Loss: 0.3031
2025-02-18 12:11:05,244 - Epoch [20/1000], Step [1500/4367], Loss: 0.5391
2025-02-18 12:11:31,416 - Epoch [20/1000], Step [1600/4367], Loss: 0.3103
2025-02-18 12:11:57,008 - Epoch [20/1000], Step [1700/4367], Loss: 0.4938
2025-02-18 12:12:23,211 - Epoch [20/1000], Step [1800/4367], Loss: 0.2326
2025-02-18 12:12:49,057 - Epoch [20/1000], Step [1900/4367], Loss: 0.4428
2025-02-18 12:13:14,720 - Epoch [20/1000], Step [2000/4367], Loss: 0.3672
2025-02-18 12:13:40,491 - Epoch [20/1000], Step [2100/4367], Loss: 0.2285
2025-02-18 12:14:06,582 - Epoch [20/1000], Step [2200/4367], Loss: 0.4235
2025-02-18 12:14:32,130 - Epoch [20/1000], Step [2300/4367], Loss: 0.2474
2025-02-18 12:14:58,058 - Epoch [20/1000], Step [2400/4367], Loss: 0.5674
2025-02-18 12:15:24,161 - Epoch [20/1000], Step [2500/4367], Loss: 0.5719
2025-02-18 12:15:50,099 - Epoch [20/1000], Step [2600/4367], Loss: 0.4365
2025-02-18 12:16:15,839 - Epoch [20/1000], Step [2700/4367], Loss: 0.4120
2025-02-18 12:16:41,810 - Epoch [20/1000], Step [2800/4367], Loss: 0.2966
2025-02-18 12:17:07,461 - Epoch [20/1000], Step [2900/4367], Loss: 0.1713
2025-02-18 12:17:33,404 - Epoch [20/1000], Step [3000/4367], Loss: 0.2489
2025-02-18 12:17:59,172 - Epoch [20/1000], Step [3100/4367], Loss: 0.2836
2025-02-18 12:18:25,233 - Epoch [20/1000], Step [3200/4367], Loss: 0.3996
2025-02-18 12:18:51,453 - Epoch [20/1000], Step [3300/4367], Loss: 0.3214
2025-02-18 12:19:17,576 - Epoch [20/1000], Step [3400/4367], Loss: 0.4514
2025-02-18 12:19:43,747 - Epoch [20/1000], Step [3500/4367], Loss: 0.3887
2025-02-18 12:20:09,821 - Epoch [20/1000], Step [3600/4367], Loss: 0.4466
2025-02-18 12:20:35,404 - Epoch [20/1000], Step [3700/4367], Loss: 0.5018
2025-02-18 12:21:01,298 - Epoch [20/1000], Step [3800/4367], Loss: 0.3540
2025-02-18 12:21:27,282 - Epoch [20/1000], Step [3900/4367], Loss: 0.5222
2025-02-18 12:21:53,216 - Epoch [20/1000], Step [4000/4367], Loss: 0.4039
2025-02-18 12:22:19,215 - Epoch [20/1000], Step [4100/4367], Loss: 0.7973
2025-02-18 12:22:45,035 - Epoch [20/1000], Step [4200/4367], Loss: 0.3130
2025-02-18 12:23:10,910 - Epoch [20/1000], Step [4300/4367], Loss: 0.6099
2025-02-18 12:23:37,649 - Epoch [20/1000], Validation Step [100/1090], Val Loss: 0.0007
2025-02-18 12:23:44,995 - Epoch [20/1000], Validation Step [200/1090], Val Loss: 0.0000
2025-02-18 12:23:52,420 - Epoch [20/1000], Validation Step [300/1090], Val Loss: 1.2717
2025-02-18 12:24:00,102 - Epoch [20/1000], Validation Step [400/1090], Val Loss: 0.3233
2025-02-18 12:24:07,349 - Epoch [20/1000], Validation Step [500/1090], Val Loss: 0.8219
2025-02-18 12:24:14,921 - Epoch [20/1000], Validation Step [600/1090], Val Loss: 1.0545
2025-02-18 12:24:22,552 - Epoch [20/1000], Validation Step [700/1090], Val Loss: 0.9177
2025-02-18 12:24:29,601 - Epoch [20/1000], Validation Step [800/1090], Val Loss: 0.2297
2025-02-18 12:24:36,435 - Epoch [20/1000], Validation Step [900/1090], Val Loss: 0.4714
2025-02-18 12:24:43,808 - Epoch [20/1000], Validation Step [1000/1090], Val Loss: 0.0840
2025-02-18 12:24:50,683 - Epoch 20/1000, Train Loss: 0.4367, Val Loss: 0.5854, Accuracy: 79.28%
2025-02-18 12:24:51,204 - Model saved at /data2/personal/sungjin/korean_dialects/classification_second_checkpoint/checkpoint_epoch_20.pth
2025-02-18 12:25:18,612 - Epoch [21/1000], Step [100/4367], Loss: 0.4510
2025-02-18 12:25:44,709 - Epoch [21/1000], Step [200/4367], Loss: 0.5156
2025-02-18 12:26:10,623 - Epoch [21/1000], Step [300/4367], Loss: 0.2950
2025-02-18 12:26:36,313 - Epoch [21/1000], Step [400/4367], Loss: 0.2977
2025-02-18 12:27:02,015 - Epoch [21/1000], Step [500/4367], Loss: 0.3755
2025-02-18 12:27:28,048 - Epoch [21/1000], Step [600/4367], Loss: 1.6359
2025-02-18 12:27:54,127 - Epoch [21/1000], Step [700/4367], Loss: 0.8968
2025-02-18 12:28:19,949 - Epoch [21/1000], Step [800/4367], Loss: 0.7335
2025-02-18 12:28:46,176 - Epoch [21/1000], Step [900/4367], Loss: 0.6491
2025-02-18 12:29:12,245 - Epoch [21/1000], Step [1000/4367], Loss: 0.6425
2025-02-18 12:29:38,094 - Epoch [21/1000], Step [1100/4367], Loss: 0.6554
2025-02-18 12:30:03,909 - Epoch [21/1000], Step [1200/4367], Loss: 0.6277
2025-02-18 12:30:29,589 - Epoch [21/1000], Step [1300/4367], Loss: 0.9750
2025-02-18 12:30:55,386 - Epoch [21/1000], Step [1400/4367], Loss: 0.6855
2025-02-18 12:31:21,203 - Epoch [21/1000], Step [1500/4367], Loss: 1.0451
2025-02-18 12:31:47,024 - Epoch [21/1000], Step [1600/4367], Loss: 0.5955
2025-02-18 12:32:12,683 - Epoch [21/1000], Step [1700/4367], Loss: 0.4232
2025-02-18 12:32:38,384 - Epoch [21/1000], Step [1800/4367], Loss: 0.6264
2025-02-18 12:33:04,212 - Epoch [21/1000], Step [1900/4367], Loss: 0.4754
2025-02-18 12:33:30,040 - Epoch [21/1000], Step [2000/4367], Loss: 0.6256
2025-02-18 12:33:56,088 - Epoch [21/1000], Step [2100/4367], Loss: 0.7390
2025-02-18 12:34:21,968 - Epoch [21/1000], Step [2200/4367], Loss: 0.9369
2025-02-18 12:34:47,752 - Epoch [21/1000], Step [2300/4367], Loss: 0.5843
2025-02-18 12:35:13,654 - Epoch [21/1000], Step [2400/4367], Loss: 0.8348
2025-02-18 12:35:39,727 - Epoch [21/1000], Step [2500/4367], Loss: 0.6807
2025-02-18 12:36:05,709 - Epoch [21/1000], Step [2600/4367], Loss: 0.8033
2025-02-18 12:36:31,841 - Epoch [21/1000], Step [2700/4367], Loss: 0.4850
2025-02-18 12:36:57,642 - Epoch [21/1000], Step [2800/4367], Loss: 0.7327
2025-02-18 12:37:23,559 - Epoch [21/1000], Step [2900/4367], Loss: 0.6392
2025-02-18 12:37:49,366 - Epoch [21/1000], Step [3000/4367], Loss: 0.8476
2025-02-18 12:38:15,195 - Epoch [21/1000], Step [3100/4367], Loss: 0.6470
2025-02-18 12:38:41,271 - Epoch [21/1000], Step [3200/4367], Loss: 0.6493
2025-02-18 12:39:07,234 - Epoch [21/1000], Step [3300/4367], Loss: 0.5935
2025-02-18 12:39:33,252 - Epoch [21/1000], Step [3400/4367], Loss: 0.6970
2025-02-18 12:39:59,392 - Epoch [21/1000], Step [3500/4367], Loss: 0.4952
2025-02-18 12:40:25,286 - Epoch [21/1000], Step [3600/4367], Loss: 0.4987
2025-02-18 12:40:51,338 - Epoch [21/1000], Step [3700/4367], Loss: 0.2371
2025-02-18 12:41:17,229 - Epoch [21/1000], Step [3800/4367], Loss: 0.6989
2025-02-18 12:41:43,161 - Epoch [21/1000], Step [3900/4367], Loss: 0.7722
2025-02-18 12:42:09,394 - Epoch [21/1000], Step [4000/4367], Loss: 0.9419
2025-02-18 12:42:35,592 - Epoch [21/1000], Step [4100/4367], Loss: 0.5130
2025-02-18 12:43:01,426 - Epoch [21/1000], Step [4200/4367], Loss: 0.5617
2025-02-18 12:43:27,291 - Epoch [21/1000], Step [4300/4367], Loss: 0.6496
2025-02-18 12:43:53,634 - Epoch [21/1000], Validation Step [100/1090], Val Loss: 0.0079
2025-02-18 12:44:00,962 - Epoch [21/1000], Validation Step [200/1090], Val Loss: 0.0008
2025-02-18 12:44:08,384 - Epoch [21/1000], Validation Step [300/1090], Val Loss: 1.3560
2025-02-18 12:44:16,068 - Epoch [21/1000], Validation Step [400/1090], Val Loss: 1.1315
2025-02-18 12:44:23,332 - Epoch [21/1000], Validation Step [500/1090], Val Loss: 2.3117
2025-02-18 12:44:30,900 - Epoch [21/1000], Validation Step [600/1090], Val Loss: 1.4205
2025-02-18 12:44:38,550 - Epoch [21/1000], Validation Step [700/1090], Val Loss: 1.2195
2025-02-18 12:44:45,568 - Epoch [21/1000], Validation Step [800/1090], Val Loss: 0.0315
2025-02-18 12:44:52,407 - Epoch [21/1000], Validation Step [900/1090], Val Loss: 0.0814
2025-02-18 12:44:59,774 - Epoch [21/1000], Validation Step [1000/1090], Val Loss: 0.3752
2025-02-18 12:45:06,676 - Epoch 21/1000, Train Loss: 0.6117, Val Loss: 0.8651, Accuracy: 70.69%
2025-02-18 12:45:34,409 - Epoch [22/1000], Step [100/4367], Loss: 0.9636
2025-02-18 12:46:00,264 - Epoch [22/1000], Step [200/4367], Loss: 0.8515
2025-02-18 12:46:26,220 - Epoch [22/1000], Step [300/4367], Loss: 0.7534
2025-02-18 12:46:52,401 - Epoch [22/1000], Step [400/4367], Loss: 0.4843
2025-02-18 12:47:18,260 - Epoch [22/1000], Step [500/4367], Loss: 0.8789
2025-02-18 12:47:43,943 - Epoch [22/1000], Step [600/4367], Loss: 0.6829
2025-02-18 12:48:09,872 - Epoch [22/1000], Step [700/4367], Loss: 0.5053
2025-02-18 12:48:35,814 - Epoch [22/1000], Step [800/4367], Loss: 0.6949
2025-02-18 12:49:01,643 - Epoch [22/1000], Step [900/4367], Loss: 0.7568
2025-02-18 12:49:27,489 - Epoch [22/1000], Step [1000/4367], Loss: 0.4977
2025-02-18 12:49:53,537 - Epoch [22/1000], Step [1100/4367], Loss: 0.4944
2025-02-18 12:50:19,518 - Epoch [22/1000], Step [1200/4367], Loss: 0.3100
2025-02-18 12:50:45,173 - Epoch [22/1000], Step [1300/4367], Loss: 0.6667
2025-02-18 12:51:10,827 - Epoch [22/1000], Step [1400/4367], Loss: 0.3858
2025-02-18 12:51:36,785 - Epoch [22/1000], Step [1500/4367], Loss: 0.2495
2025-02-18 12:52:02,484 - Epoch [22/1000], Step [1600/4367], Loss: 0.5594
2025-02-18 12:52:28,055 - Epoch [22/1000], Step [1700/4367], Loss: 0.3165
2025-02-18 12:52:54,094 - Epoch [22/1000], Step [1800/4367], Loss: 0.4066
2025-02-18 12:53:19,886 - Epoch [22/1000], Step [1900/4367], Loss: 0.4649
2025-02-18 12:53:45,815 - Epoch [22/1000], Step [2000/4367], Loss: 0.2273
2025-02-18 12:54:11,849 - Epoch [22/1000], Step [2100/4367], Loss: 0.1953
2025-02-18 12:54:37,575 - Epoch [22/1000], Step [2200/4367], Loss: 0.5524
2025-02-18 12:55:03,080 - Epoch [22/1000], Step [2300/4367], Loss: 0.4657
2025-02-18 12:55:28,710 - Epoch [22/1000], Step [2400/4367], Loss: 0.5315
2025-02-18 12:55:54,412 - Epoch [22/1000], Step [2500/4367], Loss: 0.6055
2025-02-18 12:56:20,110 - Epoch [22/1000], Step [2600/4367], Loss: 0.2806
2025-02-18 12:56:45,712 - Epoch [22/1000], Step [2700/4367], Loss: 0.2722
2025-02-18 12:57:11,654 - Epoch [22/1000], Step [2800/4367], Loss: 0.4753
2025-02-18 12:57:37,683 - Epoch [22/1000], Step [2900/4367], Loss: 0.2387
2025-02-18 12:58:03,475 - Epoch [22/1000], Step [3000/4367], Loss: 0.4284
2025-02-18 12:58:29,356 - Epoch [22/1000], Step [3100/4367], Loss: 0.4186
2025-02-18 12:58:55,268 - Epoch [22/1000], Step [3200/4367], Loss: 0.4627
2025-02-18 12:59:21,040 - Epoch [22/1000], Step [3300/4367], Loss: 0.4084
2025-02-18 12:59:46,804 - Epoch [22/1000], Step [3400/4367], Loss: 0.4452
2025-02-18 13:00:12,661 - Epoch [22/1000], Step [3500/4367], Loss: 0.3310
2025-02-18 13:00:38,632 - Epoch [22/1000], Step [3600/4367], Loss: 0.3906
2025-02-18 13:01:04,740 - Epoch [22/1000], Step [3700/4367], Loss: 0.4767
2025-02-18 13:01:30,614 - Epoch [22/1000], Step [3800/4367], Loss: 0.3160
2025-02-18 13:01:56,658 - Epoch [22/1000], Step [3900/4367], Loss: 0.9617
2025-02-18 13:02:22,730 - Epoch [22/1000], Step [4000/4367], Loss: 0.6168
2025-02-18 13:02:48,771 - Epoch [22/1000], Step [4100/4367], Loss: 0.7091
2025-02-18 13:03:14,821 - Epoch [22/1000], Step [4200/4367], Loss: 0.5337
2025-02-18 13:03:40,830 - Epoch [22/1000], Step [4300/4367], Loss: 0.8636
2025-02-18 13:04:07,869 - Epoch [22/1000], Validation Step [100/1090], Val Loss: 0.0489
2025-02-18 13:04:15,257 - Epoch [22/1000], Validation Step [200/1090], Val Loss: 0.1018
2025-02-18 13:04:22,758 - Epoch [22/1000], Validation Step [300/1090], Val Loss: 0.8012
2025-02-18 13:04:30,490 - Epoch [22/1000], Validation Step [400/1090], Val Loss: 2.1733
2025-02-18 13:04:37,768 - Epoch [22/1000], Validation Step [500/1090], Val Loss: 1.2888
2025-02-18 13:04:45,407 - Epoch [22/1000], Validation Step [600/1090], Val Loss: 0.4548
2025-02-18 13:04:53,069 - Epoch [22/1000], Validation Step [700/1090], Val Loss: 0.3565
2025-02-18 13:05:00,112 - Epoch [22/1000], Validation Step [800/1090], Val Loss: 4.7081
2025-02-18 13:05:07,014 - Epoch [22/1000], Validation Step [900/1090], Val Loss: 4.8546
2025-02-18 13:05:14,416 - Epoch [22/1000], Validation Step [1000/1090], Val Loss: 1.2181
2025-02-18 13:05:21,378 - Epoch 22/1000, Train Loss: 0.5268, Val Loss: 1.5069, Accuracy: 59.55%
2025-02-18 13:05:22,499 - Model saved at /data2/personal/sungjin/korean_dialects/classification_second_checkpoint/checkpoint_epoch_22.pth
2025-02-18 13:05:51,052 - Epoch [23/1000], Step [100/4367], Loss: 0.7684
2025-02-18 13:06:17,498 - Epoch [23/1000], Step [200/4367], Loss: 0.4479
2025-02-18 13:06:43,637 - Epoch [23/1000], Step [300/4367], Loss: 0.4829
2025-02-18 13:07:10,099 - Epoch [23/1000], Step [400/4367], Loss: 0.6014
2025-02-18 13:07:35,863 - Epoch [23/1000], Step [500/4367], Loss: 0.5152
2025-02-18 13:08:02,014 - Epoch [23/1000], Step [600/4367], Loss: 0.5341
2025-02-18 13:08:27,921 - Epoch [23/1000], Step [700/4367], Loss: 0.7944
2025-02-18 13:08:53,874 - Epoch [23/1000], Step [800/4367], Loss: 0.7778
2025-02-18 13:09:19,698 - Epoch [23/1000], Step [900/4367], Loss: 0.7549
2025-02-18 13:09:46,945 - Epoch [23/1000], Step [1000/4367], Loss: 0.6778
2025-02-18 13:10:14,035 - Epoch [23/1000], Step [1100/4367], Loss: 0.3328
2025-02-18 13:10:40,062 - Epoch [23/1000], Step [1200/4367], Loss: 0.6162
2025-02-18 13:11:06,645 - Epoch [23/1000], Step [1300/4367], Loss: 0.4131
2025-02-18 13:11:32,696 - Epoch [23/1000], Step [1400/4367], Loss: 0.7505
2025-02-18 13:11:58,802 - Epoch [23/1000], Step [1500/4367], Loss: 0.7757
2025-02-18 13:12:24,663 - Epoch [23/1000], Step [1600/4367], Loss: 0.4334
2025-02-18 13:12:50,538 - Epoch [23/1000], Step [1700/4367], Loss: 0.5641
2025-02-18 13:13:16,323 - Epoch [23/1000], Step [1800/4367], Loss: 0.6327
2025-02-18 13:13:42,323 - Epoch [23/1000], Step [1900/4367], Loss: 0.5538
2025-02-18 13:14:08,194 - Epoch [23/1000], Step [2000/4367], Loss: 0.4103
2025-02-18 13:14:33,984 - Epoch [23/1000], Step [2100/4367], Loss: 0.5606
2025-02-18 13:15:00,052 - Epoch [23/1000], Step [2200/4367], Loss: 0.5140
2025-02-18 13:15:25,800 - Epoch [23/1000], Step [2300/4367], Loss: 0.4253
2025-02-18 13:15:51,619 - Epoch [23/1000], Step [2400/4367], Loss: 0.4432
2025-02-18 13:16:17,080 - Epoch [23/1000], Step [2500/4367], Loss: 0.7610
2025-02-18 13:16:42,761 - Epoch [23/1000], Step [2600/4367], Loss: 0.5274
2025-02-18 13:17:08,736 - Epoch [23/1000], Step [2700/4367], Loss: 0.5817
2025-02-18 13:17:34,677 - Epoch [23/1000], Step [2800/4367], Loss: 0.5466
2025-02-18 13:18:00,761 - Epoch [23/1000], Step [2900/4367], Loss: 0.4799
2025-02-18 13:18:27,032 - Epoch [23/1000], Step [3000/4367], Loss: 0.4602
2025-02-18 13:18:53,086 - Epoch [23/1000], Step [3100/4367], Loss: 0.2620
2025-02-18 13:19:19,299 - Epoch [23/1000], Step [3200/4367], Loss: 0.3334
2025-02-18 13:19:45,271 - Epoch [23/1000], Step [3300/4367], Loss: 0.2272
2025-02-18 13:20:11,682 - Epoch [23/1000], Step [3400/4367], Loss: 0.6978
2025-02-18 13:20:39,223 - Epoch [23/1000], Step [3500/4367], Loss: 0.5998
2025-02-18 13:21:08,003 - Epoch [23/1000], Step [3600/4367], Loss: 0.5017
2025-02-18 13:21:34,116 - Epoch [23/1000], Step [3700/4367], Loss: 0.5509
2025-02-18 13:22:00,394 - Epoch [23/1000], Step [3800/4367], Loss: 0.4790
2025-02-18 13:22:26,547 - Epoch [23/1000], Step [3900/4367], Loss: 0.3173
2025-02-18 13:22:52,430 - Epoch [23/1000], Step [4000/4367], Loss: 0.5522
2025-02-18 13:23:18,424 - Epoch [23/1000], Step [4100/4367], Loss: 0.3400
2025-02-18 13:23:44,268 - Epoch [23/1000], Step [4200/4367], Loss: 0.5119
2025-02-18 13:24:10,203 - Epoch [23/1000], Step [4300/4367], Loss: 0.3116
2025-02-18 13:24:36,809 - Epoch [23/1000], Validation Step [100/1090], Val Loss: 0.0007
2025-02-18 13:24:44,131 - Epoch [23/1000], Validation Step [200/1090], Val Loss: 0.0015
2025-02-18 13:24:51,598 - Epoch [23/1000], Validation Step [300/1090], Val Loss: 0.5005
2025-02-18 13:24:59,278 - Epoch [23/1000], Validation Step [400/1090], Val Loss: 0.8717
2025-02-18 13:25:06,518 - Epoch [23/1000], Validation Step [500/1090], Val Loss: 0.7509
2025-02-18 13:25:14,104 - Epoch [23/1000], Validation Step [600/1090], Val Loss: 0.6363
2025-02-18 13:25:21,743 - Epoch [23/1000], Validation Step [700/1090], Val Loss: 0.4606
2025-02-18 13:25:28,801 - Epoch [23/1000], Validation Step [800/1090], Val Loss: 0.6895
2025-02-18 13:25:35,635 - Epoch [23/1000], Validation Step [900/1090], Val Loss: 0.8664
2025-02-18 13:25:43,023 - Epoch [23/1000], Validation Step [1000/1090], Val Loss: 0.0661
2025-02-18 13:25:49,910 - Epoch 23/1000, Train Loss: 0.5322, Val Loss: 0.4871, Accuracy: 80.82%
2025-02-18 13:26:17,801 - Epoch [24/1000], Step [100/4367], Loss: 0.6951
2025-02-18 13:26:43,623 - Epoch [24/1000], Step [200/4367], Loss: 0.4945
2025-02-18 13:27:09,835 - Epoch [24/1000], Step [300/4367], Loss: 0.7209
2025-02-18 13:27:35,636 - Epoch [24/1000], Step [400/4367], Loss: 0.6816
2025-02-18 13:28:01,834 - Epoch [24/1000], Step [500/4367], Loss: 0.6568
2025-02-18 13:28:27,891 - Epoch [24/1000], Step [600/4367], Loss: 0.2025
2025-02-18 13:28:53,782 - Epoch [24/1000], Step [700/4367], Loss: 0.4721
2025-02-18 13:29:19,685 - Epoch [24/1000], Step [800/4367], Loss: 0.4571
2025-02-18 13:29:45,818 - Epoch [24/1000], Step [900/4367], Loss: 0.5024
2025-02-18 13:30:11,818 - Epoch [24/1000], Step [1000/4367], Loss: 0.6186
2025-02-18 13:30:37,534 - Epoch [24/1000], Step [1100/4367], Loss: 0.5214
2025-02-18 13:31:03,275 - Epoch [24/1000], Step [1200/4367], Loss: 0.2624
2025-02-18 13:31:29,179 - Epoch [24/1000], Step [1300/4367], Loss: 0.6143
2025-02-18 13:31:55,285 - Epoch [24/1000], Step [1400/4367], Loss: 0.4863
2025-02-18 13:32:21,239 - Epoch [24/1000], Step [1500/4367], Loss: 0.2186
2025-02-18 13:32:46,895 - Epoch [24/1000], Step [1600/4367], Loss: 0.4174
2025-02-18 13:33:13,108 - Epoch [24/1000], Step [1700/4367], Loss: 0.4184
2025-02-18 13:33:39,010 - Epoch [24/1000], Step [1800/4367], Loss: 0.3163
2025-02-18 13:34:04,516 - Epoch [24/1000], Step [1900/4367], Loss: 0.3135
2025-02-18 13:34:30,357 - Epoch [24/1000], Step [2000/4367], Loss: 0.3574
2025-02-18 13:34:56,425 - Epoch [24/1000], Step [2100/4367], Loss: 0.3424
2025-02-18 13:35:22,324 - Epoch [24/1000], Step [2200/4367], Loss: 0.4263
2025-02-18 13:35:48,239 - Epoch [24/1000], Step [2300/4367], Loss: 0.3753
2025-02-18 13:36:14,005 - Epoch [24/1000], Step [2400/4367], Loss: 0.5173
2025-02-18 13:36:40,026 - Epoch [24/1000], Step [2500/4367], Loss: 0.4359
2025-02-18 13:37:05,817 - Epoch [24/1000], Step [2600/4367], Loss: 0.2462
2025-02-18 13:37:31,875 - Epoch [24/1000], Step [2700/4367], Loss: 0.3547
2025-02-18 13:37:57,939 - Epoch [24/1000], Step [2800/4367], Loss: 0.2521
2025-02-18 13:38:23,773 - Epoch [24/1000], Step [2900/4367], Loss: 0.4298
2025-02-18 13:38:49,409 - Epoch [24/1000], Step [3000/4367], Loss: 0.8371
2025-02-18 13:39:15,557 - Epoch [24/1000], Step [3100/4367], Loss: 0.4703
2025-02-18 13:39:41,292 - Epoch [24/1000], Step [3200/4367], Loss: 0.5392
2025-02-18 13:40:07,375 - Epoch [24/1000], Step [3300/4367], Loss: 0.3467
2025-02-18 13:40:33,173 - Epoch [24/1000], Step [3400/4367], Loss: 0.6189
2025-02-18 13:40:59,092 - Epoch [24/1000], Step [3500/4367], Loss: 0.3934
2025-02-18 13:41:25,227 - Epoch [24/1000], Step [3600/4367], Loss: 0.4022
2025-02-18 13:41:50,901 - Epoch [24/1000], Step [3700/4367], Loss: 0.3608
2025-02-18 13:42:17,039 - Epoch [24/1000], Step [3800/4367], Loss: 0.2541
2025-02-18 13:42:42,948 - Epoch [24/1000], Step [3900/4367], Loss: 0.6485
2025-02-18 13:43:08,864 - Epoch [24/1000], Step [4000/4367], Loss: 0.3232
2025-02-18 13:43:34,732 - Epoch [24/1000], Step [4100/4367], Loss: 0.6075
2025-02-18 13:44:00,877 - Epoch [24/1000], Step [4200/4367], Loss: 0.3439
2025-02-18 13:44:27,140 - Epoch [24/1000], Step [4300/4367], Loss: 0.5573
2025-02-18 13:44:53,633 - Epoch [24/1000], Validation Step [100/1090], Val Loss: 0.0439
2025-02-18 13:45:01,005 - Epoch [24/1000], Validation Step [200/1090], Val Loss: 0.0097
2025-02-18 13:45:08,495 - Epoch [24/1000], Validation Step [300/1090], Val Loss: 0.4391
2025-02-18 13:45:16,250 - Epoch [24/1000], Validation Step [400/1090], Val Loss: 0.5807
2025-02-18 13:45:23,583 - Epoch [24/1000], Validation Step [500/1090], Val Loss: 1.0543
2025-02-18 13:45:31,189 - Epoch [24/1000], Validation Step [600/1090], Val Loss: 0.7489
2025-02-18 13:45:38,864 - Epoch [24/1000], Validation Step [700/1090], Val Loss: 0.5332
2025-02-18 13:45:45,951 - Epoch [24/1000], Validation Step [800/1090], Val Loss: 0.5040
2025-02-18 13:45:52,880 - Epoch [24/1000], Validation Step [900/1090], Val Loss: 0.6313
2025-02-18 13:46:00,343 - Epoch [24/1000], Validation Step [1000/1090], Val Loss: 0.0711
2025-02-18 13:46:07,322 - Epoch 24/1000, Train Loss: 0.4521, Val Loss: 0.5389, Accuracy: 79.98%
2025-02-18 13:46:08,311 - Model saved at /data2/personal/sungjin/korean_dialects/classification_second_checkpoint/checkpoint_epoch_24.pth
2025-02-18 13:46:36,380 - Epoch [25/1000], Step [100/4367], Loss: 0.4419
2025-02-18 13:47:02,184 - Epoch [25/1000], Step [200/4367], Loss: 0.4041
2025-02-18 13:47:28,527 - Epoch [25/1000], Step [300/4367], Loss: 0.4395
2025-02-18 13:47:54,235 - Epoch [25/1000], Step [400/4367], Loss: 0.4749
2025-02-18 13:48:20,512 - Epoch [25/1000], Step [500/4367], Loss: 0.4995
2025-02-18 13:48:46,477 - Epoch [25/1000], Step [600/4367], Loss: 0.2629
2025-02-18 13:49:12,709 - Epoch [25/1000], Step [700/4367], Loss: 0.3365
2025-02-18 13:49:38,629 - Epoch [25/1000], Step [800/4367], Loss: 0.3243
2025-02-18 13:50:04,496 - Epoch [25/1000], Step [900/4367], Loss: 0.7012
2025-02-18 13:50:30,447 - Epoch [25/1000], Step [1000/4367], Loss: 0.3626
2025-02-18 13:50:56,480 - Epoch [25/1000], Step [1100/4367], Loss: 0.3590
2025-02-18 13:51:22,216 - Epoch [25/1000], Step [1200/4367], Loss: 0.4338
2025-02-18 13:51:47,918 - Epoch [25/1000], Step [1300/4367], Loss: 0.5865
2025-02-18 13:52:13,629 - Epoch [25/1000], Step [1400/4367], Loss: 0.5099
2025-02-18 13:52:39,727 - Epoch [25/1000], Step [1500/4367], Loss: 0.6009
2025-02-18 13:53:05,731 - Epoch [25/1000], Step [1600/4367], Loss: 0.3178
2025-02-18 13:53:31,605 - Epoch [25/1000], Step [1700/4367], Loss: 0.6268
2025-02-18 13:53:57,645 - Epoch [25/1000], Step [1800/4367], Loss: 0.4682
2025-02-18 13:54:23,691 - Epoch [25/1000], Step [1900/4367], Loss: 0.4800
2025-02-18 13:54:49,940 - Epoch [25/1000], Step [2000/4367], Loss: 0.4043
2025-02-18 13:55:16,112 - Epoch [25/1000], Step [2100/4367], Loss: 0.8199
2025-02-18 13:55:42,224 - Epoch [25/1000], Step [2200/4367], Loss: 0.5475
2025-02-18 13:56:08,164 - Epoch [25/1000], Step [2300/4367], Loss: 0.3917
2025-02-18 13:56:34,283 - Epoch [25/1000], Step [2400/4367], Loss: 0.4224
2025-02-18 13:57:00,115 - Epoch [25/1000], Step [2500/4367], Loss: 0.3658
2025-02-18 13:57:25,822 - Epoch [25/1000], Step [2600/4367], Loss: 0.4974
2025-02-18 13:57:51,603 - Epoch [25/1000], Step [2700/4367], Loss: 0.4060
2025-02-18 13:58:17,492 - Epoch [25/1000], Step [2800/4367], Loss: 0.6899
2025-02-18 13:58:43,326 - Epoch [25/1000], Step [2900/4367], Loss: 0.4177
2025-02-18 13:59:09,057 - Epoch [25/1000], Step [3000/4367], Loss: 0.5789
2025-02-18 13:59:34,862 - Epoch [25/1000], Step [3100/4367], Loss: 0.2897
2025-02-18 14:00:00,846 - Epoch [25/1000], Step [3200/4367], Loss: 0.8022
2025-02-18 14:00:26,709 - Epoch [25/1000], Step [3300/4367], Loss: 0.3022
2025-02-18 14:00:52,232 - Epoch [25/1000], Step [3400/4367], Loss: 0.6159
2025-02-18 14:01:18,386 - Epoch [25/1000], Step [3500/4367], Loss: 0.4557
2025-02-18 14:01:44,618 - Epoch [25/1000], Step [3600/4367], Loss: 0.5193
2025-02-18 14:02:10,577 - Epoch [25/1000], Step [3700/4367], Loss: 0.6126
2025-02-18 14:02:36,474 - Epoch [25/1000], Step [3800/4367], Loss: 0.4943
2025-02-18 14:03:02,715 - Epoch [25/1000], Step [3900/4367], Loss: 0.3312
2025-02-18 14:03:28,661 - Epoch [25/1000], Step [4000/4367], Loss: 0.4603
2025-02-18 14:03:54,630 - Epoch [25/1000], Step [4100/4367], Loss: 0.5004
2025-02-18 14:04:20,486 - Epoch [25/1000], Step [4200/4367], Loss: 0.3370
2025-02-18 14:04:46,394 - Epoch [25/1000], Step [4300/4367], Loss: 0.6409
2025-02-18 14:05:14,060 - Epoch [25/1000], Validation Step [100/1090], Val Loss: 0.0032
2025-02-18 14:05:21,532 - Epoch [25/1000], Validation Step [200/1090], Val Loss: 0.0058
2025-02-18 14:05:28,993 - Epoch [25/1000], Validation Step [300/1090], Val Loss: 0.5088
2025-02-18 14:05:36,735 - Epoch [25/1000], Validation Step [400/1090], Val Loss: 0.5914
2025-02-18 14:05:44,040 - Epoch [25/1000], Validation Step [500/1090], Val Loss: 0.6309
2025-02-18 14:05:51,649 - Epoch [25/1000], Validation Step [600/1090], Val Loss: 0.6887
2025-02-18 14:05:59,361 - Epoch [25/1000], Validation Step [700/1090], Val Loss: 0.5975
2025-02-18 14:06:06,431 - Epoch [25/1000], Validation Step [800/1090], Val Loss: 0.6420
2025-02-18 14:06:13,359 - Epoch [25/1000], Validation Step [900/1090], Val Loss: 0.7521
2025-02-18 14:06:20,827 - Epoch [25/1000], Validation Step [1000/1090], Val Loss: 0.0491
2025-02-18 14:06:27,745 - Epoch 25/1000, Train Loss: 0.4380, Val Loss: 0.4884, Accuracy: 81.17%
2025-02-18 14:06:55,976 - Epoch [26/1000], Step [100/4367], Loss: 0.2493
2025-02-18 14:07:22,119 - Epoch [26/1000], Step [200/4367], Loss: 0.6257
2025-02-18 14:07:48,255 - Epoch [26/1000], Step [300/4367], Loss: 0.3184
2025-02-18 14:08:14,005 - Epoch [26/1000], Step [400/4367], Loss: 0.3974
2025-02-18 14:08:39,290 - Epoch [26/1000], Step [500/4367], Loss: 0.3023
2025-02-18 14:09:05,200 - Epoch [26/1000], Step [600/4367], Loss: 0.5048
2025-02-18 14:09:31,231 - Epoch [26/1000], Step [700/4367], Loss: 0.3649
2025-02-18 14:09:57,122 - Epoch [26/1000], Step [800/4367], Loss: 0.5738
2025-02-18 14:10:23,311 - Epoch [26/1000], Step [900/4367], Loss: 0.4452
2025-02-18 14:10:49,289 - Epoch [26/1000], Step [1000/4367], Loss: 0.2456
2025-02-18 14:11:15,208 - Epoch [26/1000], Step [1100/4367], Loss: 0.4258
2025-02-18 14:11:41,314 - Epoch [26/1000], Step [1200/4367], Loss: 0.5493
2025-02-18 14:12:07,373 - Epoch [26/1000], Step [1300/4367], Loss: 0.1374
2025-02-18 14:12:33,306 - Epoch [26/1000], Step [1400/4367], Loss: 0.2484
2025-02-18 14:12:59,242 - Epoch [26/1000], Step [1500/4367], Loss: 0.5845
2025-02-18 14:13:24,756 - Epoch [26/1000], Step [1600/4367], Loss: 0.5222
2025-02-18 14:13:50,997 - Epoch [26/1000], Step [1700/4367], Loss: 0.4732
2025-02-18 14:14:16,896 - Epoch [26/1000], Step [1800/4367], Loss: 0.7507
2025-02-18 14:14:42,804 - Epoch [26/1000], Step [1900/4367], Loss: 0.5069
2025-02-18 14:15:08,602 - Epoch [26/1000], Step [2000/4367], Loss: 0.2386
2025-02-18 14:15:34,487 - Epoch [26/1000], Step [2100/4367], Loss: 0.1820
2025-02-18 14:16:00,290 - Epoch [26/1000], Step [2200/4367], Loss: 0.4609
2025-02-18 14:16:26,354 - Epoch [26/1000], Step [2300/4367], Loss: 0.4577
2025-02-18 14:16:52,375 - Epoch [26/1000], Step [2400/4367], Loss: 0.3302
2025-02-18 14:17:18,538 - Epoch [26/1000], Step [2500/4367], Loss: 0.7927
2025-02-18 14:17:44,681 - Epoch [26/1000], Step [2600/4367], Loss: 0.4031
2025-02-18 14:18:10,875 - Epoch [26/1000], Step [2700/4367], Loss: 0.1726
2025-02-18 14:18:37,092 - Epoch [26/1000], Step [2800/4367], Loss: 0.1615
2025-02-18 14:19:03,498 - Epoch [26/1000], Step [2900/4367], Loss: 0.2893
2025-02-18 14:19:29,427 - Epoch [26/1000], Step [3000/4367], Loss: 0.5423
2025-02-18 14:19:55,565 - Epoch [26/1000], Step [3100/4367], Loss: 0.4753
2025-02-18 14:20:21,579 - Epoch [26/1000], Step [3200/4367], Loss: 0.5463
2025-02-18 14:20:47,559 - Epoch [26/1000], Step [3300/4367], Loss: 0.4087
2025-02-18 14:21:13,637 - Epoch [26/1000], Step [3400/4367], Loss: 0.2291
2025-02-18 14:21:39,296 - Epoch [26/1000], Step [3500/4367], Loss: 0.4448
2025-02-18 14:22:05,191 - Epoch [26/1000], Step [3600/4367], Loss: 0.5913
2025-02-18 14:22:30,797 - Epoch [26/1000], Step [3700/4367], Loss: 0.3754
2025-02-18 14:22:56,554 - Epoch [26/1000], Step [3800/4367], Loss: 0.2577
2025-02-18 14:23:22,559 - Epoch [26/1000], Step [3900/4367], Loss: 0.2755
2025-02-18 14:23:48,447 - Epoch [26/1000], Step [4000/4367], Loss: 0.4125
2025-02-18 14:24:14,344 - Epoch [26/1000], Step [4100/4367], Loss: 0.1864
2025-02-18 14:24:40,372 - Epoch [26/1000], Step [4200/4367], Loss: 0.1990
2025-02-18 14:25:05,976 - Epoch [26/1000], Step [4300/4367], Loss: 0.4100
2025-02-18 14:25:32,572 - Epoch [26/1000], Validation Step [100/1090], Val Loss: 0.0141
2025-02-18 14:25:39,857 - Epoch [26/1000], Validation Step [200/1090], Val Loss: 0.0277
2025-02-18 14:25:47,237 - Epoch [26/1000], Validation Step [300/1090], Val Loss: 0.5491
2025-02-18 14:25:54,882 - Epoch [26/1000], Validation Step [400/1090], Val Loss: 0.5014
2025-02-18 14:26:02,075 - Epoch [26/1000], Validation Step [500/1090], Val Loss: 0.5145
2025-02-18 14:26:09,594 - Epoch [26/1000], Validation Step [600/1090], Val Loss: 0.5804
2025-02-18 14:26:17,166 - Epoch [26/1000], Validation Step [700/1090], Val Loss: 0.4581
2025-02-18 14:26:24,133 - Epoch [26/1000], Validation Step [800/1090], Val Loss: 0.3178
2025-02-18 14:26:30,922 - Epoch [26/1000], Validation Step [900/1090], Val Loss: 0.5377
2025-02-18 14:26:38,241 - Epoch [26/1000], Validation Step [1000/1090], Val Loss: 0.0811
2025-02-18 14:26:45,081 - Epoch 26/1000, Train Loss: 0.4165, Val Loss: 0.4253, Accuracy: 84.36%
2025-02-18 14:26:45,585 - Model saved at /data2/personal/sungjin/korean_dialects/classification_second_checkpoint/checkpoint_epoch_26.pth
2025-02-18 14:27:13,544 - Epoch [27/1000], Step [100/4367], Loss: 0.3557
2025-02-18 14:27:39,553 - Epoch [27/1000], Step [200/4367], Loss: 0.3793
2025-02-18 14:28:05,394 - Epoch [27/1000], Step [300/4367], Loss: 0.5480
2025-02-18 14:28:31,108 - Epoch [27/1000], Step [400/4367], Loss: 0.5211
2025-02-18 14:28:57,153 - Epoch [27/1000], Step [500/4367], Loss: 0.7685
2025-02-18 14:29:22,906 - Epoch [27/1000], Step [600/4367], Loss: 0.2188
2025-02-18 14:29:49,056 - Epoch [27/1000], Step [700/4367], Loss: 0.6115
2025-02-18 14:30:15,040 - Epoch [27/1000], Step [800/4367], Loss: 0.4342
2025-02-18 14:30:40,833 - Epoch [27/1000], Step [900/4367], Loss: 0.3480
2025-02-18 14:31:07,001 - Epoch [27/1000], Step [1000/4367], Loss: 0.3742
2025-02-18 14:31:33,039 - Epoch [27/1000], Step [1100/4367], Loss: 0.5150
2025-02-18 14:31:59,122 - Epoch [27/1000], Step [1200/4367], Loss: 0.5231
2025-02-18 14:32:24,976 - Epoch [27/1000], Step [1300/4367], Loss: 0.3223
2025-02-18 14:32:50,615 - Epoch [27/1000], Step [1400/4367], Loss: 0.4482
2025-02-18 14:33:16,480 - Epoch [27/1000], Step [1500/4367], Loss: 0.5751
2025-02-18 14:33:42,239 - Epoch [27/1000], Step [1600/4367], Loss: 0.4039
2025-02-18 14:34:08,472 - Epoch [27/1000], Step [1700/4367], Loss: 0.4508
2025-02-18 14:34:34,929 - Epoch [27/1000], Step [1800/4367], Loss: 0.6224
2025-02-18 14:35:01,312 - Epoch [27/1000], Step [1900/4367], Loss: 0.3027
2025-02-18 14:35:28,718 - Epoch [27/1000], Step [2000/4367], Loss: 0.1644
2025-02-18 14:35:55,003 - Epoch [27/1000], Step [2100/4367], Loss: 0.4609
2025-02-18 14:36:21,060 - Epoch [27/1000], Step [2200/4367], Loss: 0.3122
2025-02-18 14:36:47,052 - Epoch [27/1000], Step [2300/4367], Loss: 0.5228
2025-02-18 14:37:13,252 - Epoch [27/1000], Step [2400/4367], Loss: 0.2221
2025-02-18 14:37:39,450 - Epoch [27/1000], Step [2500/4367], Loss: 0.4377
2025-02-18 14:38:06,194 - Epoch [27/1000], Step [2600/4367], Loss: 0.1659
2025-02-18 14:38:32,717 - Epoch [27/1000], Step [2700/4367], Loss: 0.2161
2025-02-18 14:38:59,505 - Epoch [27/1000], Step [2800/4367], Loss: 0.7114
2025-02-18 14:39:29,477 - Epoch [27/1000], Step [2900/4367], Loss: 0.1869
2025-02-18 14:39:55,713 - Epoch [27/1000], Step [3000/4367], Loss: 0.3565
2025-02-18 14:40:21,797 - Epoch [27/1000], Step [3100/4367], Loss: 0.4503
2025-02-18 14:40:47,882 - Epoch [27/1000], Step [3200/4367], Loss: 0.4721
2025-02-18 14:41:14,177 - Epoch [27/1000], Step [3300/4367], Loss: 0.3241
2025-02-18 14:41:40,290 - Epoch [27/1000], Step [3400/4367], Loss: 0.2961
2025-02-18 14:42:06,060 - Epoch [27/1000], Step [3500/4367], Loss: 0.4824
2025-02-18 14:42:31,851 - Epoch [27/1000], Step [3600/4367], Loss: 0.5523
2025-02-18 14:42:58,015 - Epoch [27/1000], Step [3700/4367], Loss: 0.3473
2025-02-18 14:43:24,052 - Epoch [27/1000], Step [3800/4367], Loss: 0.4415
2025-02-18 14:43:50,081 - Epoch [27/1000], Step [3900/4367], Loss: 0.2394
2025-02-18 14:44:16,156 - Epoch [27/1000], Step [4000/4367], Loss: 0.4492
2025-02-18 14:44:42,268 - Epoch [27/1000], Step [4100/4367], Loss: 0.5105
2025-02-18 14:45:08,264 - Epoch [27/1000], Step [4200/4367], Loss: 0.3231
2025-02-18 14:45:34,369 - Epoch [27/1000], Step [4300/4367], Loss: 0.4611
2025-02-18 14:46:01,272 - Epoch [27/1000], Validation Step [100/1090], Val Loss: 0.0211
2025-02-18 14:46:08,664 - Epoch [27/1000], Validation Step [200/1090], Val Loss: 0.0428
2025-02-18 14:46:16,211 - Epoch [27/1000], Validation Step [300/1090], Val Loss: 0.3870
2025-02-18 14:46:24,004 - Epoch [27/1000], Validation Step [400/1090], Val Loss: 0.7961
2025-02-18 14:46:31,346 - Epoch [27/1000], Validation Step [500/1090], Val Loss: 0.6866
2025-02-18 14:46:38,994 - Epoch [27/1000], Validation Step [600/1090], Val Loss: 0.6945
2025-02-18 14:46:46,657 - Epoch [27/1000], Validation Step [700/1090], Val Loss: 0.5649
2025-02-18 14:46:53,736 - Epoch [27/1000], Validation Step [800/1090], Val Loss: 0.3372
2025-02-18 14:47:00,694 - Epoch [27/1000], Validation Step [900/1090], Val Loss: 0.5714
2025-02-18 14:47:08,119 - Epoch [27/1000], Validation Step [1000/1090], Val Loss: 0.0710
2025-02-18 14:47:15,070 - Epoch 27/1000, Train Loss: 0.4034, Val Loss: 0.4517, Accuracy: 83.03%
2025-02-18 14:47:43,842 - Epoch [28/1000], Step [100/4367], Loss: 0.2116
2025-02-18 14:48:09,954 - Epoch [28/1000], Step [200/4367], Loss: 0.2383
2025-02-18 14:48:36,033 - Epoch [28/1000], Step [300/4367], Loss: 0.3364
2025-02-18 14:49:02,112 - Epoch [28/1000], Step [400/4367], Loss: 0.4778
2025-02-18 14:49:28,038 - Epoch [28/1000], Step [500/4367], Loss: 0.4497
2025-02-18 14:49:53,990 - Epoch [28/1000], Step [600/4367], Loss: 0.3720
2025-02-18 14:50:19,885 - Epoch [28/1000], Step [700/4367], Loss: 0.4238
2025-02-18 14:50:45,887 - Epoch [28/1000], Step [800/4367], Loss: 0.0752
2025-02-18 14:51:11,640 - Epoch [28/1000], Step [900/4367], Loss: 0.5037
2025-02-18 14:51:37,704 - Epoch [28/1000], Step [1000/4367], Loss: 0.2811
2025-02-18 14:52:03,695 - Epoch [28/1000], Step [1100/4367], Loss: 0.2899
2025-02-18 14:52:29,652 - Epoch [28/1000], Step [1200/4367], Loss: 0.5639
2025-02-18 14:52:55,813 - Epoch [28/1000], Step [1300/4367], Loss: 0.3463
2025-02-18 14:53:21,675 - Epoch [28/1000], Step [1400/4367], Loss: 0.2896
2025-02-18 14:53:47,661 - Epoch [28/1000], Step [1500/4367], Loss: 0.3050
2025-02-18 14:54:13,760 - Epoch [28/1000], Step [1600/4367], Loss: 0.2785
2025-02-18 14:54:39,577 - Epoch [28/1000], Step [1700/4367], Loss: 0.3773
2025-02-18 14:55:05,329 - Epoch [28/1000], Step [1800/4367], Loss: 0.1980
2025-02-18 14:55:31,790 - Epoch [28/1000], Step [1900/4367], Loss: 0.1539
2025-02-18 14:55:58,058 - Epoch [28/1000], Step [2000/4367], Loss: 0.3217
2025-02-18 14:56:24,197 - Epoch [28/1000], Step [2100/4367], Loss: 0.2661
2025-02-18 14:56:50,115 - Epoch [28/1000], Step [2200/4367], Loss: 0.5564
2025-02-18 14:57:16,203 - Epoch [28/1000], Step [2300/4367], Loss: 0.6451
2025-02-18 14:57:42,119 - Epoch [28/1000], Step [2400/4367], Loss: 0.4606
2025-02-18 14:58:08,439 - Epoch [28/1000], Step [2500/4367], Loss: 0.2441
2025-02-18 14:58:34,430 - Epoch [28/1000], Step [2600/4367], Loss: 0.5001
2025-02-18 14:59:00,429 - Epoch [28/1000], Step [2700/4367], Loss: 0.2678
2025-02-18 14:59:26,238 - Epoch [28/1000], Step [2800/4367], Loss: 0.4591
2025-02-18 14:59:52,390 - Epoch [28/1000], Step [2900/4367], Loss: 0.5288
2025-02-18 15:00:18,065 - Epoch [28/1000], Step [3000/4367], Loss: 0.4437
2025-02-18 15:00:43,553 - Epoch [28/1000], Step [3100/4367], Loss: 0.2843
2025-02-18 15:01:09,467 - Epoch [28/1000], Step [3200/4367], Loss: 0.6853
2025-02-18 15:01:35,441 - Epoch [28/1000], Step [3300/4367], Loss: 0.4936
2025-02-18 15:02:01,613 - Epoch [28/1000], Step [3400/4367], Loss: 0.4793
2025-02-18 15:02:27,380 - Epoch [28/1000], Step [3500/4367], Loss: 0.3698
2025-02-18 15:02:53,419 - Epoch [28/1000], Step [3600/4367], Loss: 0.5148
2025-02-18 15:03:19,305 - Epoch [28/1000], Step [3700/4367], Loss: 0.5996
2025-02-18 15:03:45,252 - Epoch [28/1000], Step [3800/4367], Loss: 0.2275
2025-02-18 15:04:11,141 - Epoch [28/1000], Step [3900/4367], Loss: 0.3676
2025-02-18 15:04:37,257 - Epoch [28/1000], Step [4000/4367], Loss: 0.3433
2025-02-18 15:05:02,826 - Epoch [28/1000], Step [4100/4367], Loss: 0.2403
2025-02-18 15:05:29,047 - Epoch [28/1000], Step [4200/4367], Loss: 0.2622
2025-02-18 15:05:55,142 - Epoch [28/1000], Step [4300/4367], Loss: 0.5229
2025-02-18 15:06:21,762 - Epoch [28/1000], Validation Step [100/1090], Val Loss: 0.0006
2025-02-18 15:06:29,137 - Epoch [28/1000], Validation Step [200/1090], Val Loss: 0.0000
2025-02-18 15:06:36,628 - Epoch [28/1000], Validation Step [300/1090], Val Loss: 0.6750
2025-02-18 15:06:44,329 - Epoch [28/1000], Validation Step [400/1090], Val Loss: 0.5244
2025-02-18 15:06:51,567 - Epoch [28/1000], Validation Step [500/1090], Val Loss: 0.4447
2025-02-18 15:06:59,098 - Epoch [28/1000], Validation Step [600/1090], Val Loss: 0.5277
2025-02-18 15:07:06,676 - Epoch [28/1000], Validation Step [700/1090], Val Loss: 0.4061
2025-02-18 15:07:13,650 - Epoch [28/1000], Validation Step [800/1090], Val Loss: 0.2370
2025-02-18 15:07:20,460 - Epoch [28/1000], Validation Step [900/1090], Val Loss: 0.4614
2025-02-18 15:07:27,797 - Epoch [28/1000], Validation Step [1000/1090], Val Loss: 0.0331
2025-02-18 15:07:34,659 - Epoch 28/1000, Train Loss: 0.4021, Val Loss: 0.3933, Accuracy: 85.59%
2025-02-18 15:07:35,200 - Model saved at /data2/personal/sungjin/korean_dialects/classification_second_checkpoint/checkpoint_epoch_28.pth
2025-02-18 15:08:03,136 - Epoch [29/1000], Step [100/4367], Loss: 0.1172
2025-02-18 15:08:29,007 - Epoch [29/1000], Step [200/4367], Loss: 0.7007
2025-02-18 15:08:55,163 - Epoch [29/1000], Step [300/4367], Loss: 0.6265
2025-02-18 15:09:21,308 - Epoch [29/1000], Step [400/4367], Loss: 0.4899
2025-02-18 15:09:47,303 - Epoch [29/1000], Step [500/4367], Loss: 0.3985
2025-02-18 15:10:13,072 - Epoch [29/1000], Step [600/4367], Loss: 0.1743
2025-02-18 15:10:39,035 - Epoch [29/1000], Step [700/4367], Loss: 0.4205
2025-02-18 15:11:04,904 - Epoch [29/1000], Step [800/4367], Loss: 0.2593
2025-02-18 15:11:30,792 - Epoch [29/1000], Step [900/4367], Loss: 0.1288
2025-02-18 15:11:56,751 - Epoch [29/1000], Step [1000/4367], Loss: 0.3675
2025-02-18 15:12:22,783 - Epoch [29/1000], Step [1100/4367], Loss: 0.1887
2025-02-18 15:12:48,724 - Epoch [29/1000], Step [1200/4367], Loss: 0.4798
2025-02-18 15:13:14,442 - Epoch [29/1000], Step [1300/4367], Loss: 0.3709
2025-02-18 15:13:40,471 - Epoch [29/1000], Step [1400/4367], Loss: 0.3561
2025-02-18 15:14:06,521 - Epoch [29/1000], Step [1500/4367], Loss: 0.3682
2025-02-18 15:14:32,676 - Epoch [29/1000], Step [1600/4367], Loss: 0.3503
2025-02-18 15:14:58,699 - Epoch [29/1000], Step [1700/4367], Loss: 0.5843
2025-02-18 15:15:24,894 - Epoch [29/1000], Step [1800/4367], Loss: 0.5225
2025-02-18 15:15:50,893 - Epoch [29/1000], Step [1900/4367], Loss: 0.6048
2025-02-18 15:16:16,935 - Epoch [29/1000], Step [2000/4367], Loss: 0.2599
2025-02-18 15:16:42,778 - Epoch [29/1000], Step [2100/4367], Loss: 0.3416
2025-02-18 15:17:08,647 - Epoch [29/1000], Step [2200/4367], Loss: 0.2356
2025-02-18 15:17:34,540 - Epoch [29/1000], Step [2300/4367], Loss: 0.2141
2025-02-18 15:18:00,472 - Epoch [29/1000], Step [2400/4367], Loss: 0.3040
2025-02-18 15:18:26,534 - Epoch [29/1000], Step [2500/4367], Loss: 0.2803
2025-02-18 15:18:52,392 - Epoch [29/1000], Step [2600/4367], Loss: 0.3183
2025-02-18 15:19:18,475 - Epoch [29/1000], Step [2700/4367], Loss: 0.5364
2025-02-18 15:19:44,670 - Epoch [29/1000], Step [2800/4367], Loss: 0.4181
2025-02-18 15:20:10,438 - Epoch [29/1000], Step [2900/4367], Loss: 0.4268
2025-02-18 15:20:36,537 - Epoch [29/1000], Step [3000/4367], Loss: 0.4783
2025-02-18 15:21:02,261 - Epoch [29/1000], Step [3100/4367], Loss: 0.6878
2025-02-18 15:21:28,673 - Epoch [29/1000], Step [3200/4367], Loss: 0.2203
2025-02-18 15:21:54,598 - Epoch [29/1000], Step [3300/4367], Loss: 0.4991
2025-02-18 15:22:21,080 - Epoch [29/1000], Step [3400/4367], Loss: 0.1453
2025-02-18 15:22:46,659 - Epoch [29/1000], Step [3500/4367], Loss: 0.3379
2025-02-18 15:23:12,732 - Epoch [29/1000], Step [3600/4367], Loss: 0.5570
2025-02-18 15:23:38,926 - Epoch [29/1000], Step [3700/4367], Loss: 0.3261
2025-02-18 15:24:04,963 - Epoch [29/1000], Step [3800/4367], Loss: 0.5797
2025-02-18 15:24:30,791 - Epoch [29/1000], Step [3900/4367], Loss: 0.3894
2025-02-18 15:24:56,901 - Epoch [29/1000], Step [4000/4367], Loss: 0.5326
2025-02-18 15:25:22,558 - Epoch [29/1000], Step [4100/4367], Loss: 0.5318
2025-02-18 15:25:48,508 - Epoch [29/1000], Step [4200/4367], Loss: 0.0897
2025-02-18 15:26:14,439 - Epoch [29/1000], Step [4300/4367], Loss: 0.5789
2025-02-18 15:26:41,539 - Epoch [29/1000], Validation Step [100/1090], Val Loss: 0.3155
2025-02-18 15:26:48,925 - Epoch [29/1000], Validation Step [200/1090], Val Loss: 0.0928
2025-02-18 15:26:56,405 - Epoch [29/1000], Validation Step [300/1090], Val Loss: 0.4375
2025-02-18 15:27:04,170 - Epoch [29/1000], Validation Step [400/1090], Val Loss: 0.6223
2025-02-18 15:27:11,475 - Epoch [29/1000], Validation Step [500/1090], Val Loss: 0.8571
2025-02-18 15:27:19,130 - Epoch [29/1000], Validation Step [600/1090], Val Loss: 0.6826
2025-02-18 15:27:26,802 - Epoch [29/1000], Validation Step [700/1090], Val Loss: 0.5351
2025-02-18 15:27:33,862 - Epoch [29/1000], Validation Step [800/1090], Val Loss: 0.1759
2025-02-18 15:27:40,809 - Epoch [29/1000], Validation Step [900/1090], Val Loss: 0.2684
2025-02-18 15:27:48,262 - Epoch [29/1000], Validation Step [1000/1090], Val Loss: 0.1824
2025-02-18 15:27:55,162 - Epoch 29/1000, Train Loss: 0.3947, Val Loss: 0.5812, Accuracy: 78.63%
2025-02-18 15:28:23,311 - Epoch [30/1000], Step [100/4367], Loss: 0.3173
2025-02-18 15:28:49,371 - Epoch [30/1000], Step [200/4367], Loss: 0.3201
2025-02-18 15:29:15,105 - Epoch [30/1000], Step [300/4367], Loss: 0.3280
2025-02-18 15:29:41,170 - Epoch [30/1000], Step [400/4367], Loss: 0.3038
2025-02-18 15:30:07,043 - Epoch [30/1000], Step [500/4367], Loss: 0.6164
2025-02-18 15:30:33,553 - Epoch [30/1000], Step [600/4367], Loss: 0.5788
2025-02-18 15:30:59,279 - Epoch [30/1000], Step [700/4367], Loss: 0.4516
2025-02-18 15:31:25,211 - Epoch [30/1000], Step [800/4367], Loss: 0.2780
2025-02-18 15:31:51,592 - Epoch [30/1000], Step [900/4367], Loss: 0.3862
2025-02-18 15:32:17,608 - Epoch [30/1000], Step [1000/4367], Loss: 0.5387
2025-02-18 15:32:43,644 - Epoch [30/1000], Step [1100/4367], Loss: 0.3813
2025-02-18 15:33:09,746 - Epoch [30/1000], Step [1200/4367], Loss: 0.4778
2025-02-18 15:33:35,716 - Epoch [30/1000], Step [1300/4367], Loss: 0.1866
2025-02-18 15:34:01,832 - Epoch [30/1000], Step [1400/4367], Loss: 0.4107
2025-02-18 15:34:27,537 - Epoch [30/1000], Step [1500/4367], Loss: 0.5089
2025-02-18 15:34:53,352 - Epoch [30/1000], Step [1600/4367], Loss: 0.3341
2025-02-18 15:35:19,355 - Epoch [30/1000], Step [1700/4367], Loss: 0.4224
2025-02-18 15:35:45,648 - Epoch [30/1000], Step [1800/4367], Loss: 0.5279
2025-02-18 15:36:11,633 - Epoch [30/1000], Step [1900/4367], Loss: 0.2131
2025-02-18 15:36:37,593 - Epoch [30/1000], Step [2000/4367], Loss: 0.4923
2025-02-18 15:37:03,495 - Epoch [30/1000], Step [2100/4367], Loss: 0.3418
2025-02-18 15:37:29,488 - Epoch [30/1000], Step [2200/4367], Loss: 0.2104
2025-02-18 15:37:55,276 - Epoch [30/1000], Step [2300/4367], Loss: 0.1934
2025-02-18 15:38:21,502 - Epoch [30/1000], Step [2400/4367], Loss: 0.3635
2025-02-18 15:38:47,378 - Epoch [30/1000], Step [2500/4367], Loss: 0.3031
2025-02-18 15:39:13,146 - Epoch [30/1000], Step [2600/4367], Loss: 0.4581
2025-02-18 15:39:38,937 - Epoch [30/1000], Step [2700/4367], Loss: 0.6009
2025-02-18 15:40:05,023 - Epoch [30/1000], Step [2800/4367], Loss: 0.3676
2025-02-18 15:40:30,751 - Epoch [30/1000], Step [2900/4367], Loss: 0.5687
2025-02-18 15:40:56,791 - Epoch [30/1000], Step [3000/4367], Loss: 0.7510
2025-02-18 15:41:22,768 - Epoch [30/1000], Step [3100/4367], Loss: 0.3648
2025-02-18 15:41:48,863 - Epoch [30/1000], Step [3200/4367], Loss: 0.4667
2025-02-18 15:42:14,991 - Epoch [30/1000], Step [3300/4367], Loss: 0.3580
2025-02-18 15:42:41,184 - Epoch [30/1000], Step [3400/4367], Loss: 0.4580
2025-02-18 15:43:07,298 - Epoch [30/1000], Step [3500/4367], Loss: 0.4137
2025-02-18 15:43:33,296 - Epoch [30/1000], Step [3600/4367], Loss: 0.1691
2025-02-18 15:43:59,403 - Epoch [30/1000], Step [3700/4367], Loss: 0.5609
2025-02-18 15:44:25,650 - Epoch [30/1000], Step [3800/4367], Loss: 0.3485
2025-02-18 15:44:51,698 - Epoch [30/1000], Step [3900/4367], Loss: 0.5264
2025-02-18 15:45:17,570 - Epoch [30/1000], Step [4000/4367], Loss: 0.0983
2025-02-18 15:45:43,283 - Epoch [30/1000], Step [4100/4367], Loss: 0.2122
2025-02-18 15:46:09,424 - Epoch [30/1000], Step [4200/4367], Loss: 0.3687
2025-02-18 15:46:35,758 - Epoch [30/1000], Step [4300/4367], Loss: 0.1987
2025-02-18 15:47:02,484 - Epoch [30/1000], Validation Step [100/1090], Val Loss: 0.0028
2025-02-18 15:47:09,872 - Epoch [30/1000], Validation Step [200/1090], Val Loss: 0.0000
2025-02-18 15:47:17,371 - Epoch [30/1000], Validation Step [300/1090], Val Loss: 0.7614
2025-02-18 15:47:25,113 - Epoch [30/1000], Validation Step [400/1090], Val Loss: 0.2428
2025-02-18 15:47:32,416 - Epoch [30/1000], Validation Step [500/1090], Val Loss: 0.5117
2025-02-18 15:47:40,049 - Epoch [30/1000], Validation Step [600/1090], Val Loss: 0.6901
2025-02-18 15:47:47,737 - Epoch [30/1000], Validation Step [700/1090], Val Loss: 0.5875
2025-02-18 15:47:54,802 - Epoch [30/1000], Validation Step [800/1090], Val Loss: 0.1467
2025-02-18 15:48:01,758 - Epoch [30/1000], Validation Step [900/1090], Val Loss: 0.2176
2025-02-18 15:48:09,210 - Epoch [30/1000], Validation Step [1000/1090], Val Loss: 0.0337
2025-02-18 15:48:16,219 - Epoch 30/1000, Train Loss: 0.3919, Val Loss: 0.4068, Accuracy: 85.27%
2025-02-18 15:48:17,212 - Model saved at /data2/personal/sungjin/korean_dialects/classification_second_checkpoint/checkpoint_epoch_30.pth
2025-02-18 15:48:45,735 - Epoch [31/1000], Step [100/4367], Loss: 0.4603
2025-02-18 15:49:11,755 - Epoch [31/1000], Step [200/4367], Loss: 0.4060
2025-02-18 15:49:37,989 - Epoch [31/1000], Step [300/4367], Loss: 0.5892
2025-02-18 15:50:03,872 - Epoch [31/1000], Step [400/4367], Loss: 0.4105
2025-02-18 15:50:30,014 - Epoch [31/1000], Step [500/4367], Loss: 0.2763
2025-02-18 15:50:55,919 - Epoch [31/1000], Step [600/4367], Loss: 0.3066
2025-02-18 15:51:21,660 - Epoch [31/1000], Step [700/4367], Loss: 0.5767
2025-02-18 15:51:47,477 - Epoch [31/1000], Step [800/4367], Loss: 0.3691
2025-02-18 15:52:13,492 - Epoch [31/1000], Step [900/4367], Loss: 0.2543
2025-02-18 15:52:39,292 - Epoch [31/1000], Step [1000/4367], Loss: 0.2856
2025-02-18 15:53:05,032 - Epoch [31/1000], Step [1100/4367], Loss: 0.3136
2025-02-18 15:53:31,197 - Epoch [31/1000], Step [1200/4367], Loss: 0.3170
2025-02-18 15:53:57,337 - Epoch [31/1000], Step [1300/4367], Loss: 0.5840
2025-02-18 15:54:23,089 - Epoch [31/1000], Step [1400/4367], Loss: 0.4488
2025-02-18 15:54:49,088 - Epoch [31/1000], Step [1500/4367], Loss: 0.4057
2025-02-18 15:55:14,952 - Epoch [31/1000], Step [1600/4367], Loss: 0.4818
2025-02-18 15:55:40,758 - Epoch [31/1000], Step [1700/4367], Loss: 0.4976
2025-02-18 15:56:06,626 - Epoch [31/1000], Step [1800/4367], Loss: 0.2481
2025-02-18 15:56:32,532 - Epoch [31/1000], Step [1900/4367], Loss: 0.2901
2025-02-18 15:56:58,554 - Epoch [31/1000], Step [2000/4367], Loss: 0.2944
2025-02-18 15:57:24,585 - Epoch [31/1000], Step [2100/4367], Loss: 0.3129
2025-02-18 15:57:50,552 - Epoch [31/1000], Step [2200/4367], Loss: 0.3537
2025-02-18 15:58:16,438 - Epoch [31/1000], Step [2300/4367], Loss: 0.3365
2025-02-18 15:58:42,438 - Epoch [31/1000], Step [2400/4367], Loss: 0.1890
2025-02-18 15:59:08,402 - Epoch [31/1000], Step [2500/4367], Loss: 0.2787
2025-02-18 15:59:34,250 - Epoch [31/1000], Step [2600/4367], Loss: 0.5038
2025-02-18 16:00:00,155 - Epoch [31/1000], Step [2700/4367], Loss: 0.6454
2025-02-18 16:00:26,200 - Epoch [31/1000], Step [2800/4367], Loss: 0.5330
2025-02-18 16:00:51,975 - Epoch [31/1000], Step [2900/4367], Loss: 0.5440
2025-02-18 16:01:17,683 - Epoch [31/1000], Step [3000/4367], Loss: 0.3447
2025-02-18 16:01:43,367 - Epoch [31/1000], Step [3100/4367], Loss: 0.1892
2025-02-18 16:02:09,428 - Epoch [31/1000], Step [3200/4367], Loss: 0.4801
2025-02-18 16:02:35,489 - Epoch [31/1000], Step [3300/4367], Loss: 0.5385
2025-02-18 16:03:01,240 - Epoch [31/1000], Step [3400/4367], Loss: 0.3957
2025-02-18 16:03:27,060 - Epoch [31/1000], Step [3500/4367], Loss: 0.3644
2025-02-18 16:03:52,854 - Epoch [31/1000], Step [3600/4367], Loss: 0.4984
2025-02-18 16:04:18,869 - Epoch [31/1000], Step [3700/4367], Loss: 0.3266
2025-02-18 16:04:44,770 - Epoch [31/1000], Step [3800/4367], Loss: 0.6416
2025-02-18 16:05:10,527 - Epoch [31/1000], Step [3900/4367], Loss: 0.2878
2025-02-18 16:05:36,467 - Epoch [31/1000], Step [4000/4367], Loss: 0.5071
2025-02-18 16:06:02,333 - Epoch [31/1000], Step [4100/4367], Loss: 0.3243
2025-02-18 16:06:28,088 - Epoch [31/1000], Step [4200/4367], Loss: 0.3081
2025-02-18 16:06:54,021 - Epoch [31/1000], Step [4300/4367], Loss: 0.5865
2025-02-18 16:07:21,298 - Epoch [31/1000], Validation Step [100/1090], Val Loss: 0.0002
2025-02-18 16:07:28,685 - Epoch [31/1000], Validation Step [200/1090], Val Loss: 0.0085
2025-02-18 16:07:36,150 - Epoch [31/1000], Validation Step [300/1090], Val Loss: 0.6847
2025-02-18 16:07:43,916 - Epoch [31/1000], Validation Step [400/1090], Val Loss: 0.3367
2025-02-18 16:07:51,241 - Epoch [31/1000], Validation Step [500/1090], Val Loss: 0.3633
2025-02-18 16:07:58,894 - Epoch [31/1000], Validation Step [600/1090], Val Loss: 0.6590
2025-02-18 16:08:06,541 - Epoch [31/1000], Validation Step [700/1090], Val Loss: 0.3631
2025-02-18 16:08:13,586 - Epoch [31/1000], Validation Step [800/1090], Val Loss: 0.1888
2025-02-18 16:08:20,504 - Epoch [31/1000], Validation Step [900/1090], Val Loss: 0.3191
2025-02-18 16:08:27,945 - Epoch [31/1000], Validation Step [1000/1090], Val Loss: 0.0814
2025-02-18 16:08:34,891 - Epoch 31/1000, Train Loss: 0.3870, Val Loss: 0.4048, Accuracy: 85.27%
2025-02-18 16:09:03,819 - Epoch [32/1000], Step [100/4367], Loss: 0.3830
2025-02-18 16:09:29,704 - Epoch [32/1000], Step [200/4367], Loss: 0.2380
2025-02-18 16:09:55,619 - Epoch [32/1000], Step [300/4367], Loss: 0.2311
2025-02-18 16:10:21,843 - Epoch [32/1000], Step [400/4367], Loss: 0.4971
2025-02-18 16:10:47,917 - Epoch [32/1000], Step [500/4367], Loss: 0.4394
2025-02-18 16:11:13,962 - Epoch [32/1000], Step [600/4367], Loss: 0.3164
2025-02-18 16:11:39,795 - Epoch [32/1000], Step [700/4367], Loss: 0.5120
2025-02-18 16:12:05,621 - Epoch [32/1000], Step [800/4367], Loss: 0.4429
2025-02-18 16:12:31,276 - Epoch [32/1000], Step [900/4367], Loss: 0.3823
2025-02-18 16:12:57,208 - Epoch [32/1000], Step [1000/4367], Loss: 0.8756
2025-02-18 16:13:23,417 - Epoch [32/1000], Step [1100/4367], Loss: 0.4829
2025-02-18 16:13:49,207 - Epoch [32/1000], Step [1200/4367], Loss: 0.3072
2025-02-18 16:14:15,228 - Epoch [32/1000], Step [1300/4367], Loss: 0.2754
2025-02-18 16:14:41,326 - Epoch [32/1000], Step [1400/4367], Loss: 0.6180
2025-02-18 16:15:07,369 - Epoch [32/1000], Step [1500/4367], Loss: 0.1854
2025-02-18 16:15:33,513 - Epoch [32/1000], Step [1600/4367], Loss: 0.5057
2025-02-18 16:15:59,644 - Epoch [32/1000], Step [1700/4367], Loss: 0.2335
2025-02-18 16:16:25,905 - Epoch [32/1000], Step [1800/4367], Loss: 0.5791
2025-02-18 16:16:51,801 - Epoch [32/1000], Step [1900/4367], Loss: 0.5507
2025-02-18 16:17:17,956 - Epoch [32/1000], Step [2000/4367], Loss: 0.6478
2025-02-18 16:17:43,658 - Epoch [32/1000], Step [2100/4367], Loss: 0.6527
2025-02-18 16:18:09,981 - Epoch [32/1000], Step [2200/4367], Loss: 0.4689
2025-02-18 16:18:36,186 - Epoch [32/1000], Step [2300/4367], Loss: 0.6987
2025-02-18 16:19:02,323 - Epoch [32/1000], Step [2400/4367], Loss: 0.2583
2025-02-18 16:19:28,299 - Epoch [32/1000], Step [2500/4367], Loss: 0.4396
2025-02-18 16:19:54,416 - Epoch [32/1000], Step [2600/4367], Loss: 0.1376
2025-02-18 16:20:20,477 - Epoch [32/1000], Step [2700/4367], Loss: 0.6217
2025-02-18 16:20:46,359 - Epoch [32/1000], Step [2800/4367], Loss: 0.3343
2025-02-18 16:21:12,592 - Epoch [32/1000], Step [2900/4367], Loss: 0.6830
2025-02-18 16:21:38,426 - Epoch [32/1000], Step [3000/4367], Loss: 0.4102
2025-02-18 16:22:04,398 - Epoch [32/1000], Step [3100/4367], Loss: 0.4333
2025-02-18 16:22:30,557 - Epoch [32/1000], Step [3200/4367], Loss: 0.4811
2025-02-18 16:22:56,666 - Epoch [32/1000], Step [3300/4367], Loss: 0.5406
2025-02-18 16:23:22,615 - Epoch [32/1000], Step [3400/4367], Loss: 0.4040
2025-02-18 16:23:48,783 - Epoch [32/1000], Step [3500/4367], Loss: 0.2327
2025-02-18 16:24:14,872 - Epoch [32/1000], Step [3600/4367], Loss: 0.4966
2025-02-18 16:24:41,032 - Epoch [32/1000], Step [3700/4367], Loss: 0.4387
2025-02-18 16:25:07,665 - Epoch [32/1000], Step [3800/4367], Loss: 0.4905
2025-02-18 16:25:33,732 - Epoch [32/1000], Step [3900/4367], Loss: 0.5393
2025-02-18 16:26:00,527 - Epoch [32/1000], Step [4000/4367], Loss: 0.3118
2025-02-18 16:26:26,653 - Epoch [32/1000], Step [4100/4367], Loss: 0.4837
2025-02-18 16:26:52,750 - Epoch [32/1000], Step [4200/4367], Loss: 0.4723
2025-02-18 16:27:19,045 - Epoch [32/1000], Step [4300/4367], Loss: 0.5132
2025-02-18 16:27:47,109 - Epoch [32/1000], Validation Step [100/1090], Val Loss: 0.0000
2025-02-18 16:27:54,904 - Epoch [32/1000], Validation Step [200/1090], Val Loss: 0.0000
2025-02-18 16:28:02,848 - Epoch [32/1000], Validation Step [300/1090], Val Loss: 1.3651
2025-02-18 16:28:11,239 - Epoch [32/1000], Validation Step [400/1090], Val Loss: 1.0441
2025-02-18 16:28:18,708 - Epoch [32/1000], Validation Step [500/1090], Val Loss: 0.4929
2025-02-18 16:28:26,502 - Epoch [32/1000], Validation Step [600/1090], Val Loss: 1.3445
2025-02-18 16:28:34,373 - Epoch [32/1000], Validation Step [700/1090], Val Loss: 0.5854
2025-02-18 16:28:41,648 - Epoch [32/1000], Validation Step [800/1090], Val Loss: 0.2511
2025-02-18 16:28:48,679 - Epoch [32/1000], Validation Step [900/1090], Val Loss: 0.6514
2025-02-18 16:28:56,365 - Epoch [32/1000], Validation Step [1000/1090], Val Loss: 0.0591
2025-02-18 16:29:03,470 - Epoch 32/1000, Train Loss: 0.4606, Val Loss: 0.5962, Accuracy: 80.45%
2025-02-18 16:29:04,170 - Model saved at /data2/personal/sungjin/korean_dialects/classification_second_checkpoint/checkpoint_epoch_32.pth
2025-02-18 16:29:33,060 - Epoch [33/1000], Step [100/4367], Loss: 0.4723
2025-02-18 16:29:59,154 - Epoch [33/1000], Step [200/4367], Loss: 0.3763
2025-02-18 16:30:25,266 - Epoch [33/1000], Step [300/4367], Loss: 0.3165
2025-02-18 16:30:51,264 - Epoch [33/1000], Step [400/4367], Loss: 0.3510
2025-02-18 16:31:17,507 - Epoch [33/1000], Step [500/4367], Loss: 0.2128
2025-02-18 16:31:43,784 - Epoch [33/1000], Step [600/4367], Loss: 0.4686
2025-02-18 16:32:10,094 - Epoch [33/1000], Step [700/4367], Loss: 0.4661
2025-02-18 16:32:36,226 - Epoch [33/1000], Step [800/4367], Loss: 0.2381
2025-02-18 16:33:02,596 - Epoch [33/1000], Step [900/4367], Loss: 0.3140
2025-02-18 16:33:28,567 - Epoch [33/1000], Step [1000/4367], Loss: 0.1706
2025-02-18 16:33:55,125 - Epoch [33/1000], Step [1100/4367], Loss: 0.4057
2025-02-18 16:34:21,238 - Epoch [33/1000], Step [1200/4367], Loss: 0.2207
2025-02-18 16:34:47,515 - Epoch [33/1000], Step [1300/4367], Loss: 0.4145
2025-02-18 16:35:13,559 - Epoch [33/1000], Step [1400/4367], Loss: 0.5098
2025-02-18 16:35:39,461 - Epoch [33/1000], Step [1500/4367], Loss: 0.5575
2025-02-18 16:36:05,858 - Epoch [33/1000], Step [1600/4367], Loss: 0.2632
2025-02-18 16:36:32,080 - Epoch [33/1000], Step [1700/4367], Loss: 0.3632
2025-02-18 16:36:58,432 - Epoch [33/1000], Step [1800/4367], Loss: 0.3143
2025-02-18 16:37:24,784 - Epoch [33/1000], Step [1900/4367], Loss: 0.3595
2025-02-18 16:37:50,933 - Epoch [33/1000], Step [2000/4367], Loss: 0.4407
2025-02-18 16:38:17,124 - Epoch [33/1000], Step [2100/4367], Loss: 0.3702
2025-02-18 16:38:43,334 - Epoch [33/1000], Step [2200/4367], Loss: 0.9043
2025-02-18 16:39:09,451 - Epoch [33/1000], Step [2300/4367], Loss: 0.3180
2025-02-18 16:39:35,464 - Epoch [33/1000], Step [2400/4367], Loss: 0.5016
2025-02-18 16:40:01,704 - Epoch [33/1000], Step [2500/4367], Loss: 0.2780
2025-02-18 16:40:27,852 - Epoch [33/1000], Step [2600/4367], Loss: 0.2546
2025-02-18 16:40:54,063 - Epoch [33/1000], Step [2700/4367], Loss: 0.3617
2025-02-18 16:41:20,057 - Epoch [33/1000], Step [2800/4367], Loss: 0.4688
2025-02-18 16:41:46,075 - Epoch [33/1000], Step [2900/4367], Loss: 0.4749
2025-02-18 16:42:12,233 - Epoch [33/1000], Step [3000/4367], Loss: 0.3513
2025-02-18 16:42:38,454 - Epoch [33/1000], Step [3100/4367], Loss: 0.4312
2025-02-18 16:43:04,556 - Epoch [33/1000], Step [3200/4367], Loss: 0.4277
2025-02-18 16:43:30,749 - Epoch [33/1000], Step [3300/4367], Loss: 0.1735
2025-02-18 16:43:56,877 - Epoch [33/1000], Step [3400/4367], Loss: 0.3182
2025-02-18 16:44:23,322 - Epoch [33/1000], Step [3500/4367], Loss: 0.2279
2025-02-18 16:44:49,373 - Epoch [33/1000], Step [3600/4367], Loss: 0.2730
2025-02-18 16:45:15,437 - Epoch [33/1000], Step [3700/4367], Loss: 0.4878
2025-02-18 16:45:41,510 - Epoch [33/1000], Step [3800/4367], Loss: 0.3774
2025-02-18 16:46:07,784 - Epoch [33/1000], Step [3900/4367], Loss: 0.4697
2025-02-18 16:46:34,018 - Epoch [33/1000], Step [4000/4367], Loss: 0.5846
2025-02-18 16:47:00,164 - Epoch [33/1000], Step [4100/4367], Loss: 0.2822
2025-02-18 16:47:26,086 - Epoch [33/1000], Step [4200/4367], Loss: 0.2709
2025-02-18 16:47:52,311 - Epoch [33/1000], Step [4300/4367], Loss: 0.4610
2025-02-18 16:48:20,939 - Epoch [33/1000], Validation Step [100/1090], Val Loss: 0.0001
2025-02-18 16:48:28,481 - Epoch [33/1000], Validation Step [200/1090], Val Loss: 0.0000
2025-02-18 16:48:36,266 - Epoch [33/1000], Validation Step [300/1090], Val Loss: 0.7261
2025-02-18 16:48:44,296 - Epoch [33/1000], Validation Step [400/1090], Val Loss: 0.2248
2025-02-18 16:48:51,902 - Epoch [33/1000], Validation Step [500/1090], Val Loss: 0.5301
2025-02-18 16:48:59,697 - Epoch [33/1000], Validation Step [600/1090], Val Loss: 1.0212
2025-02-18 16:49:07,806 - Epoch [33/1000], Validation Step [700/1090], Val Loss: 0.5085
2025-02-18 16:49:15,158 - Epoch [33/1000], Validation Step [800/1090], Val Loss: 0.1846
2025-02-18 16:49:22,457 - Epoch [33/1000], Validation Step [900/1090], Val Loss: 0.3952
2025-02-18 16:49:30,205 - Epoch [33/1000], Validation Step [1000/1090], Val Loss: 0.0171
2025-02-18 16:49:37,403 - Epoch 33/1000, Train Loss: 0.4431, Val Loss: 0.4539, Accuracy: 83.84%
2025-02-18 16:50:07,457 - Epoch [34/1000], Step [100/4367], Loss: 0.4468
2025-02-18 16:50:33,226 - Epoch [34/1000], Step [200/4367], Loss: 0.5043
2025-02-18 16:50:59,481 - Epoch [34/1000], Step [300/4367], Loss: 0.7962
2025-02-18 16:51:25,149 - Epoch [34/1000], Step [400/4367], Loss: 0.4535
2025-02-18 16:51:51,096 - Epoch [34/1000], Step [500/4367], Loss: 0.2233
2025-02-18 16:52:17,151 - Epoch [34/1000], Step [600/4367], Loss: 0.7311
2025-02-18 16:52:43,154 - Epoch [34/1000], Step [700/4367], Loss: 0.5042
2025-02-18 16:53:09,489 - Epoch [34/1000], Step [800/4367], Loss: 0.2371
2025-02-18 16:53:35,475 - Epoch [34/1000], Step [900/4367], Loss: 0.3010
2025-02-18 16:54:01,810 - Epoch [34/1000], Step [1000/4367], Loss: 0.3609
2025-02-18 16:54:27,946 - Epoch [34/1000], Step [1100/4367], Loss: 0.5080
2025-02-18 16:54:53,930 - Epoch [34/1000], Step [1200/4367], Loss: 0.4129
2025-02-18 16:55:20,374 - Epoch [34/1000], Step [1300/4367], Loss: 0.1882
2025-02-18 16:55:46,429 - Epoch [34/1000], Step [1400/4367], Loss: 0.5753
2025-02-18 16:56:12,850 - Epoch [34/1000], Step [1500/4367], Loss: 0.5104
2025-02-18 16:56:38,930 - Epoch [34/1000], Step [1600/4367], Loss: 0.3165
2025-02-18 16:57:05,027 - Epoch [34/1000], Step [1700/4367], Loss: 0.5012
2025-02-18 16:57:31,449 - Epoch [34/1000], Step [1800/4367], Loss: 0.3174
2025-02-18 16:57:57,734 - Epoch [34/1000], Step [1900/4367], Loss: 0.4710
2025-02-18 16:58:23,929 - Epoch [34/1000], Step [2000/4367], Loss: 0.8320
2025-02-18 16:58:49,979 - Epoch [34/1000], Step [2100/4367], Loss: 0.3850
2025-02-18 16:59:15,914 - Epoch [34/1000], Step [2200/4367], Loss: 0.3584
2025-02-18 16:59:42,276 - Epoch [34/1000], Step [2300/4367], Loss: 0.6604
2025-02-18 17:00:08,335 - Epoch [34/1000], Step [2400/4367], Loss: 0.3740
2025-02-18 17:00:34,643 - Epoch [34/1000], Step [2500/4367], Loss: 0.6168
2025-02-18 17:01:01,065 - Epoch [34/1000], Step [2600/4367], Loss: 0.2319
2025-02-18 17:01:27,613 - Epoch [34/1000], Step [2700/4367], Loss: 0.3029
2025-02-18 17:01:53,572 - Epoch [34/1000], Step [2800/4367], Loss: 0.1750
2025-02-18 17:02:19,767 - Epoch [34/1000], Step [2900/4367], Loss: 0.6892
2025-02-18 17:02:46,266 - Epoch [34/1000], Step [3000/4367], Loss: 0.5120
2025-02-18 17:03:12,552 - Epoch [34/1000], Step [3100/4367], Loss: 0.5381
2025-02-18 17:03:38,760 - Epoch [34/1000], Step [3200/4367], Loss: 0.3829
2025-02-18 17:04:04,747 - Epoch [34/1000], Step [3300/4367], Loss: 0.3489
2025-02-18 17:04:31,051 - Epoch [34/1000], Step [3400/4367], Loss: 0.5346
2025-02-18 17:04:57,105 - Epoch [34/1000], Step [3500/4367], Loss: 0.1605
2025-02-18 17:05:23,433 - Epoch [34/1000], Step [3600/4367], Loss: 0.8316
2025-02-18 17:05:49,640 - Epoch [34/1000], Step [3700/4367], Loss: 0.4941
2025-02-18 17:06:15,818 - Epoch [34/1000], Step [3800/4367], Loss: 0.4776
2025-02-18 17:06:41,813 - Epoch [34/1000], Step [3900/4367], Loss: 0.2937
2025-02-18 17:07:08,141 - Epoch [34/1000], Step [4000/4367], Loss: 0.3896
2025-02-18 17:07:34,231 - Epoch [34/1000], Step [4100/4367], Loss: 0.3611
2025-02-18 17:08:00,576 - Epoch [34/1000], Step [4200/4367], Loss: 0.2819
2025-02-18 17:08:26,874 - Epoch [34/1000], Step [4300/4367], Loss: 0.3545
2025-02-18 17:08:55,812 - Epoch [34/1000], Validation Step [100/1090], Val Loss: 0.0001
2025-02-18 17:09:04,099 - Epoch [34/1000], Validation Step [200/1090], Val Loss: 0.0000
2025-02-18 17:09:12,907 - Epoch [34/1000], Validation Step [300/1090], Val Loss: 0.6811
2025-02-18 17:09:21,460 - Epoch [34/1000], Validation Step [400/1090], Val Loss: 0.3356
2025-02-18 17:09:29,562 - Epoch [34/1000], Validation Step [500/1090], Val Loss: 0.5945
2025-02-18 17:09:38,206 - Epoch [34/1000], Validation Step [600/1090], Val Loss: 0.8503
2025-02-18 17:09:46,728 - Epoch [34/1000], Validation Step [700/1090], Val Loss: 0.4294
2025-02-18 17:09:55,020 - Epoch [34/1000], Validation Step [800/1090], Val Loss: 0.1035
2025-02-18 17:10:02,732 - Epoch [34/1000], Validation Step [900/1090], Val Loss: 0.1561
2025-02-18 17:10:11,112 - Epoch [34/1000], Validation Step [1000/1090], Val Loss: 0.0403
2025-02-18 17:10:18,749 - Epoch 34/1000, Train Loss: 0.4174, Val Loss: 0.4164, Accuracy: 84.85%
2025-02-18 17:10:19,532 - Model saved at /data2/personal/sungjin/korean_dialects/classification_second_checkpoint/checkpoint_epoch_34.pth
2025-02-18 17:10:49,823 - Epoch [35/1000], Step [100/4367], Loss: 0.2062
2025-02-18 17:11:16,669 - Epoch [35/1000], Step [200/4367], Loss: 0.3614
2025-02-18 17:11:43,091 - Epoch [35/1000], Step [300/4367], Loss: 0.5099
2025-02-18 17:12:09,822 - Epoch [35/1000], Step [400/4367], Loss: 0.3750
2025-02-18 17:12:36,416 - Epoch [35/1000], Step [500/4367], Loss: 0.3944
2025-02-18 17:13:03,241 - Epoch [35/1000], Step [600/4367], Loss: 0.4164
2025-02-18 17:13:29,946 - Epoch [35/1000], Step [700/4367], Loss: 0.5574
2025-02-18 17:13:56,900 - Epoch [35/1000], Step [800/4367], Loss: 0.3048
2025-02-18 17:14:23,956 - Epoch [35/1000], Step [900/4367], Loss: 0.5620
2025-02-18 17:14:50,549 - Epoch [35/1000], Step [1000/4367], Loss: 0.7110
2025-02-18 17:15:17,394 - Epoch [35/1000], Step [1100/4367], Loss: 0.6813
2025-02-18 17:15:43,655 - Epoch [35/1000], Step [1200/4367], Loss: 0.4110
2025-02-18 17:16:09,908 - Epoch [35/1000], Step [1300/4367], Loss: 0.6304
2025-02-18 17:16:36,542 - Epoch [35/1000], Step [1400/4367], Loss: 0.4100
2025-02-18 17:17:03,706 - Epoch [35/1000], Step [1500/4367], Loss: 0.5909
2025-02-18 17:17:30,697 - Epoch [35/1000], Step [1600/4367], Loss: 0.4036
2025-02-18 17:17:57,436 - Epoch [35/1000], Step [1700/4367], Loss: 0.2825
2025-02-18 17:18:23,908 - Epoch [35/1000], Step [1800/4367], Loss: 0.2660
2025-02-18 17:18:50,670 - Epoch [35/1000], Step [1900/4367], Loss: 0.3833
2025-02-18 17:19:17,568 - Epoch [35/1000], Step [2000/4367], Loss: 0.1857
2025-02-18 17:19:44,169 - Epoch [35/1000], Step [2100/4367], Loss: 0.2186
2025-02-18 17:20:10,774 - Epoch [35/1000], Step [2200/4367], Loss: 0.4884
2025-02-18 17:20:37,477 - Epoch [35/1000], Step [2300/4367], Loss: 0.3343
2025-02-18 17:21:04,242 - Epoch [35/1000], Step [2400/4367], Loss: 0.2428
2025-02-18 17:21:30,440 - Epoch [35/1000], Step [2500/4367], Loss: 0.2960
2025-02-18 17:21:57,175 - Epoch [35/1000], Step [2600/4367], Loss: 0.4114
2025-02-18 17:22:24,098 - Epoch [35/1000], Step [2700/4367], Loss: 0.2795
2025-02-18 17:22:50,773 - Epoch [35/1000], Step [2800/4367], Loss: 0.5787
2025-02-18 17:23:17,448 - Epoch [35/1000], Step [2900/4367], Loss: 0.4809
2025-02-18 17:23:44,236 - Epoch [35/1000], Step [3000/4367], Loss: 0.5156
2025-02-18 17:24:11,504 - Epoch [35/1000], Step [3100/4367], Loss: 0.3705
2025-02-18 17:24:37,757 - Epoch [35/1000], Step [3200/4367], Loss: 0.3847
2025-02-18 17:25:04,261 - Epoch [35/1000], Step [3300/4367], Loss: 0.7133
2025-02-18 17:25:30,929 - Epoch [35/1000], Step [3400/4367], Loss: 0.3894
2025-02-18 17:25:57,803 - Epoch [35/1000], Step [3500/4367], Loss: 0.2494
2025-02-18 17:26:24,243 - Epoch [35/1000], Step [3600/4367], Loss: 0.3366
2025-02-18 17:26:51,259 - Epoch [35/1000], Step [3700/4367], Loss: 0.6473
2025-02-18 17:27:18,128 - Epoch [35/1000], Step [3800/4367], Loss: 0.3495
2025-02-18 17:27:44,580 - Epoch [35/1000], Step [3900/4367], Loss: 0.3349
2025-02-18 17:28:11,952 - Epoch [35/1000], Step [4000/4367], Loss: 0.4642
2025-02-18 17:28:38,783 - Epoch [35/1000], Step [4100/4367], Loss: 0.3774
2025-02-18 17:29:05,259 - Epoch [35/1000], Step [4200/4367], Loss: 0.5567
2025-02-18 17:29:31,726 - Epoch [35/1000], Step [4300/4367], Loss: 0.3274
2025-02-18 17:30:01,386 - Epoch [35/1000], Validation Step [100/1090], Val Loss: 0.0007
2025-02-18 17:30:09,715 - Epoch [35/1000], Validation Step [200/1090], Val Loss: 0.0002
2025-02-18 17:30:18,527 - Epoch [35/1000], Validation Step [300/1090], Val Loss: 0.6406
2025-02-18 17:30:26,891 - Epoch [35/1000], Validation Step [400/1090], Val Loss: 0.3781
2025-02-18 17:30:34,869 - Epoch [35/1000], Validation Step [500/1090], Val Loss: 0.7270
2025-02-18 17:30:43,532 - Epoch [35/1000], Validation Step [600/1090], Val Loss: 0.6273
2025-02-18 17:30:51,778 - Epoch [35/1000], Validation Step [700/1090], Val Loss: 0.3486
2025-02-18 17:30:59,794 - Epoch [35/1000], Validation Step [800/1090], Val Loss: 0.2281
2025-02-18 17:31:07,344 - Epoch [35/1000], Validation Step [900/1090], Val Loss: 0.3148
2025-02-18 17:31:15,710 - Epoch [35/1000], Validation Step [1000/1090], Val Loss: 0.0506
2025-02-18 17:31:23,086 - Epoch 35/1000, Train Loss: 0.4124, Val Loss: 0.4115, Accuracy: 84.98%
2025-02-18 17:31:53,519 - Epoch [36/1000], Step [100/4367], Loss: 0.4163
2025-02-18 17:32:20,433 - Epoch [36/1000], Step [200/4367], Loss: 0.4063
2025-02-18 17:32:47,233 - Epoch [36/1000], Step [300/4367], Loss: 0.2688
2025-02-18 17:33:13,933 - Epoch [36/1000], Step [400/4367], Loss: 0.3009
2025-02-18 17:33:40,820 - Epoch [36/1000], Step [500/4367], Loss: 0.2679
2025-02-18 17:34:07,686 - Epoch [36/1000], Step [600/4367], Loss: 0.4639
2025-02-18 17:34:34,300 - Epoch [36/1000], Step [700/4367], Loss: 0.4963
2025-02-18 17:35:00,884 - Epoch [36/1000], Step [800/4367], Loss: 0.5897
2025-02-18 17:35:27,911 - Epoch [36/1000], Step [900/4367], Loss: 0.4749
2025-02-18 17:35:54,611 - Epoch [36/1000], Step [1000/4367], Loss: 0.4314
2025-02-18 17:36:21,360 - Epoch [36/1000], Step [1100/4367], Loss: 0.4354
2025-02-18 17:36:48,491 - Epoch [36/1000], Step [1200/4367], Loss: 0.4895
2025-02-18 17:37:15,225 - Epoch [36/1000], Step [1300/4367], Loss: 0.2327
2025-02-18 17:37:42,246 - Epoch [36/1000], Step [1400/4367], Loss: 0.6359
2025-02-18 17:38:08,900 - Epoch [36/1000], Step [1500/4367], Loss: 0.2814
2025-02-18 17:38:35,730 - Epoch [36/1000], Step [1600/4367], Loss: 0.3778
2025-02-18 17:39:02,400 - Epoch [36/1000], Step [1700/4367], Loss: 0.3199
2025-02-18 17:39:29,142 - Epoch [36/1000], Step [1800/4367], Loss: 0.5830
2025-02-18 17:39:56,107 - Epoch [36/1000], Step [1900/4367], Loss: 0.2259
2025-02-18 17:40:22,678 - Epoch [36/1000], Step [2000/4367], Loss: 0.4369
2025-02-18 17:40:49,450 - Epoch [36/1000], Step [2100/4367], Loss: 0.4469
2025-02-18 17:41:16,461 - Epoch [36/1000], Step [2200/4367], Loss: 0.5578
2025-02-18 17:41:43,305 - Epoch [36/1000], Step [2300/4367], Loss: 0.5092
2025-02-18 17:42:10,166 - Epoch [36/1000], Step [2400/4367], Loss: 0.4757
2025-02-18 17:42:36,474 - Epoch [36/1000], Step [2500/4367], Loss: 0.5714
2025-02-18 17:43:02,992 - Epoch [36/1000], Step [2600/4367], Loss: 0.2430
2025-02-18 17:43:29,340 - Epoch [36/1000], Step [2700/4367], Loss: 0.5667
2025-02-18 17:43:55,959 - Epoch [36/1000], Step [2800/4367], Loss: 0.5528
2025-02-18 17:44:22,407 - Epoch [36/1000], Step [2900/4367], Loss: 0.4848
2025-02-18 17:44:49,052 - Epoch [36/1000], Step [3000/4367], Loss: 0.6880
2025-02-18 17:45:15,583 - Epoch [36/1000], Step [3100/4367], Loss: 0.2990
2025-02-18 17:45:42,603 - Epoch [36/1000], Step [3200/4367], Loss: 0.3651
2025-02-18 17:46:09,046 - Epoch [36/1000], Step [3300/4367], Loss: 0.1878
2025-02-18 17:46:36,062 - Epoch [36/1000], Step [3400/4367], Loss: 0.6415
2025-02-18 17:47:02,474 - Epoch [36/1000], Step [3500/4367], Loss: 0.6198
2025-02-18 17:47:28,997 - Epoch [36/1000], Step [3600/4367], Loss: 0.5985
2025-02-18 17:47:55,870 - Epoch [36/1000], Step [3700/4367], Loss: 0.4698
2025-02-18 17:48:22,736 - Epoch [36/1000], Step [3800/4367], Loss: 0.3305
2025-02-18 17:48:49,352 - Epoch [36/1000], Step [3900/4367], Loss: 0.2460
2025-02-18 17:49:16,423 - Epoch [36/1000], Step [4000/4367], Loss: 0.5935
2025-02-18 17:49:42,987 - Epoch [36/1000], Step [4100/4367], Loss: 0.3554
2025-02-18 17:50:09,933 - Epoch [36/1000], Step [4200/4367], Loss: 0.3808
2025-02-18 17:50:36,659 - Epoch [36/1000], Step [4300/4367], Loss: 0.7904
2025-02-18 17:51:06,613 - Epoch [36/1000], Validation Step [100/1090], Val Loss: 0.0005
2025-02-18 17:51:15,080 - Epoch [36/1000], Validation Step [200/1090], Val Loss: 0.0000
2025-02-18 17:51:23,835 - Epoch [36/1000], Validation Step [300/1090], Val Loss: 0.6313
2025-02-18 17:51:32,191 - Epoch [36/1000], Validation Step [400/1090], Val Loss: 0.4613
2025-02-18 17:51:40,961 - Epoch [36/1000], Validation Step [500/1090], Val Loss: 0.7216
2025-02-18 17:51:49,737 - Epoch [36/1000], Validation Step [600/1090], Val Loss: 0.5467
2025-02-18 17:51:58,512 - Epoch [36/1000], Validation Step [700/1090], Val Loss: 0.2982
2025-02-18 17:52:06,539 - Epoch [36/1000], Validation Step [800/1090], Val Loss: 0.1563
2025-02-18 17:52:14,802 - Epoch [36/1000], Validation Step [900/1090], Val Loss: 0.2917
2025-02-18 17:52:23,668 - Epoch [36/1000], Validation Step [1000/1090], Val Loss: 0.2178
2025-02-18 17:52:31,212 - Epoch 36/1000, Train Loss: 0.4075, Val Loss: 0.4341, Accuracy: 84.36%
2025-02-18 17:52:32,215 - Model saved at /data2/personal/sungjin/korean_dialects/classification_second_checkpoint/checkpoint_epoch_36.pth
2025-02-18 17:53:03,139 - Epoch [37/1000], Step [100/4367], Loss: 0.4322
2025-02-18 17:53:29,301 - Epoch [37/1000], Step [200/4367], Loss: 0.4173
2025-02-18 17:53:56,085 - Epoch [37/1000], Step [300/4367], Loss: 0.1390
2025-02-18 17:54:22,600 - Epoch [37/1000], Step [400/4367], Loss: 0.6724
2025-02-18 17:54:49,325 - Epoch [37/1000], Step [500/4367], Loss: 0.4915
2025-02-18 17:55:16,351 - Epoch [37/1000], Step [600/4367], Loss: 0.5322
2025-02-18 17:55:43,116 - Epoch [37/1000], Step [700/4367], Loss: 0.5330
2025-02-18 17:56:10,166 - Epoch [37/1000], Step [800/4367], Loss: 0.3141
2025-02-18 17:56:37,016 - Epoch [37/1000], Step [900/4367], Loss: 0.5185
2025-02-18 17:57:03,927 - Epoch [37/1000], Step [1000/4367], Loss: 0.3057
2025-02-18 17:57:30,297 - Epoch [37/1000], Step [1100/4367], Loss: 0.5892
2025-02-18 17:57:57,248 - Epoch [37/1000], Step [1200/4367], Loss: 0.2236
2025-02-18 17:58:23,983 - Epoch [37/1000], Step [1300/4367], Loss: 0.4115
2025-02-18 17:58:50,512 - Epoch [37/1000], Step [1400/4367], Loss: 0.2672
2025-02-18 17:59:17,730 - Epoch [37/1000], Step [1500/4367], Loss: 0.4171
2025-02-18 17:59:44,795 - Epoch [37/1000], Step [1600/4367], Loss: 0.7065
2025-02-18 18:00:11,845 - Epoch [37/1000], Step [1700/4367], Loss: 0.4682
2025-02-18 18:00:38,819 - Epoch [37/1000], Step [1800/4367], Loss: 0.4944
2025-02-18 18:01:05,433 - Epoch [37/1000], Step [1900/4367], Loss: 0.4377
2025-02-18 18:01:32,039 - Epoch [37/1000], Step [2000/4367], Loss: 0.1868
2025-02-18 18:01:58,415 - Epoch [37/1000], Step [2100/4367], Loss: 0.1311
2025-02-18 18:02:24,669 - Epoch [37/1000], Step [2200/4367], Loss: 0.1942
2025-02-18 18:02:51,108 - Epoch [37/1000], Step [2300/4367], Loss: 0.1855
2025-02-18 18:03:17,275 - Epoch [37/1000], Step [2400/4367], Loss: 0.4873
2025-02-18 18:03:43,782 - Epoch [37/1000], Step [2500/4367], Loss: 0.3769
2025-02-18 18:04:10,432 - Epoch [37/1000], Step [2600/4367], Loss: 0.0901
2025-02-18 18:04:36,794 - Epoch [37/1000], Step [2700/4367], Loss: 0.5448
2025-02-18 18:05:03,187 - Epoch [37/1000], Step [2800/4367], Loss: 0.4215
2025-02-18 18:05:29,465 - Epoch [37/1000], Step [2900/4367], Loss: 0.5879
2025-02-18 18:05:56,023 - Epoch [37/1000], Step [3000/4367], Loss: 0.4257
2025-02-18 18:06:22,152 - Epoch [37/1000], Step [3100/4367], Loss: 0.1833
2025-02-18 18:06:48,463 - Epoch [37/1000], Step [3200/4367], Loss: 0.3440
2025-02-18 18:07:14,775 - Epoch [37/1000], Step [3300/4367], Loss: 0.1909
2025-02-18 18:07:41,256 - Epoch [37/1000], Step [3400/4367], Loss: 0.2939
2025-02-18 18:08:07,479 - Epoch [37/1000], Step [3500/4367], Loss: 0.2875
2025-02-18 18:08:33,586 - Epoch [37/1000], Step [3600/4367], Loss: 0.6137
2025-02-18 18:09:00,165 - Epoch [37/1000], Step [3700/4367], Loss: 0.3452
2025-02-18 18:09:26,656 - Epoch [37/1000], Step [3800/4367], Loss: 0.3705
2025-02-18 18:09:53,068 - Epoch [37/1000], Step [3900/4367], Loss: 0.2701
2025-02-18 18:10:19,439 - Epoch [37/1000], Step [4000/4367], Loss: 0.3498
2025-02-18 18:10:45,806 - Epoch [37/1000], Step [4100/4367], Loss: 0.3098
2025-02-18 18:11:11,946 - Epoch [37/1000], Step [4200/4367], Loss: 0.3172
2025-02-18 18:11:38,465 - Epoch [37/1000], Step [4300/4367], Loss: 0.3182
2025-02-18 18:12:07,629 - Epoch [37/1000], Validation Step [100/1090], Val Loss: 0.0002
2025-02-18 18:12:15,718 - Epoch [37/1000], Validation Step [200/1090], Val Loss: 0.0000
2025-02-18 18:12:23,899 - Epoch [37/1000], Validation Step [300/1090], Val Loss: 0.7584
2025-02-18 18:12:32,360 - Epoch [37/1000], Validation Step [400/1090], Val Loss: 0.4448
2025-02-18 18:12:40,272 - Epoch [37/1000], Validation Step [500/1090], Val Loss: 0.6166
2025-02-18 18:12:48,692 - Epoch [37/1000], Validation Step [600/1090], Val Loss: 0.5522
2025-02-18 18:12:57,275 - Epoch [37/1000], Validation Step [700/1090], Val Loss: 0.2820
2025-02-18 18:13:04,939 - Epoch [37/1000], Validation Step [800/1090], Val Loss: 0.1114
2025-02-18 18:13:12,564 - Epoch [37/1000], Validation Step [900/1090], Val Loss: 0.2652
2025-02-18 18:13:20,805 - Epoch [37/1000], Validation Step [1000/1090], Val Loss: 0.1590
2025-02-18 18:13:28,163 - Epoch 37/1000, Train Loss: 0.3881, Val Loss: 0.4086, Accuracy: 85.14%
2025-02-18 18:13:58,302 - Epoch [38/1000], Step [100/4367], Loss: 0.3910
2025-02-18 18:14:25,049 - Epoch [38/1000], Step [200/4367], Loss: 0.4604
2025-02-18 18:14:51,739 - Epoch [38/1000], Step [300/4367], Loss: 0.3986
2025-02-18 18:15:18,320 - Epoch [38/1000], Step [400/4367], Loss: 0.3007
2025-02-18 18:15:45,249 - Epoch [38/1000], Step [500/4367], Loss: 0.4651
2025-02-18 18:16:11,920 - Epoch [38/1000], Step [600/4367], Loss: 0.1806
2025-02-18 18:16:38,785 - Epoch [38/1000], Step [700/4367], Loss: 0.4547
2025-02-18 18:17:05,640 - Epoch [38/1000], Step [800/4367], Loss: 0.3952
2025-02-18 18:17:32,042 - Epoch [38/1000], Step [900/4367], Loss: 0.4427
2025-02-18 18:17:58,925 - Epoch [38/1000], Step [1000/4367], Loss: 0.3164
2025-02-18 18:18:25,827 - Epoch [38/1000], Step [1100/4367], Loss: 0.3430
2025-02-18 18:18:52,343 - Epoch [38/1000], Step [1200/4367], Loss: 0.8550
2025-02-18 18:19:19,422 - Epoch [38/1000], Step [1300/4367], Loss: 0.4780
2025-02-18 18:19:45,754 - Epoch [38/1000], Step [1400/4367], Loss: 0.4819
2025-02-18 18:20:12,334 - Epoch [38/1000], Step [1500/4367], Loss: 0.4399
2025-02-18 18:20:39,116 - Epoch [38/1000], Step [1600/4367], Loss: 0.5385
2025-02-18 18:21:06,015 - Epoch [38/1000], Step [1700/4367], Loss: 0.2675
2025-02-18 18:21:33,081 - Epoch [38/1000], Step [1800/4367], Loss: 0.2016
2025-02-18 18:22:00,494 - Epoch [38/1000], Step [1900/4367], Loss: 0.4070
2025-02-18 18:22:27,498 - Epoch [38/1000], Step [2000/4367], Loss: 0.3477
2025-02-18 18:22:54,197 - Epoch [38/1000], Step [2100/4367], Loss: 0.4477
2025-02-18 18:23:20,890 - Epoch [38/1000], Step [2200/4367], Loss: 0.2509
2025-02-18 18:23:47,622 - Epoch [38/1000], Step [2300/4367], Loss: 0.5601
2025-02-18 18:24:14,363 - Epoch [38/1000], Step [2400/4367], Loss: 0.5838
2025-02-18 18:24:40,778 - Epoch [38/1000], Step [2500/4367], Loss: 0.4711
2025-02-18 18:25:07,517 - Epoch [38/1000], Step [2600/4367], Loss: 0.3859
2025-02-18 18:25:34,071 - Epoch [38/1000], Step [2700/4367], Loss: 0.7467
2025-02-18 18:26:00,546 - Epoch [38/1000], Step [2800/4367], Loss: 0.2711
2025-02-18 18:26:27,145 - Epoch [38/1000], Step [2900/4367], Loss: 0.3959
2025-02-18 18:26:53,808 - Epoch [38/1000], Step [3000/4367], Loss: 0.3019
2025-02-18 18:27:20,311 - Epoch [38/1000], Step [3100/4367], Loss: 0.5664
2025-02-18 18:27:47,032 - Epoch [38/1000], Step [3200/4367], Loss: 0.4376
2025-02-18 18:28:13,567 - Epoch [38/1000], Step [3300/4367], Loss: 0.2891
2025-02-18 18:28:40,107 - Epoch [38/1000], Step [3400/4367], Loss: 0.6794
2025-02-18 18:29:06,609 - Epoch [38/1000], Step [3500/4367], Loss: 0.4364
2025-02-18 18:29:33,304 - Epoch [38/1000], Step [3600/4367], Loss: 0.5088
2025-02-18 18:30:00,237 - Epoch [38/1000], Step [3700/4367], Loss: 0.4497
2025-02-18 18:30:26,797 - Epoch [38/1000], Step [3800/4367], Loss: 0.1155
2025-02-18 18:30:53,784 - Epoch [38/1000], Step [3900/4367], Loss: 0.4346
2025-02-18 18:31:20,299 - Epoch [38/1000], Step [4000/4367], Loss: 0.0906
2025-02-18 18:31:47,061 - Epoch [38/1000], Step [4100/4367], Loss: 0.7557
2025-02-18 18:32:13,732 - Epoch [38/1000], Step [4200/4367], Loss: 0.3585
2025-02-18 18:32:40,731 - Epoch [38/1000], Step [4300/4367], Loss: 0.2565
2025-02-18 18:33:10,569 - Epoch [38/1000], Validation Step [100/1090], Val Loss: 0.0001
2025-02-18 18:33:18,788 - Epoch [38/1000], Validation Step [200/1090], Val Loss: 0.0000
2025-02-18 18:33:27,201 - Epoch [38/1000], Validation Step [300/1090], Val Loss: 0.6703
2025-02-18 18:33:35,763 - Epoch [38/1000], Validation Step [400/1090], Val Loss: 0.3582
2025-02-18 18:33:44,300 - Epoch [38/1000], Validation Step [500/1090], Val Loss: 0.4695
2025-02-18 18:33:52,813 - Epoch [38/1000], Validation Step [600/1090], Val Loss: 0.5414
2025-02-18 18:34:01,097 - Epoch [38/1000], Validation Step [700/1090], Val Loss: 0.3151
2025-02-18 18:34:09,123 - Epoch [38/1000], Validation Step [800/1090], Val Loss: 0.1600
2025-02-18 18:34:17,127 - Epoch [38/1000], Validation Step [900/1090], Val Loss: 0.2391
2025-02-18 18:34:25,657 - Epoch [38/1000], Validation Step [1000/1090], Val Loss: 0.1932
2025-02-18 18:34:33,172 - Epoch 38/1000, Train Loss: 0.3805, Val Loss: 0.4128, Accuracy: 85.09%
2025-02-18 18:34:33,912 - Model saved at /data2/personal/sungjin/korean_dialects/classification_second_checkpoint/checkpoint_epoch_38.pth
2025-02-18 18:35:04,361 - Epoch [39/1000], Step [100/4367], Loss: 0.5725
2025-02-18 18:35:31,401 - Epoch [39/1000], Step [200/4367], Loss: 0.3602
2025-02-18 18:35:57,977 - Epoch [39/1000], Step [300/4367], Loss: 0.2566
2025-02-18 18:36:24,193 - Epoch [39/1000], Step [400/4367], Loss: 0.4005
2025-02-18 18:36:51,054 - Epoch [39/1000], Step [500/4367], Loss: 0.2466
2025-02-18 18:37:17,686 - Epoch [39/1000], Step [600/4367], Loss: 0.3895
2025-02-18 18:37:43,630 - Epoch [39/1000], Step [700/4367], Loss: 0.5218
2025-02-18 18:38:10,161 - Epoch [39/1000], Step [800/4367], Loss: 0.3609
2025-02-18 18:38:36,514 - Epoch [39/1000], Step [900/4367], Loss: 0.2304
2025-02-18 18:39:02,673 - Epoch [39/1000], Step [1000/4367], Loss: 0.5728
2025-02-18 18:39:29,118 - Epoch [39/1000], Step [1100/4367], Loss: 0.3465
2025-02-18 18:39:55,376 - Epoch [39/1000], Step [1200/4367], Loss: 0.3788
2025-02-18 18:40:21,779 - Epoch [39/1000], Step [1300/4367], Loss: 0.3703
2025-02-18 18:40:48,225 - Epoch [39/1000], Step [1400/4367], Loss: 0.4054
2025-02-18 18:41:14,331 - Epoch [39/1000], Step [1500/4367], Loss: 0.4397
2025-02-18 18:41:40,805 - Epoch [39/1000], Step [1600/4367], Loss: 0.3050
2025-02-18 18:42:07,277 - Epoch [39/1000], Step [1700/4367], Loss: 0.3018
2025-02-18 18:42:33,579 - Epoch [39/1000], Step [1800/4367], Loss: 0.2194
2025-02-18 18:42:59,813 - Epoch [39/1000], Step [1900/4367], Loss: 0.3007
2025-02-18 18:43:26,588 - Epoch [39/1000], Step [2000/4367], Loss: 0.4378
2025-02-18 18:43:52,798 - Epoch [39/1000], Step [2100/4367], Loss: 0.4120
2025-02-18 18:44:19,205 - Epoch [39/1000], Step [2200/4367], Loss: 0.3366
2025-02-18 18:44:45,305 - Epoch [39/1000], Step [2300/4367], Loss: 0.2378
2025-02-18 18:45:11,596 - Epoch [39/1000], Step [2400/4367], Loss: 0.5597
2025-02-18 18:45:37,751 - Epoch [39/1000], Step [2500/4367], Loss: 0.4226
2025-02-18 18:46:03,773 - Epoch [39/1000], Step [2600/4367], Loss: 0.4238
2025-02-18 18:46:29,947 - Epoch [39/1000], Step [2700/4367], Loss: 0.1630
2025-02-18 18:46:56,148 - Epoch [39/1000], Step [2800/4367], Loss: 0.3895
2025-02-18 18:47:22,572 - Epoch [39/1000], Step [2900/4367], Loss: 0.3534
2025-02-18 18:47:48,922 - Epoch [39/1000], Step [3000/4367], Loss: 0.1770
2025-02-18 18:48:15,091 - Epoch [39/1000], Step [3100/4367], Loss: 0.2029
2025-02-18 18:48:41,396 - Epoch [39/1000], Step [3200/4367], Loss: 0.3423
2025-02-18 18:49:07,737 - Epoch [39/1000], Step [3300/4367], Loss: 0.2834
2025-02-18 18:49:33,961 - Epoch [39/1000], Step [3400/4367], Loss: 0.3923
2025-02-18 18:50:00,398 - Epoch [39/1000], Step [3500/4367], Loss: 0.2587
2025-02-18 18:50:26,643 - Epoch [39/1000], Step [3600/4367], Loss: 0.4079
2025-02-18 18:50:52,804 - Epoch [39/1000], Step [3700/4367], Loss: 0.3716
2025-02-18 18:51:19,247 - Epoch [39/1000], Step [3800/4367], Loss: 0.6558
2025-02-18 18:51:45,444 - Epoch [39/1000], Step [3900/4367], Loss: 0.2975
2025-02-18 18:52:11,759 - Epoch [39/1000], Step [4000/4367], Loss: 0.4089
2025-02-18 18:52:37,980 - Epoch [39/1000], Step [4100/4367], Loss: 0.5376
2025-02-18 18:53:04,372 - Epoch [39/1000], Step [4200/4367], Loss: 0.3946
2025-02-18 18:53:30,660 - Epoch [39/1000], Step [4300/4367], Loss: 0.2817
2025-02-18 18:53:59,248 - Epoch [39/1000], Validation Step [100/1090], Val Loss: 0.0000
2025-02-18 18:54:07,430 - Epoch [39/1000], Validation Step [200/1090], Val Loss: 0.0000
2025-02-18 18:54:15,456 - Epoch [39/1000], Validation Step [300/1090], Val Loss: 0.8920
2025-02-18 18:54:23,890 - Epoch [39/1000], Validation Step [400/1090], Val Loss: 0.2369
2025-02-18 18:54:31,655 - Epoch [39/1000], Validation Step [500/1090], Val Loss: 0.2778
2025-02-18 18:54:39,632 - Epoch [39/1000], Validation Step [600/1090], Val Loss: 0.6523
2025-02-18 18:54:48,219 - Epoch [39/1000], Validation Step [700/1090], Val Loss: 0.3600
2025-02-18 18:54:55,948 - Epoch [39/1000], Validation Step [800/1090], Val Loss: 0.1757
2025-02-18 18:55:03,303 - Epoch [39/1000], Validation Step [900/1090], Val Loss: 0.2392
2025-02-18 18:55:11,494 - Epoch [39/1000], Validation Step [1000/1090], Val Loss: 0.0878
2025-02-18 18:55:18,831 - Epoch 39/1000, Train Loss: 0.3783, Val Loss: 0.4142, Accuracy: 84.86%
2025-02-18 18:55:48,196 - Epoch [40/1000], Step [100/4367], Loss: 0.4273
2025-02-18 18:56:14,528 - Epoch [40/1000], Step [200/4367], Loss: 0.6623
2025-02-18 18:56:40,948 - Epoch [40/1000], Step [300/4367], Loss: 0.5777
2025-02-18 18:57:07,427 - Epoch [40/1000], Step [400/4367], Loss: 0.3421
2025-02-18 18:57:33,850 - Epoch [40/1000], Step [500/4367], Loss: 0.1479
2025-02-18 18:58:00,234 - Epoch [40/1000], Step [600/4367], Loss: 0.4585
2025-02-18 18:58:26,187 - Epoch [40/1000], Step [700/4367], Loss: 0.2435
2025-02-18 18:58:52,697 - Epoch [40/1000], Step [800/4367], Loss: 0.3038
2025-02-18 18:59:18,974 - Epoch [40/1000], Step [900/4367], Loss: 0.3258
2025-02-18 18:59:45,220 - Epoch [40/1000], Step [1000/4367], Loss: 0.5833
2025-02-18 19:00:11,778 - Epoch [40/1000], Step [1100/4367], Loss: 0.4597
2025-02-18 19:00:38,195 - Epoch [40/1000], Step [1200/4367], Loss: 0.3519
2025-02-18 19:01:04,444 - Epoch [40/1000], Step [1300/4367], Loss: 0.3600
2025-02-18 19:01:30,914 - Epoch [40/1000], Step [1400/4367], Loss: 0.2149
2025-02-18 19:01:57,173 - Epoch [40/1000], Step [1500/4367], Loss: 0.6617
2025-02-18 19:02:23,530 - Epoch [40/1000], Step [1600/4367], Loss: 0.5943
2025-02-18 19:02:49,978 - Epoch [40/1000], Step [1700/4367], Loss: 0.3901
2025-02-18 19:03:16,351 - Epoch [40/1000], Step [1800/4367], Loss: 0.4379
2025-02-18 19:03:42,717 - Epoch [40/1000], Step [1900/4367], Loss: 0.5118
2025-02-18 19:04:08,982 - Epoch [40/1000], Step [2000/4367], Loss: 0.1192
2025-02-18 19:04:35,437 - Epoch [40/1000], Step [2100/4367], Loss: 0.4597
2025-02-18 19:05:01,824 - Epoch [40/1000], Step [2200/4367], Loss: 0.2745
2025-02-18 19:05:28,238 - Epoch [40/1000], Step [2300/4367], Loss: 0.5763
2025-02-18 19:05:54,644 - Epoch [40/1000], Step [2400/4367], Loss: 0.3992
2025-02-18 19:06:20,466 - Epoch [40/1000], Step [2500/4367], Loss: 0.1648
2025-02-18 19:06:46,670 - Epoch [40/1000], Step [2600/4367], Loss: 0.2899
2025-02-18 19:07:12,968 - Epoch [40/1000], Step [2700/4367], Loss: 0.3906
2025-02-18 19:07:38,815 - Epoch [40/1000], Step [2800/4367], Loss: 0.4734
2025-02-18 19:08:04,983 - Epoch [40/1000], Step [2900/4367], Loss: 0.2987
2025-02-18 19:08:30,981 - Epoch [40/1000], Step [3000/4367], Loss: 0.2485
2025-02-18 19:08:57,021 - Epoch [40/1000], Step [3100/4367], Loss: 0.2909
2025-02-18 19:09:23,145 - Epoch [40/1000], Step [3200/4367], Loss: 0.1702
2025-02-18 19:09:49,275 - Epoch [40/1000], Step [3300/4367], Loss: 0.6447
2025-02-18 19:10:15,118 - Epoch [40/1000], Step [3400/4367], Loss: 0.1375
2025-02-18 19:10:41,306 - Epoch [40/1000], Step [3500/4367], Loss: 0.3067
2025-02-18 19:11:07,303 - Epoch [40/1000], Step [3600/4367], Loss: 0.4872
2025-02-18 19:11:33,368 - Epoch [40/1000], Step [3700/4367], Loss: 0.4056
2025-02-18 19:11:59,198 - Epoch [40/1000], Step [3800/4367], Loss: 0.2745
2025-02-18 19:12:24,881 - Epoch [40/1000], Step [3900/4367], Loss: 0.3659
2025-02-18 19:12:51,014 - Epoch [40/1000], Step [4000/4367], Loss: 0.7183
2025-02-18 19:13:17,172 - Epoch [40/1000], Step [4100/4367], Loss: 0.5076
2025-02-18 19:13:43,356 - Epoch [40/1000], Step [4200/4367], Loss: 0.2549
2025-02-18 19:14:09,369 - Epoch [40/1000], Step [4300/4367], Loss: 0.2280
2025-02-18 19:14:37,643 - Epoch [40/1000], Validation Step [100/1090], Val Loss: 0.0001
2025-02-18 19:14:45,555 - Epoch [40/1000], Validation Step [200/1090], Val Loss: 0.0001
2025-02-18 19:14:53,420 - Epoch [40/1000], Validation Step [300/1090], Val Loss: 0.6253
2025-02-18 19:15:01,470 - Epoch [40/1000], Validation Step [400/1090], Val Loss: 0.2831
2025-02-18 19:15:09,196 - Epoch [40/1000], Validation Step [500/1090], Val Loss: 0.4013
2025-02-18 19:15:17,111 - Epoch [40/1000], Validation Step [600/1090], Val Loss: 0.6389
2025-02-18 19:15:25,109 - Epoch [40/1000], Validation Step [700/1090], Val Loss: 0.3598
2025-02-18 19:15:32,457 - Epoch [40/1000], Validation Step [800/1090], Val Loss: 0.1941
2025-02-18 19:15:39,628 - Epoch [40/1000], Validation Step [900/1090], Val Loss: 0.3170
2025-02-18 19:15:47,471 - Epoch [40/1000], Validation Step [1000/1090], Val Loss: 0.0275
2025-02-18 19:15:54,600 - Epoch 40/1000, Train Loss: 0.3827, Val Loss: 0.3988, Accuracy: 85.11%
2025-02-18 19:15:55,353 - Model saved at /data2/personal/sungjin/korean_dialects/classification_second_checkpoint/checkpoint_epoch_40.pth
2025-02-18 19:16:24,384 - Epoch [41/1000], Step [100/4367], Loss: 0.3168
2025-02-18 19:16:50,563 - Epoch [41/1000], Step [200/4367], Loss: 0.2787
2025-02-18 19:17:16,384 - Epoch [41/1000], Step [300/4367], Loss: 0.2411
2025-02-18 19:17:42,382 - Epoch [41/1000], Step [400/4367], Loss: 0.3437
2025-02-18 19:18:08,294 - Epoch [41/1000], Step [500/4367], Loss: 0.3173
2025-02-18 19:18:34,534 - Epoch [41/1000], Step [600/4367], Loss: 0.4736
2025-02-18 19:19:00,939 - Epoch [41/1000], Step [700/4367], Loss: 0.3215
2025-02-18 19:19:27,413 - Epoch [41/1000], Step [800/4367], Loss: 0.2740
2025-02-18 19:19:53,809 - Epoch [41/1000], Step [900/4367], Loss: 0.3562
2025-02-18 19:20:19,935 - Epoch [41/1000], Step [1000/4367], Loss: 0.1351
2025-02-18 19:20:45,981 - Epoch [41/1000], Step [1100/4367], Loss: 0.1323
2025-02-18 19:21:11,984 - Epoch [41/1000], Step [1200/4367], Loss: 0.3529
2025-02-18 19:21:38,206 - Epoch [41/1000], Step [1300/4367], Loss: 0.1847
2025-02-18 19:22:04,182 - Epoch [41/1000], Step [1400/4367], Loss: 0.4150
2025-02-18 19:22:30,453 - Epoch [41/1000], Step [1500/4367], Loss: 0.2513
2025-02-18 19:22:56,552 - Epoch [41/1000], Step [1600/4367], Loss: 0.4236
2025-02-18 19:23:22,454 - Epoch [41/1000], Step [1700/4367], Loss: 0.3625
2025-02-18 19:23:48,256 - Epoch [41/1000], Step [1800/4367], Loss: 0.5247
2025-02-18 19:24:14,228 - Epoch [41/1000], Step [1900/4367], Loss: 0.4992
2025-02-18 19:24:40,333 - Epoch [41/1000], Step [2000/4367], Loss: 0.6347
2025-02-18 19:25:06,263 - Epoch [41/1000], Step [2100/4367], Loss: 0.1287
2025-02-18 19:25:32,545 - Epoch [41/1000], Step [2200/4367], Loss: 0.6552
2025-02-18 19:25:58,836 - Epoch [41/1000], Step [2300/4367], Loss: 0.4029
2025-02-18 19:26:25,171 - Epoch [41/1000], Step [2400/4367], Loss: 0.2379
2025-02-18 19:26:50,992 - Epoch [41/1000], Step [2500/4367], Loss: 0.3237
2025-02-18 19:27:16,919 - Epoch [41/1000], Step [2600/4367], Loss: 0.2803
2025-02-18 19:27:42,877 - Epoch [41/1000], Step [2700/4367], Loss: 0.3258
2025-02-18 19:28:08,746 - Epoch [41/1000], Step [2800/4367], Loss: 0.2585
2025-02-18 19:28:34,692 - Epoch [41/1000], Step [2900/4367], Loss: 0.4337
2025-02-18 19:29:00,572 - Epoch [41/1000], Step [3000/4367], Loss: 0.4979
2025-02-18 19:29:26,863 - Epoch [41/1000], Step [3100/4367], Loss: 0.3597
2025-02-18 19:29:52,896 - Epoch [41/1000], Step [3200/4367], Loss: 0.3771
2025-02-18 19:30:19,319 - Epoch [41/1000], Step [3300/4367], Loss: 0.1884
2025-02-18 19:30:45,480 - Epoch [41/1000], Step [3400/4367], Loss: 0.5640
2025-02-18 19:31:11,514 - Epoch [41/1000], Step [3500/4367], Loss: 0.1474
2025-02-18 19:31:37,226 - Epoch [41/1000], Step [3600/4367], Loss: 0.3057
2025-02-18 19:32:03,502 - Epoch [41/1000], Step [3700/4367], Loss: 0.6808
2025-02-18 19:32:29,614 - Epoch [41/1000], Step [3800/4367], Loss: 0.2945
2025-02-18 19:32:55,926 - Epoch [41/1000], Step [3900/4367], Loss: 0.2195
2025-02-18 19:33:21,940 - Epoch [41/1000], Step [4000/4367], Loss: 0.3717
2025-02-18 19:33:48,202 - Epoch [41/1000], Step [4100/4367], Loss: 0.1133
2025-02-18 19:34:14,263 - Epoch [41/1000], Step [4200/4367], Loss: 0.2993
2025-02-18 19:34:40,270 - Epoch [41/1000], Step [4300/4367], Loss: 0.1995
2025-02-18 19:35:08,736 - Epoch [41/1000], Validation Step [100/1090], Val Loss: 0.0001
2025-02-18 19:35:16,431 - Epoch [41/1000], Validation Step [200/1090], Val Loss: 0.0001
2025-02-18 19:35:24,098 - Epoch [41/1000], Validation Step [300/1090], Val Loss: 0.6866
2025-02-18 19:35:32,122 - Epoch [41/1000], Validation Step [400/1090], Val Loss: 0.2079
2025-02-18 19:35:39,703 - Epoch [41/1000], Validation Step [500/1090], Val Loss: 0.3882
2025-02-18 19:35:47,510 - Epoch [41/1000], Validation Step [600/1090], Val Loss: 0.6517
2025-02-18 19:35:55,301 - Epoch [41/1000], Validation Step [700/1090], Val Loss: 0.3670
2025-02-18 19:36:02,657 - Epoch [41/1000], Validation Step [800/1090], Val Loss: 0.1523
2025-02-18 19:36:09,882 - Epoch [41/1000], Validation Step [900/1090], Val Loss: 0.2538
2025-02-18 19:36:17,656 - Epoch [41/1000], Validation Step [1000/1090], Val Loss: 0.0456
2025-02-18 19:36:24,805 - Epoch 41/1000, Train Loss: 0.3740, Val Loss: 0.3911, Accuracy: 85.93%
2025-02-18 19:36:54,291 - Epoch [42/1000], Step [100/4367], Loss: 0.3685
2025-02-18 19:37:20,371 - Epoch [42/1000], Step [200/4367], Loss: 0.4222
2025-02-18 19:37:46,614 - Epoch [42/1000], Step [300/4367], Loss: 0.3121
2025-02-18 19:38:12,935 - Epoch [42/1000], Step [400/4367], Loss: 0.3346
2025-02-18 19:38:39,051 - Epoch [42/1000], Step [500/4367], Loss: 0.3558
2025-02-18 19:39:04,870 - Epoch [42/1000], Step [600/4367], Loss: 0.5185
2025-02-18 19:39:30,827 - Epoch [42/1000], Step [700/4367], Loss: 0.1862
2025-02-18 19:39:56,903 - Epoch [42/1000], Step [800/4367], Loss: 0.3805
2025-02-18 19:40:22,849 - Epoch [42/1000], Step [900/4367], Loss: 0.5770
2025-02-18 19:40:48,877 - Epoch [42/1000], Step [1000/4367], Loss: 0.2064
2025-02-18 19:41:14,967 - Epoch [42/1000], Step [1100/4367], Loss: 0.2636
2025-02-18 19:41:41,335 - Epoch [42/1000], Step [1200/4367], Loss: 0.3667
2025-02-18 19:42:07,392 - Epoch [42/1000], Step [1300/4367], Loss: 0.4449
2025-02-18 19:42:33,486 - Epoch [42/1000], Step [1400/4367], Loss: 0.2873
2025-02-18 19:42:59,238 - Epoch [42/1000], Step [1500/4367], Loss: 0.1796
2025-02-18 19:43:25,818 - Epoch [42/1000], Step [1600/4367], Loss: 0.2873
2025-02-18 19:43:51,748 - Epoch [42/1000], Step [1700/4367], Loss: 0.4014
2025-02-18 19:44:17,750 - Epoch [42/1000], Step [1800/4367], Loss: 0.4104
2025-02-18 19:44:43,541 - Epoch [42/1000], Step [1900/4367], Loss: 0.2201
2025-02-18 19:45:09,633 - Epoch [42/1000], Step [2000/4367], Loss: 0.1953
2025-02-18 19:45:35,467 - Epoch [42/1000], Step [2100/4367], Loss: 0.3767
2025-02-18 19:46:01,738 - Epoch [42/1000], Step [2200/4367], Loss: 0.2922
2025-02-18 19:46:27,817 - Epoch [42/1000], Step [2300/4367], Loss: 0.4630
2025-02-18 19:46:53,755 - Epoch [42/1000], Step [2400/4367], Loss: 0.4355
2025-02-18 19:47:19,983 - Epoch [42/1000], Step [2500/4367], Loss: 0.5666
2025-02-18 19:47:46,258 - Epoch [42/1000], Step [2600/4367], Loss: 0.2770
2025-02-18 19:48:12,371 - Epoch [42/1000], Step [2700/4367], Loss: 0.7204
2025-02-18 19:48:38,542 - Epoch [42/1000], Step [2800/4367], Loss: 0.2214
2025-02-18 19:49:04,618 - Epoch [42/1000], Step [2900/4367], Loss: 0.2509
2025-02-18 19:49:30,564 - Epoch [42/1000], Step [3000/4367], Loss: 0.3296
2025-02-18 19:49:56,747 - Epoch [42/1000], Step [3100/4367], Loss: 0.2629
2025-02-18 19:50:22,783 - Epoch [42/1000], Step [3200/4367], Loss: 0.1815
2025-02-18 19:50:49,087 - Epoch [42/1000], Step [3300/4367], Loss: 0.4289
2025-02-18 19:51:15,345 - Epoch [42/1000], Step [3400/4367], Loss: 0.2610
2025-02-18 19:51:41,244 - Epoch [42/1000], Step [3500/4367], Loss: 0.4326
2025-02-18 19:52:07,404 - Epoch [42/1000], Step [3600/4367], Loss: 0.2125
2025-02-18 19:52:33,554 - Epoch [42/1000], Step [3700/4367], Loss: 0.2106
2025-02-18 19:52:59,402 - Epoch [42/1000], Step [3800/4367], Loss: 0.2473
2025-02-18 19:53:25,522 - Epoch [42/1000], Step [3900/4367], Loss: 0.2434
2025-02-18 19:53:51,396 - Epoch [42/1000], Step [4000/4367], Loss: 0.2072
2025-02-18 19:54:17,899 - Epoch [42/1000], Step [4100/4367], Loss: 0.5021
2025-02-18 19:54:44,037 - Epoch [42/1000], Step [4200/4367], Loss: 0.3123
2025-02-18 19:55:10,299 - Epoch [42/1000], Step [4300/4367], Loss: 0.3597
2025-02-18 19:55:38,438 - Epoch [42/1000], Validation Step [100/1090], Val Loss: 0.0002
2025-02-18 19:55:46,192 - Epoch [42/1000], Validation Step [200/1090], Val Loss: 0.0001
2025-02-18 19:55:54,034 - Epoch [42/1000], Validation Step [300/1090], Val Loss: 0.7288
2025-02-18 19:56:01,998 - Epoch [42/1000], Validation Step [400/1090], Val Loss: 0.2078
2025-02-18 19:56:09,577 - Epoch [42/1000], Validation Step [500/1090], Val Loss: 0.3688
2025-02-18 19:56:17,485 - Epoch [42/1000], Validation Step [600/1090], Val Loss: 0.5819
2025-02-18 19:56:25,382 - Epoch [42/1000], Validation Step [700/1090], Val Loss: 0.2995
2025-02-18 19:56:32,739 - Epoch [42/1000], Validation Step [800/1090], Val Loss: 0.1566
2025-02-18 19:56:39,734 - Epoch [42/1000], Validation Step [900/1090], Val Loss: 0.2395
2025-02-18 19:56:47,454 - Epoch [42/1000], Validation Step [1000/1090], Val Loss: 0.0891
2025-02-18 19:56:54,511 - Epoch 42/1000, Train Loss: 0.3701, Val Loss: 0.4005, Accuracy: 85.49%
2025-02-18 19:56:55,291 - Model saved at /data2/personal/sungjin/korean_dialects/classification_second_checkpoint/checkpoint_epoch_42.pth
2025-02-18 19:57:24,635 - Epoch [43/1000], Step [100/4367], Loss: 0.2117
2025-02-18 19:57:50,577 - Epoch [43/1000], Step [200/4367], Loss: 0.2454
2025-02-18 19:58:16,469 - Epoch [43/1000], Step [300/4367], Loss: 0.3646
2025-02-18 19:58:42,613 - Epoch [43/1000], Step [400/4367], Loss: 0.3612
2025-02-18 19:59:08,690 - Epoch [43/1000], Step [500/4367], Loss: 0.3133
2025-02-18 19:59:34,801 - Epoch [43/1000], Step [600/4367], Loss: 0.4140
2025-02-18 20:00:01,083 - Epoch [43/1000], Step [700/4367], Loss: 0.3363
2025-02-18 20:00:27,283 - Epoch [43/1000], Step [800/4367], Loss: 0.3281
2025-02-18 20:00:53,695 - Epoch [43/1000], Step [900/4367], Loss: 0.5300
2025-02-18 20:01:19,703 - Epoch [43/1000], Step [1000/4367], Loss: 0.3401
2025-02-18 20:01:45,926 - Epoch [43/1000], Step [1100/4367], Loss: 0.3417
2025-02-18 20:02:11,790 - Epoch [43/1000], Step [1200/4367], Loss: 0.3413
2025-02-18 20:02:37,660 - Epoch [43/1000], Step [1300/4367], Loss: 0.2141
2025-02-18 20:03:03,698 - Epoch [43/1000], Step [1400/4367], Loss: 0.1350
2025-02-18 20:03:29,491 - Epoch [43/1000], Step [1500/4367], Loss: 0.3315
2025-02-18 20:03:55,472 - Epoch [43/1000], Step [1600/4367], Loss: 0.3697
2025-02-18 20:04:21,516 - Epoch [43/1000], Step [1700/4367], Loss: 0.3841
2025-02-18 20:04:47,736 - Epoch [43/1000], Step [1800/4367], Loss: 0.4167
2025-02-18 20:05:13,981 - Epoch [43/1000], Step [1900/4367], Loss: 0.3143
2025-02-18 20:05:40,236 - Epoch [43/1000], Step [2000/4367], Loss: 0.1510
2025-02-18 20:06:06,727 - Epoch [43/1000], Step [2100/4367], Loss: 0.6535
2025-02-18 20:06:33,158 - Epoch [43/1000], Step [2200/4367], Loss: 0.2846
2025-02-18 20:06:59,502 - Epoch [43/1000], Step [2300/4367], Loss: 0.6876
2025-02-18 20:07:25,925 - Epoch [43/1000], Step [2400/4367], Loss: 0.2145
2025-02-18 20:07:52,568 - Epoch [43/1000], Step [2500/4367], Loss: 0.3720
2025-02-18 20:08:18,943 - Epoch [43/1000], Step [2600/4367], Loss: 0.2847
2025-02-18 20:08:45,115 - Epoch [43/1000], Step [2700/4367], Loss: 0.2666
2025-02-18 20:09:11,602 - Epoch [43/1000], Step [2800/4367], Loss: 0.1356
2025-02-18 20:09:38,124 - Epoch [43/1000], Step [2900/4367], Loss: 0.2645
2025-02-18 20:10:04,651 - Epoch [43/1000], Step [3000/4367], Loss: 0.6336
2025-02-18 20:10:31,185 - Epoch [43/1000], Step [3100/4367], Loss: 0.6291
2025-02-18 20:10:57,688 - Epoch [43/1000], Step [3200/4367], Loss: 0.4396
2025-02-18 20:11:24,239 - Epoch [43/1000], Step [3300/4367], Loss: 0.1885
2025-02-18 20:11:50,767 - Epoch [43/1000], Step [3400/4367], Loss: 0.2341
2025-02-18 20:12:17,371 - Epoch [43/1000], Step [3500/4367], Loss: 0.3383
2025-02-18 20:12:43,946 - Epoch [43/1000], Step [3600/4367], Loss: 0.2314
2025-02-18 20:13:10,264 - Epoch [43/1000], Step [3700/4367], Loss: 0.3047
2025-02-18 20:13:36,424 - Epoch [43/1000], Step [3800/4367], Loss: 0.2473
2025-02-18 20:14:02,493 - Epoch [43/1000], Step [3900/4367], Loss: 0.1603
2025-02-18 20:14:29,062 - Epoch [43/1000], Step [4000/4367], Loss: 0.2506
2025-02-18 20:14:55,183 - Epoch [43/1000], Step [4100/4367], Loss: 0.4693
2025-02-18 20:15:21,765 - Epoch [43/1000], Step [4200/4367], Loss: 0.3823
2025-02-18 20:15:48,151 - Epoch [43/1000], Step [4300/4367], Loss: 0.5277
2025-02-18 20:16:17,387 - Epoch [43/1000], Validation Step [100/1090], Val Loss: 0.0001
2025-02-18 20:16:25,483 - Epoch [43/1000], Validation Step [200/1090], Val Loss: 0.0003
2025-02-18 20:16:33,485 - Epoch [43/1000], Validation Step [300/1090], Val Loss: 0.7818
2025-02-18 20:16:41,963 - Epoch [43/1000], Validation Step [400/1090], Val Loss: 0.1872
2025-02-18 20:16:49,983 - Epoch [43/1000], Validation Step [500/1090], Val Loss: 0.3192
2025-02-18 20:16:58,257 - Epoch [43/1000], Validation Step [600/1090], Val Loss: 0.6365
2025-02-18 20:17:06,778 - Epoch [43/1000], Validation Step [700/1090], Val Loss: 0.3608
2025-02-18 20:17:14,620 - Epoch [43/1000], Validation Step [800/1090], Val Loss: 0.1659
2025-02-18 20:17:22,017 - Epoch [43/1000], Validation Step [900/1090], Val Loss: 0.2800
2025-02-18 20:17:30,214 - Epoch [43/1000], Validation Step [1000/1090], Val Loss: 0.0607
2025-02-18 20:17:37,511 - Epoch 43/1000, Train Loss: 0.3679, Val Loss: 0.4020, Accuracy: 85.54%
2025-02-18 20:18:07,175 - Epoch [44/1000], Step [100/4367], Loss: 0.3935
2025-02-18 20:18:33,813 - Epoch [44/1000], Step [200/4367], Loss: 0.1470
2025-02-18 20:19:00,603 - Epoch [44/1000], Step [300/4367], Loss: 0.6116
2025-02-18 20:19:27,600 - Epoch [44/1000], Step [400/4367], Loss: 0.3065
2025-02-18 20:19:54,451 - Epoch [44/1000], Step [500/4367], Loss: 0.5420
2025-02-18 20:20:21,209 - Epoch [44/1000], Step [600/4367], Loss: 0.4247
2025-02-18 20:20:47,904 - Epoch [44/1000], Step [700/4367], Loss: 0.3288
2025-02-18 20:21:14,663 - Epoch [44/1000], Step [800/4367], Loss: 0.6186
2025-02-18 20:21:41,346 - Epoch [44/1000], Step [900/4367], Loss: 0.2238
2025-02-18 20:22:07,983 - Epoch [44/1000], Step [1000/4367], Loss: 0.3264
2025-02-18 20:22:34,946 - Epoch [44/1000], Step [1100/4367], Loss: 0.2678
2025-02-18 20:23:02,100 - Epoch [44/1000], Step [1200/4367], Loss: 0.5357
2025-02-18 20:23:29,183 - Epoch [44/1000], Step [1300/4367], Loss: 0.2807
2025-02-18 20:23:55,924 - Epoch [44/1000], Step [1400/4367], Loss: 0.1822
2025-02-18 20:24:22,621 - Epoch [44/1000], Step [1500/4367], Loss: 0.2362
2025-02-18 20:24:49,403 - Epoch [44/1000], Step [1600/4367], Loss: 0.4431
2025-02-18 20:25:16,197 - Epoch [44/1000], Step [1700/4367], Loss: 0.2977
2025-02-18 20:25:43,195 - Epoch [44/1000], Step [1800/4367], Loss: 0.1541
2025-02-18 20:26:09,937 - Epoch [44/1000], Step [1900/4367], Loss: 0.2719
2025-02-18 20:26:36,990 - Epoch [44/1000], Step [2000/4367], Loss: 0.6248
2025-02-18 20:27:03,915 - Epoch [44/1000], Step [2100/4367], Loss: 0.4846
2025-02-18 20:27:30,724 - Epoch [44/1000], Step [2200/4367], Loss: 0.4466
2025-02-18 20:27:57,226 - Epoch [44/1000], Step [2300/4367], Loss: 0.4532
2025-02-18 20:28:23,831 - Epoch [44/1000], Step [2400/4367], Loss: 0.4029
2025-02-18 20:28:50,814 - Epoch [44/1000], Step [2500/4367], Loss: 0.5099
2025-02-18 20:29:17,487 - Epoch [44/1000], Step [2600/4367], Loss: 0.2039
2025-02-18 20:29:44,277 - Epoch [44/1000], Step [2700/4367], Loss: 0.3425
2025-02-18 20:30:11,087 - Epoch [44/1000], Step [2800/4367], Loss: 0.3009
2025-02-18 20:30:37,783 - Epoch [44/1000], Step [2900/4367], Loss: 0.3119
2025-02-18 20:31:04,759 - Epoch [44/1000], Step [3000/4367], Loss: 0.3652
2025-02-18 20:31:31,457 - Epoch [44/1000], Step [3100/4367], Loss: 0.3342
2025-02-18 20:31:58,465 - Epoch [44/1000], Step [3200/4367], Loss: 0.2619
2025-02-18 20:32:25,036 - Epoch [44/1000], Step [3300/4367], Loss: 0.2163
2025-02-18 20:32:51,826 - Epoch [44/1000], Step [3400/4367], Loss: 0.1994
2025-02-18 20:33:18,176 - Epoch [44/1000], Step [3500/4367], Loss: 0.3938
2025-02-18 20:33:44,615 - Epoch [44/1000], Step [3600/4367], Loss: 0.4073
2025-02-18 20:34:11,141 - Epoch [44/1000], Step [3700/4367], Loss: 0.3975
2025-02-18 20:34:37,882 - Epoch [44/1000], Step [3800/4367], Loss: 0.2645
2025-02-18 20:35:04,630 - Epoch [44/1000], Step [3900/4367], Loss: 0.4608
2025-02-18 20:35:31,673 - Epoch [44/1000], Step [4000/4367], Loss: 0.4021
2025-02-18 20:35:58,107 - Epoch [44/1000], Step [4100/4367], Loss: 0.3686
2025-02-18 20:36:24,700 - Epoch [44/1000], Step [4200/4367], Loss: 0.3173
2025-02-18 20:36:51,823 - Epoch [44/1000], Step [4300/4367], Loss: 0.2888
2025-02-18 20:37:21,720 - Epoch [44/1000], Validation Step [100/1090], Val Loss: 0.0003
2025-02-18 20:37:29,965 - Epoch [44/1000], Validation Step [200/1090], Val Loss: 0.0000
2025-02-18 20:37:38,729 - Epoch [44/1000], Validation Step [300/1090], Val Loss: 0.7484
2025-02-18 20:37:47,410 - Epoch [44/1000], Validation Step [400/1090], Val Loss: 0.2600
2025-02-18 20:37:55,672 - Epoch [44/1000], Validation Step [500/1090], Val Loss: 0.3557
2025-02-18 20:38:04,260 - Epoch [44/1000], Validation Step [600/1090], Val Loss: 0.6482
2025-02-18 20:38:12,958 - Epoch [44/1000], Validation Step [700/1090], Val Loss: 0.3378
2025-02-18 20:38:20,768 - Epoch [44/1000], Validation Step [800/1090], Val Loss: 0.1380
2025-02-18 20:38:28,851 - Epoch [44/1000], Validation Step [900/1090], Val Loss: 0.2084
2025-02-18 20:38:37,474 - Epoch [44/1000], Validation Step [1000/1090], Val Loss: 0.1498
2025-02-18 20:38:45,138 - Epoch 44/1000, Train Loss: 0.3648, Val Loss: 0.4086, Accuracy: 85.37%
2025-02-18 20:38:45,874 - Model saved at /data2/personal/sungjin/korean_dialects/classification_second_checkpoint/checkpoint_epoch_44.pth
2025-02-18 20:39:17,033 - Epoch [45/1000], Step [100/4367], Loss: 0.4123
2025-02-18 20:39:43,660 - Epoch [45/1000], Step [200/4367], Loss: 0.3302
2025-02-18 20:40:10,503 - Epoch [45/1000], Step [300/4367], Loss: 0.3402
2025-02-18 20:40:36,704 - Epoch [45/1000], Step [400/4367], Loss: 0.3219
2025-02-18 20:41:03,615 - Epoch [45/1000], Step [500/4367], Loss: 0.4506
2025-02-18 20:41:30,790 - Epoch [45/1000], Step [600/4367], Loss: 0.4400
2025-02-18 20:41:57,509 - Epoch [45/1000], Step [700/4367], Loss: 0.3852
2025-02-18 20:42:24,683 - Epoch [45/1000], Step [800/4367], Loss: 0.1820
2025-02-18 20:42:51,371 - Epoch [45/1000], Step [900/4367], Loss: 0.4520
2025-02-18 20:43:18,524 - Epoch [45/1000], Step [1000/4367], Loss: 0.4578
2025-02-18 20:43:45,249 - Epoch [45/1000], Step [1100/4367], Loss: 0.2789
2025-02-18 20:44:12,188 - Epoch [45/1000], Step [1200/4367], Loss: 0.2398
2025-02-18 20:44:39,032 - Epoch [45/1000], Step [1300/4367], Loss: 0.3422
2025-02-18 20:45:06,275 - Epoch [45/1000], Step [1400/4367], Loss: 0.3124
2025-02-18 20:45:33,344 - Epoch [45/1000], Step [1500/4367], Loss: 0.4302
2025-02-18 20:46:00,668 - Epoch [45/1000], Step [1600/4367], Loss: 0.3583
2025-02-18 20:46:27,979 - Epoch [45/1000], Step [1700/4367], Loss: 0.2448
2025-02-18 20:46:54,811 - Epoch [45/1000], Step [1800/4367], Loss: 0.3247
2025-02-18 20:47:21,645 - Epoch [45/1000], Step [1900/4367], Loss: 0.3050
2025-02-18 20:47:48,338 - Epoch [45/1000], Step [2000/4367], Loss: 0.2572
2025-02-18 20:48:15,373 - Epoch [45/1000], Step [2100/4367], Loss: 0.2721
2025-02-18 20:48:42,143 - Epoch [45/1000], Step [2200/4367], Loss: 0.1028
2025-02-18 20:49:08,767 - Epoch [45/1000], Step [2300/4367], Loss: 0.4374
2025-02-18 20:49:35,770 - Epoch [45/1000], Step [2400/4367], Loss: 0.3526
2025-02-18 20:50:02,523 - Epoch [45/1000], Step [2500/4367], Loss: 0.2285
2025-02-18 20:50:29,784 - Epoch [45/1000], Step [2600/4367], Loss: 0.3652
2025-02-18 20:50:56,450 - Epoch [45/1000], Step [2700/4367], Loss: 0.7228
2025-02-18 20:51:23,337 - Epoch [45/1000], Step [2800/4367], Loss: 0.3814
2025-02-18 20:51:50,254 - Epoch [45/1000], Step [2900/4367], Loss: 0.4250
2025-02-18 20:52:16,932 - Epoch [45/1000], Step [3000/4367], Loss: 0.2410
2025-02-18 20:52:43,643 - Epoch [45/1000], Step [3100/4367], Loss: 0.1781
2025-02-18 20:53:10,344 - Epoch [45/1000], Step [3200/4367], Loss: 0.5775
2025-02-18 20:53:37,007 - Epoch [45/1000], Step [3300/4367], Loss: 0.1720
2025-02-18 20:54:03,726 - Epoch [45/1000], Step [3400/4367], Loss: 0.3859
2025-02-18 20:54:30,795 - Epoch [45/1000], Step [3500/4367], Loss: 0.2155
2025-02-18 20:54:57,324 - Epoch [45/1000], Step [3600/4367], Loss: 0.1029
2025-02-18 20:55:23,729 - Epoch [45/1000], Step [3700/4367], Loss: 0.4555
2025-02-18 20:55:50,682 - Epoch [45/1000], Step [3800/4367], Loss: 0.2614
2025-02-18 20:56:16,841 - Epoch [45/1000], Step [3900/4367], Loss: 0.4687
2025-02-18 20:56:43,040 - Epoch [45/1000], Step [4000/4367], Loss: 0.5769
2025-02-18 20:57:09,352 - Epoch [45/1000], Step [4100/4367], Loss: 0.2167
2025-02-18 20:57:35,397 - Epoch [45/1000], Step [4200/4367], Loss: 0.5729
2025-02-18 20:58:01,712 - Epoch [45/1000], Step [4300/4367], Loss: 0.3034
2025-02-18 20:58:31,313 - Epoch [45/1000], Validation Step [100/1090], Val Loss: 0.0006
2025-02-18 20:58:39,378 - Epoch [45/1000], Validation Step [200/1090], Val Loss: 0.0001
2025-02-18 20:58:47,794 - Epoch [45/1000], Validation Step [300/1090], Val Loss: 0.6154
2025-02-18 20:58:56,457 - Epoch [45/1000], Validation Step [400/1090], Val Loss: 0.3173
2025-02-18 20:59:04,503 - Epoch [45/1000], Validation Step [500/1090], Val Loss: 0.5318
2025-02-18 20:59:13,045 - Epoch [45/1000], Validation Step [600/1090], Val Loss: 0.5287
2025-02-18 20:59:21,581 - Epoch [45/1000], Validation Step [700/1090], Val Loss: 0.3349
2025-02-18 20:59:29,251 - Epoch [45/1000], Validation Step [800/1090], Val Loss: 0.1720
2025-02-18 20:59:37,099 - Epoch [45/1000], Validation Step [900/1090], Val Loss: 0.2362
2025-02-18 20:59:45,614 - Epoch [45/1000], Validation Step [1000/1090], Val Loss: 0.0759
2025-02-18 20:59:53,051 - Epoch 45/1000, Train Loss: 0.3623, Val Loss: 0.3799, Accuracy: 86.24%
2025-02-18 21:00:22,868 - Epoch [46/1000], Step [100/4367], Loss: 0.2589
2025-02-18 21:00:49,343 - Epoch [46/1000], Step [200/4367], Loss: 0.4216
2025-02-18 21:01:15,839 - Epoch [46/1000], Step [300/4367], Loss: 0.2309
2025-02-18 21:01:42,463 - Epoch [46/1000], Step [400/4367], Loss: 0.4458
2025-02-18 21:02:08,470 - Epoch [46/1000], Step [500/4367], Loss: 0.2152
2025-02-18 21:02:34,855 - Epoch [46/1000], Step [600/4367], Loss: 0.3115
2025-02-18 21:03:01,332 - Epoch [46/1000], Step [700/4367], Loss: 0.6336
2025-02-18 21:03:27,459 - Epoch [46/1000], Step [800/4367], Loss: 0.5293
2025-02-18 21:03:53,693 - Epoch [46/1000], Step [900/4367], Loss: 0.3744
2025-02-18 21:04:20,045 - Epoch [46/1000], Step [1000/4367], Loss: 0.2362
2025-02-18 21:04:46,302 - Epoch [46/1000], Step [1100/4367], Loss: 0.3432
2025-02-18 21:05:12,407 - Epoch [46/1000], Step [1200/4367], Loss: 0.3779
2025-02-18 21:05:38,577 - Epoch [46/1000], Step [1300/4367], Loss: 0.3747
2025-02-18 21:06:04,774 - Epoch [46/1000], Step [1400/4367], Loss: 0.5467
2025-02-18 21:06:31,078 - Epoch [46/1000], Step [1500/4367], Loss: 0.6455
2025-02-18 21:06:57,353 - Epoch [46/1000], Step [1600/4367], Loss: 0.3039
2025-02-18 21:07:24,190 - Epoch [46/1000], Step [1700/4367], Loss: 0.4050
2025-02-18 21:07:50,504 - Epoch [46/1000], Step [1800/4367], Loss: 0.4373
2025-02-18 21:08:16,690 - Epoch [46/1000], Step [1900/4367], Loss: 0.2408
2025-02-18 21:08:43,047 - Epoch [46/1000], Step [2000/4367], Loss: 0.5367
2025-02-18 21:09:09,485 - Epoch [46/1000], Step [2100/4367], Loss: 0.2409
2025-02-18 21:09:35,539 - Epoch [46/1000], Step [2200/4367], Loss: 0.3475
2025-02-18 21:10:01,853 - Epoch [46/1000], Step [2300/4367], Loss: 0.3143
2025-02-18 21:10:28,170 - Epoch [46/1000], Step [2400/4367], Loss: 0.3565
2025-02-18 21:10:54,061 - Epoch [46/1000], Step [2500/4367], Loss: 0.3483
2025-02-18 21:11:20,457 - Epoch [46/1000], Step [2600/4367], Loss: 0.2755
2025-02-18 21:11:46,169 - Epoch [46/1000], Step [2700/4367], Loss: 0.1947
2025-02-18 21:12:12,367 - Epoch [46/1000], Step [2800/4367], Loss: 0.3485
2025-02-18 21:12:38,439 - Epoch [46/1000], Step [2900/4367], Loss: 0.2269
2025-02-18 21:13:04,428 - Epoch [46/1000], Step [3000/4367], Loss: 0.2431
2025-02-18 21:13:30,550 - Epoch [46/1000], Step [3100/4367], Loss: 0.2999
2025-02-18 21:13:56,535 - Epoch [46/1000], Step [3200/4367], Loss: 0.2263
2025-02-18 21:14:22,565 - Epoch [46/1000], Step [3300/4367], Loss: 0.1808
2025-02-18 21:14:48,386 - Epoch [46/1000], Step [3400/4367], Loss: 0.2672
2025-02-18 21:15:14,642 - Epoch [46/1000], Step [3500/4367], Loss: 0.3074
2025-02-18 21:15:40,863 - Epoch [46/1000], Step [3600/4367], Loss: 0.2411
2025-02-18 21:16:07,037 - Epoch [46/1000], Step [3700/4367], Loss: 0.3374
2025-02-18 21:16:33,359 - Epoch [46/1000], Step [3800/4367], Loss: 0.3030
2025-02-18 21:16:59,732 - Epoch [46/1000], Step [3900/4367], Loss: 0.4957
2025-02-18 21:17:26,448 - Epoch [46/1000], Step [4000/4367], Loss: 0.2796
2025-02-18 21:17:52,771 - Epoch [46/1000], Step [4100/4367], Loss: 0.3909
2025-02-18 21:18:18,860 - Epoch [46/1000], Step [4200/4367], Loss: 0.5108
2025-02-18 21:18:45,169 - Epoch [46/1000], Step [4300/4367], Loss: 0.4003
2025-02-18 21:19:14,639 - Epoch [46/1000], Validation Step [100/1090], Val Loss: 0.0002
2025-02-18 21:19:22,618 - Epoch [46/1000], Validation Step [200/1090], Val Loss: 0.0000
2025-02-18 21:19:30,920 - Epoch [46/1000], Validation Step [300/1090], Val Loss: 0.7306
2025-02-18 21:19:39,423 - Epoch [46/1000], Validation Step [400/1090], Val Loss: 0.2299
2025-02-18 21:19:47,401 - Epoch [46/1000], Validation Step [500/1090], Val Loss: 0.3953
2025-02-18 21:19:55,568 - Epoch [46/1000], Validation Step [600/1090], Val Loss: 0.6002
2025-02-18 21:20:03,877 - Epoch [46/1000], Validation Step [700/1090], Val Loss: 0.3686
2025-02-18 21:20:11,545 - Epoch [46/1000], Validation Step [800/1090], Val Loss: 0.1698
2025-02-18 21:20:19,092 - Epoch [46/1000], Validation Step [900/1090], Val Loss: 0.2310
2025-02-18 21:20:27,500 - Epoch [46/1000], Validation Step [1000/1090], Val Loss: 0.0681
2025-02-18 21:20:34,758 - Epoch 46/1000, Train Loss: 0.3593, Val Loss: 0.3854, Accuracy: 86.00%
2025-02-18 21:20:35,526 - Model saved at /data2/personal/sungjin/korean_dialects/classification_second_checkpoint/checkpoint_epoch_46.pth
2025-02-18 21:21:05,261 - Epoch [47/1000], Step [100/4367], Loss: 0.2319
2025-02-18 21:21:31,376 - Epoch [47/1000], Step [200/4367], Loss: 0.2104
2025-02-18 21:21:57,522 - Epoch [47/1000], Step [300/4367], Loss: 0.2332
2025-02-18 21:22:23,788 - Epoch [47/1000], Step [400/4367], Loss: 0.3173
2025-02-18 21:22:50,212 - Epoch [47/1000], Step [500/4367], Loss: 0.4321
2025-02-18 21:23:16,957 - Epoch [47/1000], Step [600/4367], Loss: 0.2274
2025-02-18 21:23:43,641 - Epoch [47/1000], Step [700/4367], Loss: 0.4947
2025-02-18 21:24:10,049 - Epoch [47/1000], Step [800/4367], Loss: 0.2600
2025-02-18 21:24:36,329 - Epoch [47/1000], Step [900/4367], Loss: 0.2100
2025-02-18 21:25:02,664 - Epoch [47/1000], Step [1000/4367], Loss: 0.1890
2025-02-18 21:25:29,119 - Epoch [47/1000], Step [1100/4367], Loss: 0.4467
2025-02-18 21:25:55,283 - Epoch [47/1000], Step [1200/4367], Loss: 0.3182
2025-02-18 21:26:21,802 - Epoch [47/1000], Step [1300/4367], Loss: 0.3300
2025-02-18 21:26:48,092 - Epoch [47/1000], Step [1400/4367], Loss: 0.3846
2025-02-18 21:27:14,317 - Epoch [47/1000], Step [1500/4367], Loss: 0.4041
2025-02-18 21:27:41,061 - Epoch [47/1000], Step [1600/4367], Loss: 0.4012
2025-02-18 21:28:07,851 - Epoch [47/1000], Step [1700/4367], Loss: 0.4086
2025-02-18 21:28:33,981 - Epoch [47/1000], Step [1800/4367], Loss: 0.3951
2025-02-18 21:29:00,250 - Epoch [47/1000], Step [1900/4367], Loss: 0.4156
2025-02-18 21:29:26,886 - Epoch [47/1000], Step [2000/4367], Loss: 0.3751
2025-02-18 21:29:53,416 - Epoch [47/1000], Step [2100/4367], Loss: 0.4552
2025-02-18 21:30:19,909 - Epoch [47/1000], Step [2200/4367], Loss: 0.2902
2025-02-18 21:30:46,606 - Epoch [47/1000], Step [2300/4367], Loss: 0.5582
2025-02-18 21:31:12,726 - Epoch [47/1000], Step [2400/4367], Loss: 0.4895
2025-02-18 21:31:39,276 - Epoch [47/1000], Step [2500/4367], Loss: 0.3642
2025-02-18 21:32:05,920 - Epoch [47/1000], Step [2600/4367], Loss: 0.2556
2025-02-18 21:32:32,453 - Epoch [47/1000], Step [2700/4367], Loss: 0.4240
2025-02-18 21:32:58,688 - Epoch [47/1000], Step [2800/4367], Loss: 0.2064
2025-02-18 21:33:25,198 - Epoch [47/1000], Step [2900/4367], Loss: 0.3264
2025-02-18 21:33:55,105 - Epoch [47/1000], Step [3000/4367], Loss: 0.4540
2025-02-18 21:34:21,345 - Epoch [47/1000], Step [3100/4367], Loss: 0.3082
2025-02-18 21:34:47,403 - Epoch [47/1000], Step [3200/4367], Loss: 0.3457
2025-02-18 21:35:14,010 - Epoch [47/1000], Step [3300/4367], Loss: 0.2746
2025-02-18 21:35:40,352 - Epoch [47/1000], Step [3400/4367], Loss: 0.5072
2025-02-18 21:36:06,691 - Epoch [47/1000], Step [3500/4367], Loss: 0.2988
2025-02-18 21:36:32,544 - Epoch [47/1000], Step [3600/4367], Loss: 0.3028
2025-02-18 21:36:58,994 - Epoch [47/1000], Step [3700/4367], Loss: 0.4552
2025-02-18 21:37:25,565 - Epoch [47/1000], Step [3800/4367], Loss: 0.4625
2025-02-18 21:37:52,006 - Epoch [47/1000], Step [3900/4367], Loss: 0.4862
2025-02-18 21:38:18,406 - Epoch [47/1000], Step [4000/4367], Loss: 0.2917
2025-02-18 21:38:44,693 - Epoch [47/1000], Step [4100/4367], Loss: 0.1638
2025-02-18 21:39:10,863 - Epoch [47/1000], Step [4200/4367], Loss: 0.4251
2025-02-18 21:39:37,176 - Epoch [47/1000], Step [4300/4367], Loss: 0.4629
2025-02-18 21:40:05,857 - Epoch [47/1000], Validation Step [100/1090], Val Loss: 0.0001
2025-02-18 21:40:13,927 - Epoch [47/1000], Validation Step [200/1090], Val Loss: 0.0000
2025-02-18 21:40:22,206 - Epoch [47/1000], Validation Step [300/1090], Val Loss: 0.6272
2025-02-18 21:40:30,795 - Epoch [47/1000], Validation Step [400/1090], Val Loss: 0.3186
2025-02-18 21:40:38,881 - Epoch [47/1000], Validation Step [500/1090], Val Loss: 0.4750
2025-02-18 21:40:47,106 - Epoch [47/1000], Validation Step [600/1090], Val Loss: 0.5585
2025-02-18 21:40:55,667 - Epoch [47/1000], Validation Step [700/1090], Val Loss: 0.3479
2025-02-18 21:41:03,277 - Epoch [47/1000], Validation Step [800/1090], Val Loss: 0.1284
2025-02-18 21:41:10,843 - Epoch [47/1000], Validation Step [900/1090], Val Loss: 0.1970
2025-02-18 21:41:18,964 - Epoch [47/1000], Validation Step [1000/1090], Val Loss: 0.0906
2025-02-18 21:41:26,366 - Epoch 47/1000, Train Loss: 0.3592, Val Loss: 0.3800, Accuracy: 86.26%
2025-02-18 21:41:56,119 - Epoch [48/1000], Step [100/4367], Loss: 0.3681
2025-02-18 21:42:22,524 - Epoch [48/1000], Step [200/4367], Loss: 0.2267
2025-02-18 21:42:48,877 - Epoch [48/1000], Step [300/4367], Loss: 0.4214
2025-02-18 21:43:15,286 - Epoch [48/1000], Step [400/4367], Loss: 0.2815
2025-02-18 21:43:41,762 - Epoch [48/1000], Step [500/4367], Loss: 0.5602
2025-02-18 21:44:07,863 - Epoch [48/1000], Step [600/4367], Loss: 0.4674
2025-02-18 21:44:34,209 - Epoch [48/1000], Step [700/4367], Loss: 0.2411
2025-02-18 21:45:00,780 - Epoch [48/1000], Step [800/4367], Loss: 0.1319
2025-02-18 21:45:27,282 - Epoch [48/1000], Step [900/4367], Loss: 0.5227
2025-02-18 21:45:53,911 - Epoch [48/1000], Step [1000/4367], Loss: 0.1803
2025-02-18 21:46:20,067 - Epoch [48/1000], Step [1100/4367], Loss: 0.4301
2025-02-18 21:46:46,559 - Epoch [48/1000], Step [1200/4367], Loss: 0.4519
2025-02-18 21:47:13,044 - Epoch [48/1000], Step [1300/4367], Loss: 0.5065
2025-02-18 21:47:39,288 - Epoch [48/1000], Step [1400/4367], Loss: 0.4715
2025-02-18 21:48:05,443 - Epoch [48/1000], Step [1500/4367], Loss: 0.5423
2025-02-18 21:48:31,564 - Epoch [48/1000], Step [1600/4367], Loss: 0.3847
2025-02-18 21:48:57,878 - Epoch [48/1000], Step [1700/4367], Loss: 0.4796
2025-02-18 21:49:24,392 - Epoch [48/1000], Step [1800/4367], Loss: 0.2979
2025-02-18 21:49:50,497 - Epoch [48/1000], Step [1900/4367], Loss: 0.1117
2025-02-18 21:50:16,974 - Epoch [48/1000], Step [2000/4367], Loss: 0.2016
2025-02-18 21:50:43,392 - Epoch [48/1000], Step [2100/4367], Loss: 0.4342
2025-02-18 21:51:09,975 - Epoch [48/1000], Step [2200/4367], Loss: 0.1387
2025-02-18 21:51:35,954 - Epoch [48/1000], Step [2300/4367], Loss: 0.4574
2025-02-18 21:52:02,520 - Epoch [48/1000], Step [2400/4367], Loss: 0.3375
2025-02-18 21:52:28,567 - Epoch [48/1000], Step [2500/4367], Loss: 0.2034
2025-02-18 21:52:54,819 - Epoch [48/1000], Step [2600/4367], Loss: 0.3390
2025-02-18 21:53:20,872 - Epoch [48/1000], Step [2700/4367], Loss: 0.3217
2025-02-18 21:53:47,358 - Epoch [48/1000], Step [2800/4367], Loss: 0.4706
2025-02-18 21:54:13,939 - Epoch [48/1000], Step [2900/4367], Loss: 0.2297
2025-02-18 21:54:40,537 - Epoch [48/1000], Step [3000/4367], Loss: 0.5203
2025-02-18 21:55:06,962 - Epoch [48/1000], Step [3100/4367], Loss: 0.2035
2025-02-18 21:55:33,088 - Epoch [48/1000], Step [3200/4367], Loss: 0.4389
2025-02-18 21:55:59,105 - Epoch [48/1000], Step [3300/4367], Loss: 0.3367
2025-02-18 21:56:25,327 - Epoch [48/1000], Step [3400/4367], Loss: 0.3061
2025-02-18 21:56:51,756 - Epoch [48/1000], Step [3500/4367], Loss: 0.2337
2025-02-18 21:57:18,271 - Epoch [48/1000], Step [3600/4367], Loss: 0.5312
2025-02-18 21:57:44,550 - Epoch [48/1000], Step [3700/4367], Loss: 0.3231
2025-02-18 21:58:10,816 - Epoch [48/1000], Step [3800/4367], Loss: 0.1914
2025-02-18 21:58:37,053 - Epoch [48/1000], Step [3900/4367], Loss: 0.2458
2025-02-18 21:59:03,345 - Epoch [48/1000], Step [4000/4367], Loss: 0.4070
2025-02-18 21:59:29,380 - Epoch [48/1000], Step [4100/4367], Loss: 0.3959
2025-02-18 21:59:55,912 - Epoch [48/1000], Step [4200/4367], Loss: 0.3334
2025-02-18 22:00:22,452 - Epoch [48/1000], Step [4300/4367], Loss: 0.2406
2025-02-18 22:00:52,369 - Epoch [48/1000], Validation Step [100/1090], Val Loss: 0.0001
2025-02-18 22:01:00,438 - Epoch [48/1000], Validation Step [200/1090], Val Loss: 0.0000
2025-02-18 22:01:08,699 - Epoch [48/1000], Validation Step [300/1090], Val Loss: 0.6856
2025-02-18 22:01:17,122 - Epoch [48/1000], Validation Step [400/1090], Val Loss: 0.1959
2025-02-18 22:01:24,962 - Epoch [48/1000], Validation Step [500/1090], Val Loss: 0.3320
2025-02-18 22:01:33,240 - Epoch [48/1000], Validation Step [600/1090], Val Loss: 0.6233
2025-02-18 22:01:41,711 - Epoch [48/1000], Validation Step [700/1090], Val Loss: 0.3325
2025-02-18 22:01:49,409 - Epoch [48/1000], Validation Step [800/1090], Val Loss: 0.1776
2025-02-18 22:01:56,990 - Epoch [48/1000], Validation Step [900/1090], Val Loss: 0.2666
2025-02-18 22:02:05,451 - Epoch [48/1000], Validation Step [1000/1090], Val Loss: 0.0601
2025-02-18 22:02:12,691 - Epoch 48/1000, Train Loss: 0.3571, Val Loss: 0.3890, Accuracy: 85.82%
2025-02-18 22:02:13,437 - Model saved at /data2/personal/sungjin/korean_dialects/classification_second_checkpoint/checkpoint_epoch_48.pth
2025-02-18 22:02:43,245 - Epoch [49/1000], Step [100/4367], Loss: 0.2713
2025-02-18 22:03:09,436 - Epoch [49/1000], Step [200/4367], Loss: 0.2188
2025-02-18 22:03:35,858 - Epoch [49/1000], Step [300/4367], Loss: 0.2284
2025-02-18 22:04:02,280 - Epoch [49/1000], Step [400/4367], Loss: 0.6673
2025-02-18 22:04:28,166 - Epoch [49/1000], Step [500/4367], Loss: 0.4742
2025-02-18 22:04:54,656 - Epoch [49/1000], Step [600/4367], Loss: 0.4269
2025-02-18 22:05:21,098 - Epoch [49/1000], Step [700/4367], Loss: 0.2711
2025-02-18 22:05:47,625 - Epoch [49/1000], Step [800/4367], Loss: 0.5485
2025-02-18 22:06:13,636 - Epoch [49/1000], Step [900/4367], Loss: 0.1636
2025-02-18 22:06:40,051 - Epoch [49/1000], Step [1000/4367], Loss: 0.3288
2025-02-18 22:07:06,313 - Epoch [49/1000], Step [1100/4367], Loss: 0.1730
2025-02-18 22:07:32,525 - Epoch [49/1000], Step [1200/4367], Loss: 0.3685
2025-02-18 22:07:58,604 - Epoch [49/1000], Step [1300/4367], Loss: 0.4877
2025-02-18 22:08:24,585 - Epoch [49/1000], Step [1400/4367], Loss: 0.3786
2025-02-18 22:08:50,898 - Epoch [49/1000], Step [1500/4367], Loss: 0.1863
2025-02-18 22:09:16,800 - Epoch [49/1000], Step [1600/4367], Loss: 0.4051
2025-02-18 22:09:42,736 - Epoch [49/1000], Step [1700/4367], Loss: 0.4246
2025-02-18 22:10:08,731 - Epoch [49/1000], Step [1800/4367], Loss: 0.2473
2025-02-18 22:10:34,864 - Epoch [49/1000], Step [1900/4367], Loss: 0.2128
2025-02-18 22:11:00,948 - Epoch [49/1000], Step [2000/4367], Loss: 0.3851
2025-02-18 22:11:26,933 - Epoch [49/1000], Step [2100/4367], Loss: 0.2174
2025-02-18 22:11:53,144 - Epoch [49/1000], Step [2200/4367], Loss: 0.6686
2025-02-18 22:12:19,326 - Epoch [49/1000], Step [2300/4367], Loss: 0.2254
2025-02-18 22:12:45,256 - Epoch [49/1000], Step [2400/4367], Loss: 0.4705
2025-02-18 22:13:11,097 - Epoch [49/1000], Step [2500/4367], Loss: 0.3990
2025-02-18 22:13:37,107 - Epoch [49/1000], Step [2600/4367], Loss: 0.2339
2025-02-18 22:14:03,018 - Epoch [49/1000], Step [2700/4367], Loss: 0.6168
2025-02-18 22:14:29,165 - Epoch [49/1000], Step [2800/4367], Loss: 0.3243
2025-02-18 22:14:54,975 - Epoch [49/1000], Step [2900/4367], Loss: 0.2484
2025-02-18 22:15:20,861 - Epoch [49/1000], Step [3000/4367], Loss: 0.4399
2025-02-18 22:15:46,846 - Epoch [49/1000], Step [3100/4367], Loss: 0.3908
2025-02-18 22:16:12,855 - Epoch [49/1000], Step [3200/4367], Loss: 0.2356
2025-02-18 22:16:38,879 - Epoch [49/1000], Step [3300/4367], Loss: 0.3972
2025-02-18 22:17:04,931 - Epoch [49/1000], Step [3400/4367], Loss: 0.4119
2025-02-18 22:17:31,102 - Epoch [49/1000], Step [3500/4367], Loss: 0.3760
2025-02-18 22:17:57,386 - Epoch [49/1000], Step [3600/4367], Loss: 0.5127
2025-02-18 22:18:23,262 - Epoch [49/1000], Step [3700/4367], Loss: 0.1246
2025-02-18 22:18:49,374 - Epoch [49/1000], Step [3800/4367], Loss: 0.4558
2025-02-18 22:19:15,313 - Epoch [49/1000], Step [3900/4367], Loss: 0.2655
2025-02-18 22:19:41,728 - Epoch [49/1000], Step [4000/4367], Loss: 0.2605
2025-02-18 22:20:07,596 - Epoch [49/1000], Step [4100/4367], Loss: 0.3762
2025-02-18 22:20:33,173 - Epoch [49/1000], Step [4200/4367], Loss: 0.2971
2025-02-18 22:20:59,361 - Epoch [49/1000], Step [4300/4367], Loss: 0.4011
2025-02-18 22:21:27,565 - Epoch [49/1000], Validation Step [100/1090], Val Loss: 0.0000
2025-02-18 22:21:35,194 - Epoch [49/1000], Validation Step [200/1090], Val Loss: 0.0000
2025-02-18 22:21:43,062 - Epoch [49/1000], Validation Step [300/1090], Val Loss: 0.6389
2025-02-18 22:21:51,128 - Epoch [49/1000], Validation Step [400/1090], Val Loss: 0.2409
2025-02-18 22:21:58,625 - Epoch [49/1000], Validation Step [500/1090], Val Loss: 0.3141
2025-02-18 22:22:06,535 - Epoch [49/1000], Validation Step [600/1090], Val Loss: 0.6194
2025-02-18 22:22:14,439 - Epoch [49/1000], Validation Step [700/1090], Val Loss: 0.3649
2025-02-18 22:22:21,758 - Epoch [49/1000], Validation Step [800/1090], Val Loss: 0.2104
2025-02-18 22:22:28,847 - Epoch [49/1000], Validation Step [900/1090], Val Loss: 0.3028
2025-02-18 22:22:36,679 - Epoch [49/1000], Validation Step [1000/1090], Val Loss: 0.0791
2025-02-18 22:22:43,810 - Epoch 49/1000, Train Loss: 0.3589, Val Loss: 0.4002, Accuracy: 85.25%
2025-02-18 22:23:12,989 - Epoch [50/1000], Step [100/4367], Loss: 0.4764
2025-02-18 22:23:39,031 - Epoch [50/1000], Step [200/4367], Loss: 0.1768
2025-02-18 22:24:04,819 - Epoch [50/1000], Step [300/4367], Loss: 0.3478
2025-02-18 22:24:31,028 - Epoch [50/1000], Step [400/4367], Loss: 0.3989
2025-02-18 22:24:57,000 - Epoch [50/1000], Step [500/4367], Loss: 0.4126
2025-02-18 22:25:23,082 - Epoch [50/1000], Step [600/4367], Loss: 0.5245
2025-02-18 22:25:49,149 - Epoch [50/1000], Step [700/4367], Loss: 0.2081
2025-02-18 22:26:15,092 - Epoch [50/1000], Step [800/4367], Loss: 0.5979
2025-02-18 22:26:41,330 - Epoch [50/1000], Step [900/4367], Loss: 0.1417
2025-02-18 22:27:07,119 - Epoch [50/1000], Step [1000/4367], Loss: 0.0905
2025-02-18 22:27:33,039 - Epoch [50/1000], Step [1100/4367], Loss: 0.6084
2025-02-18 22:27:59,042 - Epoch [50/1000], Step [1200/4367], Loss: 0.3388
2025-02-18 22:28:25,247 - Epoch [50/1000], Step [1300/4367], Loss: 0.5539
2025-02-18 22:28:51,518 - Epoch [50/1000], Step [1400/4367], Loss: 0.1784
2025-02-18 22:29:17,498 - Epoch [50/1000], Step [1500/4367], Loss: 0.3494
2025-02-18 22:29:43,422 - Epoch [50/1000], Step [1600/4367], Loss: 0.1548
2025-02-18 22:30:09,171 - Epoch [50/1000], Step [1700/4367], Loss: 0.2375
2025-02-18 22:30:35,160 - Epoch [50/1000], Step [1800/4367], Loss: 0.4134
2025-02-18 22:31:01,174 - Epoch [50/1000], Step [1900/4367], Loss: 0.4558
2025-02-18 22:31:27,359 - Epoch [50/1000], Step [2000/4367], Loss: 0.1336
2025-02-18 22:31:53,352 - Epoch [50/1000], Step [2100/4367], Loss: 0.2343
2025-02-18 22:32:19,320 - Epoch [50/1000], Step [2200/4367], Loss: 0.4163
2025-02-18 22:32:45,447 - Epoch [50/1000], Step [2300/4367], Loss: 0.4107
2025-02-18 22:33:11,674 - Epoch [50/1000], Step [2400/4367], Loss: 0.2301
2025-02-18 22:33:38,054 - Epoch [50/1000], Step [2500/4367], Loss: 0.3578
2025-02-18 22:34:04,212 - Epoch [50/1000], Step [2600/4367], Loss: 0.3947
2025-02-18 22:34:30,112 - Epoch [50/1000], Step [2700/4367], Loss: 0.3199
2025-02-18 22:34:56,180 - Epoch [50/1000], Step [2800/4367], Loss: 0.2705
2025-02-18 22:35:22,461 - Epoch [50/1000], Step [2900/4367], Loss: 0.2906
2025-02-18 22:35:48,516 - Epoch [50/1000], Step [3000/4367], Loss: 0.2518
2025-02-18 22:36:14,860 - Epoch [50/1000], Step [3100/4367], Loss: 0.4404
2025-02-18 22:36:40,936 - Epoch [50/1000], Step [3200/4367], Loss: 0.2742
2025-02-18 22:37:07,107 - Epoch [50/1000], Step [3300/4367], Loss: 0.2568
2025-02-18 22:37:32,981 - Epoch [50/1000], Step [3400/4367], Loss: 0.4191
2025-02-18 22:37:58,780 - Epoch [50/1000], Step [3500/4367], Loss: 0.2577
2025-02-18 22:38:24,988 - Epoch [50/1000], Step [3600/4367], Loss: 0.4635
2025-02-18 22:38:51,236 - Epoch [50/1000], Step [3700/4367], Loss: 0.2225
2025-02-18 22:39:17,591 - Epoch [50/1000], Step [3800/4367], Loss: 0.5791
2025-02-18 22:39:44,007 - Epoch [50/1000], Step [3900/4367], Loss: 0.2956
2025-02-18 22:40:10,036 - Epoch [50/1000], Step [4000/4367], Loss: 0.2870
2025-02-18 22:40:36,270 - Epoch [50/1000], Step [4100/4367], Loss: 0.4406
2025-02-18 22:41:01,996 - Epoch [50/1000], Step [4200/4367], Loss: 0.1427
2025-02-18 22:41:28,111 - Epoch [50/1000], Step [4300/4367], Loss: 0.2026
2025-02-18 22:41:56,492 - Epoch [50/1000], Validation Step [100/1090], Val Loss: 0.0000
2025-02-18 22:42:04,142 - Epoch [50/1000], Validation Step [200/1090], Val Loss: 0.0000
2025-02-18 22:42:12,017 - Epoch [50/1000], Validation Step [300/1090], Val Loss: 0.6792
2025-02-18 22:42:20,099 - Epoch [50/1000], Validation Step [400/1090], Val Loss: 0.1978
2025-02-18 22:42:27,658 - Epoch [50/1000], Validation Step [500/1090], Val Loss: 0.3675
2025-02-18 22:42:35,492 - Epoch [50/1000], Validation Step [600/1090], Val Loss: 0.6141
2025-02-18 22:42:43,433 - Epoch [50/1000], Validation Step [700/1090], Val Loss: 0.3451
2025-02-18 22:42:50,685 - Epoch [50/1000], Validation Step [800/1090], Val Loss: 0.1243
2025-02-18 22:42:57,937 - Epoch [50/1000], Validation Step [900/1090], Val Loss: 0.1972
2025-02-18 22:43:05,741 - Epoch [50/1000], Validation Step [1000/1090], Val Loss: 0.0614
2025-02-18 22:43:12,926 - Epoch 50/1000, Train Loss: 0.3539, Val Loss: 0.3765, Accuracy: 86.28%
2025-02-18 22:43:13,649 - Model saved at /data2/personal/sungjin/korean_dialects/classification_second_checkpoint/checkpoint_epoch_50.pth
2025-02-18 22:43:42,607 - Epoch [51/1000], Step [100/4367], Loss: 0.4764
2025-02-18 22:44:08,678 - Epoch [51/1000], Step [200/4367], Loss: 0.2620
2025-02-18 22:44:34,891 - Epoch [51/1000], Step [300/4367], Loss: 0.2226
2025-02-18 22:45:01,239 - Epoch [51/1000], Step [400/4367], Loss: 0.4998
2025-02-18 22:45:27,506 - Epoch [51/1000], Step [500/4367], Loss: 0.5438
2025-02-18 22:45:53,294 - Epoch [51/1000], Step [600/4367], Loss: 0.0980
2025-02-18 22:46:19,357 - Epoch [51/1000], Step [700/4367], Loss: 0.4588
2025-02-18 22:46:45,267 - Epoch [51/1000], Step [800/4367], Loss: 0.5121
2025-02-18 22:47:11,162 - Epoch [51/1000], Step [900/4367], Loss: 0.3479
2025-02-18 22:47:37,514 - Epoch [51/1000], Step [1000/4367], Loss: 0.5246
2025-02-18 22:48:03,562 - Epoch [51/1000], Step [1100/4367], Loss: 0.4417
2025-02-18 22:48:29,506 - Epoch [51/1000], Step [1200/4367], Loss: 0.3846
2025-02-18 22:48:55,455 - Epoch [51/1000], Step [1300/4367], Loss: 0.4131
2025-02-18 22:49:21,643 - Epoch [51/1000], Step [1400/4367], Loss: 0.7408
2025-02-18 22:49:47,481 - Epoch [51/1000], Step [1500/4367], Loss: 0.2104
2025-02-18 22:50:13,512 - Epoch [51/1000], Step [1600/4367], Loss: 0.3162
2025-02-18 22:50:39,637 - Epoch [51/1000], Step [1700/4367], Loss: 0.2976
2025-02-18 22:51:05,938 - Epoch [51/1000], Step [1800/4367], Loss: 0.4011
2025-02-18 22:51:32,002 - Epoch [51/1000], Step [1900/4367], Loss: 0.4170
2025-02-18 22:51:58,031 - Epoch [51/1000], Step [2000/4367], Loss: 0.2207
2025-02-18 22:52:24,143 - Epoch [51/1000], Step [2100/4367], Loss: 0.1348
2025-02-18 22:52:50,021 - Epoch [51/1000], Step [2200/4367], Loss: 0.1571
2025-02-18 22:53:16,679 - Epoch [51/1000], Step [2300/4367], Loss: 0.1852
2025-02-18 22:53:43,115 - Epoch [51/1000], Step [2400/4367], Loss: 0.2579
2025-02-18 22:54:09,310 - Epoch [51/1000], Step [2500/4367], Loss: 0.2721
2025-02-18 22:54:35,842 - Epoch [51/1000], Step [2600/4367], Loss: 0.2829
2025-02-18 22:55:02,164 - Epoch [51/1000], Step [2700/4367], Loss: 0.4170
2025-02-18 22:55:28,504 - Epoch [51/1000], Step [2800/4367], Loss: 0.3850
2025-02-18 22:55:55,189 - Epoch [51/1000], Step [2900/4367], Loss: 0.2138
2025-02-18 22:56:21,489 - Epoch [51/1000], Step [3000/4367], Loss: 0.1648
2025-02-18 22:56:47,823 - Epoch [51/1000], Step [3100/4367], Loss: 0.2207
2025-02-18 22:57:14,181 - Epoch [51/1000], Step [3200/4367], Loss: 0.2155
2025-02-18 22:57:40,526 - Epoch [51/1000], Step [3300/4367], Loss: 0.2144
2025-02-18 22:58:07,457 - Epoch [51/1000], Step [3400/4367], Loss: 0.1920
2025-02-18 22:58:34,273 - Epoch [51/1000], Step [3500/4367], Loss: 0.3882
2025-02-18 22:59:01,100 - Epoch [51/1000], Step [3600/4367], Loss: 0.3512
2025-02-18 22:59:27,993 - Epoch [51/1000], Step [3700/4367], Loss: 0.3404
2025-02-18 22:59:54,423 - Epoch [51/1000], Step [3800/4367], Loss: 0.4001
2025-02-18 23:00:21,296 - Epoch [51/1000], Step [3900/4367], Loss: 0.2717
2025-02-18 23:00:47,935 - Epoch [51/1000], Step [4000/4367], Loss: 0.2889
2025-02-18 23:01:14,199 - Epoch [51/1000], Step [4100/4367], Loss: 0.5864
2025-02-18 23:01:41,108 - Epoch [51/1000], Step [4200/4367], Loss: 0.4846
2025-02-18 23:02:08,115 - Epoch [51/1000], Step [4300/4367], Loss: 0.6794
2025-02-18 23:02:38,304 - Epoch [51/1000], Validation Step [100/1090], Val Loss: 0.0001
2025-02-18 23:02:47,171 - Epoch [51/1000], Validation Step [200/1090], Val Loss: 0.0001
2025-02-18 23:02:55,864 - Epoch [51/1000], Validation Step [300/1090], Val Loss: 0.6618
2025-02-18 23:03:04,571 - Epoch [51/1000], Validation Step [400/1090], Val Loss: 0.1722
2025-02-18 23:03:12,921 - Epoch [51/1000], Validation Step [500/1090], Val Loss: 0.3763
2025-02-18 23:03:21,553 - Epoch [51/1000], Validation Step [600/1090], Val Loss: 0.5904
2025-02-18 23:03:30,429 - Epoch [51/1000], Validation Step [700/1090], Val Loss: 0.3505
2025-02-18 23:03:38,469 - Epoch [51/1000], Validation Step [800/1090], Val Loss: 0.1318
2025-02-18 23:03:46,610 - Epoch [51/1000], Validation Step [900/1090], Val Loss: 0.1938
2025-02-18 23:03:54,860 - Epoch [51/1000], Validation Step [1000/1090], Val Loss: 0.0543
2025-02-18 23:04:02,405 - Epoch 51/1000, Train Loss: 0.3526, Val Loss: 0.3796, Accuracy: 86.18%
2025-02-18 23:04:32,767 - Epoch [52/1000], Step [100/4367], Loss: 0.3349
2025-02-18 23:04:59,563 - Epoch [52/1000], Step [200/4367], Loss: 0.1376
2025-02-18 23:05:26,594 - Epoch [52/1000], Step [300/4367], Loss: 0.1507
2025-02-18 23:05:53,965 - Epoch [52/1000], Step [400/4367], Loss: 0.4529
2025-02-18 23:06:20,812 - Epoch [52/1000], Step [500/4367], Loss: 0.2290
2025-02-18 23:06:47,677 - Epoch [52/1000], Step [600/4367], Loss: 0.3162
2025-02-18 23:07:14,498 - Epoch [52/1000], Step [700/4367], Loss: 0.2197
2025-02-18 23:07:41,424 - Epoch [52/1000], Step [800/4367], Loss: 0.1973
2025-02-18 23:08:08,615 - Epoch [52/1000], Step [900/4367], Loss: 0.2805
2025-02-18 23:08:35,778 - Epoch [52/1000], Step [1000/4367], Loss: 0.5187
2025-02-18 23:09:03,152 - Epoch [52/1000], Step [1100/4367], Loss: 0.1857
2025-02-18 23:09:30,076 - Epoch [52/1000], Step [1200/4367], Loss: 0.1647
2025-02-18 23:09:56,620 - Epoch [52/1000], Step [1300/4367], Loss: 0.1856
2025-02-18 23:10:23,820 - Epoch [52/1000], Step [1400/4367], Loss: 0.1879
2025-02-18 23:10:50,653 - Epoch [52/1000], Step [1500/4367], Loss: 0.2105
2025-02-18 23:11:17,330 - Epoch [52/1000], Step [1600/4367], Loss: 0.6697
2025-02-18 23:11:44,578 - Epoch [52/1000], Step [1700/4367], Loss: 0.6804
2025-02-18 23:12:10,951 - Epoch [52/1000], Step [1800/4367], Loss: 0.3796
2025-02-18 23:12:37,811 - Epoch [52/1000], Step [1900/4367], Loss: 0.3974
2025-02-18 23:13:04,389 - Epoch [52/1000], Step [2000/4367], Loss: 0.3725
2025-02-18 23:13:31,262 - Epoch [52/1000], Step [2100/4367], Loss: 0.1771
2025-02-18 23:13:57,827 - Epoch [52/1000], Step [2200/4367], Loss: 0.3989
2025-02-18 23:14:24,885 - Epoch [52/1000], Step [2300/4367], Loss: 0.5791
2025-02-18 23:14:51,808 - Epoch [52/1000], Step [2400/4367], Loss: 0.2871
2025-02-18 23:15:18,455 - Epoch [52/1000], Step [2500/4367], Loss: 0.2440
2025-02-18 23:15:44,885 - Epoch [52/1000], Step [2600/4367], Loss: 0.2151
2025-02-18 23:16:11,551 - Epoch [52/1000], Step [2700/4367], Loss: 0.2722
2025-02-18 23:16:38,488 - Epoch [52/1000], Step [2800/4367], Loss: 0.3058
2025-02-18 23:17:05,157 - Epoch [52/1000], Step [2900/4367], Loss: 0.5298
2025-02-18 23:17:32,092 - Epoch [52/1000], Step [3000/4367], Loss: 0.2186
2025-02-18 23:17:58,815 - Epoch [52/1000], Step [3100/4367], Loss: 0.3870
2025-02-18 23:18:25,568 - Epoch [52/1000], Step [3200/4367], Loss: 0.4491
2025-02-18 23:18:52,579 - Epoch [52/1000], Step [3300/4367], Loss: 0.2715
2025-02-18 23:19:19,523 - Epoch [52/1000], Step [3400/4367], Loss: 0.5645
2025-02-18 23:19:46,460 - Epoch [52/1000], Step [3500/4367], Loss: 0.4033
2025-02-18 23:20:13,209 - Epoch [52/1000], Step [3600/4367], Loss: 0.2246
2025-02-18 23:20:40,013 - Epoch [52/1000], Step [3700/4367], Loss: 0.5403
2025-02-18 23:21:06,706 - Epoch [52/1000], Step [3800/4367], Loss: 0.1664
2025-02-18 23:21:33,172 - Epoch [52/1000], Step [3900/4367], Loss: 0.2292
2025-02-18 23:21:59,958 - Epoch [52/1000], Step [4000/4367], Loss: 0.3128
2025-02-18 23:22:26,800 - Epoch [52/1000], Step [4100/4367], Loss: 0.2645
2025-02-18 23:22:53,869 - Epoch [52/1000], Step [4200/4367], Loss: 0.4979
2025-02-18 23:23:20,488 - Epoch [52/1000], Step [4300/4367], Loss: 0.4220
2025-02-18 23:23:50,790 - Epoch [52/1000], Validation Step [100/1090], Val Loss: 0.0001
2025-02-18 23:23:59,009 - Epoch [52/1000], Validation Step [200/1090], Val Loss: 0.0000
2025-02-18 23:24:07,679 - Epoch [52/1000], Validation Step [300/1090], Val Loss: 0.6969
2025-02-18 23:24:16,773 - Epoch [52/1000], Validation Step [400/1090], Val Loss: 0.1351
2025-02-18 23:24:24,875 - Epoch [52/1000], Validation Step [500/1090], Val Loss: 0.3244
2025-02-18 23:24:33,572 - Epoch [52/1000], Validation Step [600/1090], Val Loss: 0.6206
2025-02-18 23:24:42,205 - Epoch [52/1000], Validation Step [700/1090], Val Loss: 0.3626
2025-02-18 23:24:50,338 - Epoch [52/1000], Validation Step [800/1090], Val Loss: 0.1564
2025-02-18 23:24:58,413 - Epoch [52/1000], Validation Step [900/1090], Val Loss: 0.2209
2025-02-18 23:25:06,518 - Epoch [52/1000], Validation Step [1000/1090], Val Loss: 0.0668
2025-02-18 23:25:13,941 - Epoch 52/1000, Train Loss: 0.3534, Val Loss: 0.3848, Accuracy: 85.99%
2025-02-18 23:25:14,667 - Model saved at /data2/personal/sungjin/korean_dialects/classification_second_checkpoint/checkpoint_epoch_52.pth
2025-02-18 23:25:44,956 - Epoch [53/1000], Step [100/4367], Loss: 0.2404
2025-02-18 23:26:11,756 - Epoch [53/1000], Step [200/4367], Loss: 0.3274
2025-02-18 23:26:38,603 - Epoch [53/1000], Step [300/4367], Loss: 0.3552
2025-02-18 23:27:04,959 - Epoch [53/1000], Step [400/4367], Loss: 0.3804
2025-02-18 23:27:32,068 - Epoch [53/1000], Step [500/4367], Loss: 0.1688
2025-02-18 23:27:59,031 - Epoch [53/1000], Step [600/4367], Loss: 0.4770
2025-02-18 23:28:26,014 - Epoch [53/1000], Step [700/4367], Loss: 0.3123
2025-02-18 23:28:52,528 - Epoch [53/1000], Step [800/4367], Loss: 0.4307
2025-02-18 23:29:19,389 - Epoch [53/1000], Step [900/4367], Loss: 0.2541
2025-02-18 23:29:46,206 - Epoch [53/1000], Step [1000/4367], Loss: 0.1984
2025-02-18 23:30:12,971 - Epoch [53/1000], Step [1100/4367], Loss: 0.4237
2025-02-18 23:30:39,694 - Epoch [53/1000], Step [1200/4367], Loss: 0.3198
2025-02-18 23:31:06,272 - Epoch [53/1000], Step [1300/4367], Loss: 0.1906
2025-02-18 23:31:33,394 - Epoch [53/1000], Step [1400/4367], Loss: 0.3552
2025-02-18 23:32:00,298 - Epoch [53/1000], Step [1500/4367], Loss: 0.1555
2025-02-18 23:32:26,613 - Epoch [53/1000], Step [1600/4367], Loss: 0.4037
2025-02-18 23:32:53,183 - Epoch [53/1000], Step [1700/4367], Loss: 0.3869
2025-02-18 23:33:20,288 - Epoch [53/1000], Step [1800/4367], Loss: 0.3600
2025-02-18 23:33:47,373 - Epoch [53/1000], Step [1900/4367], Loss: 0.3149
2025-02-18 23:34:14,495 - Epoch [53/1000], Step [2000/4367], Loss: 0.2652
2025-02-18 23:34:41,462 - Epoch [53/1000], Step [2100/4367], Loss: 0.3909
2025-02-18 23:35:08,901 - Epoch [53/1000], Step [2200/4367], Loss: 0.2883
2025-02-18 23:35:35,560 - Epoch [53/1000], Step [2300/4367], Loss: 0.3726
2025-02-18 23:36:02,230 - Epoch [53/1000], Step [2400/4367], Loss: 0.4791
2025-02-18 23:36:28,770 - Epoch [53/1000], Step [2500/4367], Loss: 0.2237
2025-02-18 23:36:55,292 - Epoch [53/1000], Step [2600/4367], Loss: 0.3764
2025-02-18 23:37:22,036 - Epoch [53/1000], Step [2700/4367], Loss: 0.3962
2025-02-18 23:37:48,889 - Epoch [53/1000], Step [2800/4367], Loss: 0.2127
2025-02-18 23:38:15,946 - Epoch [53/1000], Step [2900/4367], Loss: 0.3934
2025-02-18 23:38:42,976 - Epoch [53/1000], Step [3000/4367], Loss: 0.3009
2025-02-18 23:39:11,351 - Epoch [53/1000], Step [3100/4367], Loss: 0.6375
2025-02-18 23:39:40,619 - Epoch [53/1000], Step [3200/4367], Loss: 0.2530
2025-02-18 23:40:19,822 - Epoch [53/1000], Step [3300/4367], Loss: 0.2513
2025-02-18 23:40:46,752 - Epoch [53/1000], Step [3400/4367], Loss: 0.4075
2025-02-18 23:41:13,727 - Epoch [53/1000], Step [3500/4367], Loss: 0.3320
2025-02-18 23:41:40,847 - Epoch [53/1000], Step [3600/4367], Loss: 0.1522
2025-02-18 23:42:08,299 - Epoch [53/1000], Step [3700/4367], Loss: 0.3831
2025-02-18 23:42:35,568 - Epoch [53/1000], Step [3800/4367], Loss: 0.6388
2025-02-18 23:43:02,277 - Epoch [53/1000], Step [3900/4367], Loss: 0.3520
2025-02-18 23:43:29,011 - Epoch [53/1000], Step [4000/4367], Loss: 0.2283
2025-02-18 23:43:55,909 - Epoch [53/1000], Step [4100/4367], Loss: 0.4375
2025-02-18 23:44:23,670 - Epoch [53/1000], Step [4200/4367], Loss: 0.5005
2025-02-18 23:44:50,622 - Epoch [53/1000], Step [4300/4367], Loss: 0.3365
2025-02-18 23:45:20,928 - Epoch [53/1000], Validation Step [100/1090], Val Loss: 0.0000
2025-02-18 23:45:29,304 - Epoch [53/1000], Validation Step [200/1090], Val Loss: 0.0000
2025-02-18 23:45:38,070 - Epoch [53/1000], Validation Step [300/1090], Val Loss: 0.6813
2025-02-18 23:45:47,137 - Epoch [53/1000], Validation Step [400/1090], Val Loss: 0.1759
2025-02-18 23:45:55,882 - Epoch [53/1000], Validation Step [500/1090], Val Loss: 0.3632
2025-02-18 23:46:04,871 - Epoch [53/1000], Validation Step [600/1090], Val Loss: 0.5780
2025-02-18 23:46:13,781 - Epoch [53/1000], Validation Step [700/1090], Val Loss: 0.3398
2025-02-18 23:46:22,228 - Epoch [53/1000], Validation Step [800/1090], Val Loss: 0.1396
2025-02-18 23:46:30,785 - Epoch [53/1000], Validation Step [900/1090], Val Loss: 0.2036
2025-02-18 23:46:39,802 - Epoch [53/1000], Validation Step [1000/1090], Val Loss: 0.1703
2025-02-18 23:46:47,851 - Epoch 53/1000, Train Loss: 0.3507, Val Loss: 0.3952, Accuracy: 85.76%
2025-02-18 23:47:18,509 - Epoch [54/1000], Step [100/4367], Loss: 0.4126
2025-02-18 23:47:45,347 - Epoch [54/1000], Step [200/4367], Loss: 0.2760
2025-02-18 23:48:12,294 - Epoch [54/1000], Step [300/4367], Loss: 0.5405
2025-02-18 23:48:39,218 - Epoch [54/1000], Step [400/4367], Loss: 0.2765
2025-02-18 23:49:06,321 - Epoch [54/1000], Step [500/4367], Loss: 0.3062
2025-02-18 23:49:33,297 - Epoch [54/1000], Step [600/4367], Loss: 0.2537
2025-02-18 23:50:00,278 - Epoch [54/1000], Step [700/4367], Loss: 0.1058
2025-02-18 23:50:27,120 - Epoch [54/1000], Step [800/4367], Loss: 0.5239
2025-02-18 23:50:54,216 - Epoch [54/1000], Step [900/4367], Loss: 0.7020
2025-02-18 23:51:21,045 - Epoch [54/1000], Step [1000/4367], Loss: 0.3109
2025-02-18 23:51:47,768 - Epoch [54/1000], Step [1100/4367], Loss: 0.2734
2025-02-18 23:52:14,256 - Epoch [54/1000], Step [1200/4367], Loss: 0.3252
2025-02-18 23:52:40,740 - Epoch [54/1000], Step [1300/4367], Loss: 0.2929
2025-02-18 23:53:07,290 - Epoch [54/1000], Step [1400/4367], Loss: 0.4521
2025-02-18 23:53:33,572 - Epoch [54/1000], Step [1500/4367], Loss: 0.3288
2025-02-18 23:53:59,871 - Epoch [54/1000], Step [1600/4367], Loss: 0.1985
2025-02-18 23:54:26,202 - Epoch [54/1000], Step [1700/4367], Loss: 0.2780
2025-02-18 23:54:52,389 - Epoch [54/1000], Step [1800/4367], Loss: 0.4718
2025-02-18 23:55:18,807 - Epoch [54/1000], Step [1900/4367], Loss: 0.1981
2025-02-18 23:55:44,947 - Epoch [54/1000], Step [2000/4367], Loss: 0.3252
2025-02-18 23:56:10,797 - Epoch [54/1000], Step [2100/4367], Loss: 0.5580
2025-02-18 23:56:36,852 - Epoch [54/1000], Step [2200/4367], Loss: 0.3730
2025-02-18 23:57:02,958 - Epoch [54/1000], Step [2300/4367], Loss: 0.3550
2025-02-18 23:57:29,355 - Epoch [54/1000], Step [2400/4367], Loss: 0.3487
2025-02-18 23:57:55,297 - Epoch [54/1000], Step [2500/4367], Loss: 0.2515
2025-02-18 23:58:21,336 - Epoch [54/1000], Step [2600/4367], Loss: 0.2982
2025-02-18 23:58:47,483 - Epoch [54/1000], Step [2700/4367], Loss: 0.3964
2025-02-18 23:59:13,707 - Epoch [54/1000], Step [2800/4367], Loss: 0.4216
2025-02-18 23:59:39,807 - Epoch [54/1000], Step [2900/4367], Loss: 0.5532
2025-02-19 00:00:05,841 - Epoch [54/1000], Step [3000/4367], Loss: 0.3278
2025-02-19 00:00:32,151 - Epoch [54/1000], Step [3100/4367], Loss: 0.4737
2025-02-19 00:00:58,363 - Epoch [54/1000], Step [3200/4367], Loss: 0.4313
2025-02-19 00:01:24,450 - Epoch [54/1000], Step [3300/4367], Loss: 0.2200
2025-02-19 00:01:50,691 - Epoch [54/1000], Step [3400/4367], Loss: 0.1041
2025-02-19 00:02:16,937 - Epoch [54/1000], Step [3500/4367], Loss: 0.4598
2025-02-19 00:02:43,366 - Epoch [54/1000], Step [3600/4367], Loss: 0.3591
2025-02-19 00:03:09,686 - Epoch [54/1000], Step [3700/4367], Loss: 0.2388
2025-02-19 00:03:35,828 - Epoch [54/1000], Step [3800/4367], Loss: 0.5139
2025-02-19 00:04:01,723 - Epoch [54/1000], Step [3900/4367], Loss: 0.3445
2025-02-19 00:04:27,864 - Epoch [54/1000], Step [4000/4367], Loss: 0.2415
2025-02-19 00:04:53,898 - Epoch [54/1000], Step [4100/4367], Loss: 0.1363
2025-02-19 00:05:20,095 - Epoch [54/1000], Step [4200/4367], Loss: 0.2440
2025-02-19 00:05:45,793 - Epoch [54/1000], Step [4300/4367], Loss: 0.2124
2025-02-19 00:06:14,551 - Epoch [54/1000], Validation Step [100/1090], Val Loss: 0.0000
2025-02-19 00:06:22,444 - Epoch [54/1000], Validation Step [200/1090], Val Loss: 0.0000
2025-02-19 00:06:30,599 - Epoch [54/1000], Validation Step [300/1090], Val Loss: 0.7404
2025-02-19 00:06:38,937 - Epoch [54/1000], Validation Step [400/1090], Val Loss: 0.1889
2025-02-19 00:06:46,562 - Epoch [54/1000], Validation Step [500/1090], Val Loss: 0.3429
2025-02-19 00:06:54,496 - Epoch [54/1000], Validation Step [600/1090], Val Loss: 0.5569
2025-02-19 00:07:02,749 - Epoch [54/1000], Validation Step [700/1090], Val Loss: 0.3272
2025-02-19 00:07:10,170 - Epoch [54/1000], Validation Step [800/1090], Val Loss: 0.1544
2025-02-19 00:07:17,642 - Epoch [54/1000], Validation Step [900/1090], Val Loss: 0.2203
2025-02-19 00:07:25,770 - Epoch [54/1000], Validation Step [1000/1090], Val Loss: 0.0956
2025-02-19 00:07:32,976 - Epoch 54/1000, Train Loss: 0.3512, Val Loss: 0.3859, Accuracy: 86.00%
2025-02-19 00:07:33,678 - Model saved at /data2/personal/sungjin/korean_dialects/classification_second_checkpoint/checkpoint_epoch_54.pth
2025-02-19 00:08:03,450 - Epoch [55/1000], Step [100/4367], Loss: 0.1980
2025-02-19 00:08:29,743 - Epoch [55/1000], Step [200/4367], Loss: 0.3285
2025-02-19 00:08:55,889 - Epoch [55/1000], Step [300/4367], Loss: 0.2229
2025-02-19 00:09:22,007 - Epoch [55/1000], Step [400/4367], Loss: 0.3940
2025-02-19 00:09:47,940 - Epoch [55/1000], Step [500/4367], Loss: 0.2140
2025-02-19 00:10:13,935 - Epoch [55/1000], Step [600/4367], Loss: 0.3264
2025-02-19 00:10:39,702 - Epoch [55/1000], Step [700/4367], Loss: 0.2908
2025-02-19 00:11:05,959 - Epoch [55/1000], Step [800/4367], Loss: 0.5113
2025-02-19 00:11:31,753 - Epoch [55/1000], Step [900/4367], Loss: 0.2007
2025-02-19 00:11:58,060 - Epoch [55/1000], Step [1000/4367], Loss: 0.2055
2025-02-19 00:12:24,296 - Epoch [55/1000], Step [1100/4367], Loss: 0.3733
2025-02-19 00:12:50,511 - Epoch [55/1000], Step [1200/4367], Loss: 0.2909
2025-02-19 00:13:16,443 - Epoch [55/1000], Step [1300/4367], Loss: 0.2626
2025-02-19 00:13:42,702 - Epoch [55/1000], Step [1400/4367], Loss: 0.5295
2025-02-19 00:14:08,733 - Epoch [55/1000], Step [1500/4367], Loss: 0.2514
2025-02-19 00:14:34,733 - Epoch [55/1000], Step [1600/4367], Loss: 0.3322
2025-02-19 00:15:00,919 - Epoch [55/1000], Step [1700/4367], Loss: 0.5151
2025-02-19 00:15:27,035 - Epoch [55/1000], Step [1800/4367], Loss: 0.2121
2025-02-19 00:15:52,941 - Epoch [55/1000], Step [1900/4367], Loss: 0.3505
2025-02-19 00:16:18,851 - Epoch [55/1000], Step [2000/4367], Loss: 0.2237
2025-02-19 00:16:45,146 - Epoch [55/1000], Step [2100/4367], Loss: 0.2779
2025-02-19 00:17:11,379 - Epoch [55/1000], Step [2200/4367], Loss: 0.4292
2025-02-19 00:17:37,766 - Epoch [55/1000], Step [2300/4367], Loss: 0.3834
2025-02-19 00:18:04,310 - Epoch [55/1000], Step [2400/4367], Loss: 0.2520
2025-02-19 00:18:30,703 - Epoch [55/1000], Step [2500/4367], Loss: 0.4099
2025-02-19 00:18:57,258 - Epoch [55/1000], Step [2600/4367], Loss: 0.2613
2025-02-19 00:19:23,616 - Epoch [55/1000], Step [2700/4367], Loss: 0.3844
2025-02-19 00:19:50,075 - Epoch [55/1000], Step [2800/4367], Loss: 0.6126
2025-02-19 00:20:16,326 - Epoch [55/1000], Step [2900/4367], Loss: 0.4974
2025-02-19 00:20:42,654 - Epoch [55/1000], Step [3000/4367], Loss: 0.3009
2025-02-19 00:21:08,909 - Epoch [55/1000], Step [3100/4367], Loss: 0.3788
2025-02-19 00:21:35,279 - Epoch [55/1000], Step [3200/4367], Loss: 0.3391
2025-02-19 00:22:01,910 - Epoch [55/1000], Step [3300/4367], Loss: 0.4761
2025-02-19 00:22:28,136 - Epoch [55/1000], Step [3400/4367], Loss: 0.3314
2025-02-19 00:22:54,833 - Epoch [55/1000], Step [3500/4367], Loss: 0.3583
2025-02-19 00:23:21,339 - Epoch [55/1000], Step [3600/4367], Loss: 0.3852
2025-02-19 00:23:47,651 - Epoch [55/1000], Step [3700/4367], Loss: 0.2709
2025-02-19 00:24:13,660 - Epoch [55/1000], Step [3800/4367], Loss: 0.2604
2025-02-19 00:24:40,229 - Epoch [55/1000], Step [3900/4367], Loss: 0.3164
2025-02-19 00:25:06,722 - Epoch [55/1000], Step [4000/4367], Loss: 0.3167
2025-02-19 00:25:33,381 - Epoch [55/1000], Step [4100/4367], Loss: 0.4937
2025-02-19 00:26:00,029 - Epoch [55/1000], Step [4200/4367], Loss: 0.4162
2025-02-19 00:26:26,258 - Epoch [55/1000], Step [4300/4367], Loss: 0.3998
2025-02-19 00:26:55,953 - Epoch [55/1000], Validation Step [100/1090], Val Loss: 0.0001
2025-02-19 00:27:04,219 - Epoch [55/1000], Validation Step [200/1090], Val Loss: 0.0000
2025-02-19 00:27:12,707 - Epoch [55/1000], Validation Step [300/1090], Val Loss: 0.6296
2025-02-19 00:27:21,239 - Epoch [55/1000], Validation Step [400/1090], Val Loss: 0.2018
2025-02-19 00:27:29,532 - Epoch [55/1000], Validation Step [500/1090], Val Loss: 0.3933
2025-02-19 00:27:37,820 - Epoch [55/1000], Validation Step [600/1090], Val Loss: 0.5561
2025-02-19 00:27:46,432 - Epoch [55/1000], Validation Step [700/1090], Val Loss: 0.3573
2025-02-19 00:27:54,412 - Epoch [55/1000], Validation Step [800/1090], Val Loss: 0.1447
2025-02-19 00:28:02,195 - Epoch [55/1000], Validation Step [900/1090], Val Loss: 0.2253
2025-02-19 00:28:11,091 - Epoch [55/1000], Validation Step [1000/1090], Val Loss: 0.0528
2025-02-19 00:28:18,593 - Epoch 55/1000, Train Loss: 0.3492, Val Loss: 0.3720, Accuracy: 86.48%
2025-02-19 00:28:48,242 - Epoch [56/1000], Step [100/4367], Loss: 0.3331
2025-02-19 00:29:14,802 - Epoch [56/1000], Step [200/4367], Loss: 0.2646
2025-02-19 00:29:40,864 - Epoch [56/1000], Step [300/4367], Loss: 0.2310
2025-02-19 00:30:07,557 - Epoch [56/1000], Step [400/4367], Loss: 0.2893
2025-02-19 00:30:33,836 - Epoch [56/1000], Step [500/4367], Loss: 0.4335
2025-02-19 00:31:00,044 - Epoch [56/1000], Step [600/4367], Loss: 0.2579
2025-02-19 00:31:26,334 - Epoch [56/1000], Step [700/4367], Loss: 0.4401
2025-02-19 00:31:52,674 - Epoch [56/1000], Step [800/4367], Loss: 0.6623
2025-02-19 00:32:19,046 - Epoch [56/1000], Step [900/4367], Loss: 0.1175
2025-02-19 00:32:45,393 - Epoch [56/1000], Step [1000/4367], Loss: 0.5133
2025-02-19 00:33:12,130 - Epoch [56/1000], Step [1100/4367], Loss: 0.1981
2025-02-19 00:33:38,552 - Epoch [56/1000], Step [1200/4367], Loss: 0.5185
2025-02-19 00:34:05,352 - Epoch [56/1000], Step [1300/4367], Loss: 0.6142
2025-02-19 00:34:31,510 - Epoch [56/1000], Step [1400/4367], Loss: 0.3589
2025-02-19 00:34:57,735 - Epoch [56/1000], Step [1500/4367], Loss: 0.3475
2025-02-19 00:35:24,150 - Epoch [56/1000], Step [1600/4367], Loss: 0.3722
2025-02-19 00:35:50,721 - Epoch [56/1000], Step [1700/4367], Loss: 0.4448
2025-02-19 00:36:17,605 - Epoch [56/1000], Step [1800/4367], Loss: 0.4314
2025-02-19 00:36:44,058 - Epoch [56/1000], Step [1900/4367], Loss: 0.5899
2025-02-19 00:37:10,663 - Epoch [56/1000], Step [2000/4367], Loss: 0.2259
2025-02-19 00:37:37,332 - Epoch [56/1000], Step [2100/4367], Loss: 0.1604
2025-02-19 00:38:04,091 - Epoch [56/1000], Step [2200/4367], Loss: 0.2762
2025-02-19 00:38:30,629 - Epoch [56/1000], Step [2300/4367], Loss: 0.2651
2025-02-19 00:38:57,078 - Epoch [56/1000], Step [2400/4367], Loss: 0.4256
2025-02-19 00:39:23,506 - Epoch [56/1000], Step [2500/4367], Loss: 0.1471
2025-02-19 00:39:50,055 - Epoch [56/1000], Step [2600/4367], Loss: 0.1955
2025-02-19 00:40:16,346 - Epoch [56/1000], Step [2700/4367], Loss: 0.2002
2025-02-19 00:40:42,954 - Epoch [56/1000], Step [2800/4367], Loss: 0.3733
2025-02-19 00:41:09,184 - Epoch [56/1000], Step [2900/4367], Loss: 0.5018
2025-02-19 00:41:35,622 - Epoch [56/1000], Step [3000/4367], Loss: 0.3087
2025-02-19 00:42:02,134 - Epoch [56/1000], Step [3100/4367], Loss: 0.3696
2025-02-19 00:42:28,439 - Epoch [56/1000], Step [3200/4367], Loss: 0.3744
2025-02-19 00:42:54,858 - Epoch [56/1000], Step [3300/4367], Loss: 0.2974
2025-02-19 00:43:21,247 - Epoch [56/1000], Step [3400/4367], Loss: 0.4541
2025-02-19 00:43:47,504 - Epoch [56/1000], Step [3500/4367], Loss: 0.4166
2025-02-19 00:44:13,991 - Epoch [56/1000], Step [3600/4367], Loss: 0.2851
2025-02-19 00:44:40,791 - Epoch [56/1000], Step [3700/4367], Loss: 0.4801
2025-02-19 00:45:07,090 - Epoch [56/1000], Step [3800/4367], Loss: 0.4404
2025-02-19 00:45:33,960 - Epoch [56/1000], Step [3900/4367], Loss: 0.4771
2025-02-19 00:46:00,574 - Epoch [56/1000], Step [4000/4367], Loss: 0.4824
2025-02-19 00:46:27,229 - Epoch [56/1000], Step [4100/4367], Loss: 0.2858
2025-02-19 00:46:53,612 - Epoch [56/1000], Step [4200/4367], Loss: 0.3288
2025-02-19 00:47:19,816 - Epoch [56/1000], Step [4300/4367], Loss: 0.2262
2025-02-19 00:47:49,491 - Epoch [56/1000], Validation Step [100/1090], Val Loss: 0.0000
2025-02-19 00:47:57,761 - Epoch [56/1000], Validation Step [200/1090], Val Loss: 0.0000
2025-02-19 00:48:06,346 - Epoch [56/1000], Validation Step [300/1090], Val Loss: 0.7338
2025-02-19 00:48:14,647 - Epoch [56/1000], Validation Step [400/1090], Val Loss: 0.2091
2025-02-19 00:48:23,049 - Epoch [56/1000], Validation Step [500/1090], Val Loss: 0.3591
2025-02-19 00:48:31,582 - Epoch [56/1000], Validation Step [600/1090], Val Loss: 0.5427
2025-02-19 00:48:40,212 - Epoch [56/1000], Validation Step [700/1090], Val Loss: 0.3298
2025-02-19 00:48:47,892 - Epoch [56/1000], Validation Step [800/1090], Val Loss: 0.1516
2025-02-19 00:48:55,938 - Epoch [56/1000], Validation Step [900/1090], Val Loss: 0.2620
2025-02-19 00:49:04,252 - Epoch [56/1000], Validation Step [1000/1090], Val Loss: 0.0523
2025-02-19 00:49:11,877 - Epoch 56/1000, Train Loss: 0.3500, Val Loss: 0.3819, Accuracy: 86.18%
2025-02-19 00:49:12,720 - Model saved at /data2/personal/sungjin/korean_dialects/classification_second_checkpoint/checkpoint_epoch_56.pth
2025-02-19 00:49:42,621 - Epoch [57/1000], Step [100/4367], Loss: 0.3255
2025-02-19 00:50:09,200 - Epoch [57/1000], Step [200/4367], Loss: 0.5124
2025-02-19 00:50:36,063 - Epoch [57/1000], Step [300/4367], Loss: 0.4043
2025-02-19 00:51:02,418 - Epoch [57/1000], Step [400/4367], Loss: 0.4695
2025-02-19 00:51:29,321 - Epoch [57/1000], Step [500/4367], Loss: 0.7231
2025-02-19 00:51:55,942 - Epoch [57/1000], Step [600/4367], Loss: 0.2369
2025-02-19 00:52:22,245 - Epoch [57/1000], Step [700/4367], Loss: 0.2992
2025-02-19 00:52:48,345 - Epoch [57/1000], Step [800/4367], Loss: 0.4113
2025-02-19 00:53:14,575 - Epoch [57/1000], Step [900/4367], Loss: 0.3402
2025-02-19 00:53:41,084 - Epoch [57/1000], Step [1000/4367], Loss: 0.2256
2025-02-19 00:54:07,987 - Epoch [57/1000], Step [1100/4367], Loss: 0.4144
2025-02-19 00:54:34,722 - Epoch [57/1000], Step [1200/4367], Loss: 0.4194
2025-02-19 00:55:01,199 - Epoch [57/1000], Step [1300/4367], Loss: 0.1576
2025-02-19 00:55:28,087 - Epoch [57/1000], Step [1400/4367], Loss: 0.3222
2025-02-19 00:55:54,894 - Epoch [57/1000], Step [1500/4367], Loss: 0.2659
2025-02-19 00:56:21,388 - Epoch [57/1000], Step [1600/4367], Loss: 0.2650
2025-02-19 00:56:47,836 - Epoch [57/1000], Step [1700/4367], Loss: 0.3414
2025-02-19 00:57:13,982 - Epoch [57/1000], Step [1800/4367], Loss: 0.5620
2025-02-19 00:57:40,271 - Epoch [57/1000], Step [1900/4367], Loss: 0.7184
2025-02-19 00:58:07,022 - Epoch [57/1000], Step [2000/4367], Loss: 0.2955
2025-02-19 00:58:33,675 - Epoch [57/1000], Step [2100/4367], Loss: 0.2466
2025-02-19 00:58:59,806 - Epoch [57/1000], Step [2200/4367], Loss: 0.3235
2025-02-19 00:59:26,326 - Epoch [57/1000], Step [2300/4367], Loss: 0.3278
2025-02-19 00:59:52,574 - Epoch [57/1000], Step [2400/4367], Loss: 0.5675
2025-02-19 01:00:19,136 - Epoch [57/1000], Step [2500/4367], Loss: 0.2737
2025-02-19 01:00:45,826 - Epoch [57/1000], Step [2600/4367], Loss: 0.2414
2025-02-19 01:01:11,839 - Epoch [57/1000], Step [2700/4367], Loss: 0.2713
2025-02-19 01:01:38,479 - Epoch [57/1000], Step [2800/4367], Loss: 0.4895
2025-02-19 01:02:04,705 - Epoch [57/1000], Step [2900/4367], Loss: 0.1722
2025-02-19 01:02:31,027 - Epoch [57/1000], Step [3000/4367], Loss: 0.1704
2025-02-19 01:02:57,360 - Epoch [57/1000], Step [3100/4367], Loss: 0.3167
2025-02-19 01:03:23,775 - Epoch [57/1000], Step [3200/4367], Loss: 0.4262
2025-02-19 01:03:50,263 - Epoch [57/1000], Step [3300/4367], Loss: 0.3988
2025-02-19 01:04:16,633 - Epoch [57/1000], Step [3400/4367], Loss: 0.3117
2025-02-19 01:04:43,517 - Epoch [57/1000], Step [3500/4367], Loss: 0.2547
2025-02-19 01:05:10,123 - Epoch [57/1000], Step [3600/4367], Loss: 0.3518
2025-02-19 01:05:37,170 - Epoch [57/1000], Step [3700/4367], Loss: 0.1659
2025-02-19 01:06:03,681 - Epoch [57/1000], Step [3800/4367], Loss: 0.6098
2025-02-19 01:06:30,012 - Epoch [57/1000], Step [3900/4367], Loss: 0.1344
2025-02-19 01:06:56,835 - Epoch [57/1000], Step [4000/4367], Loss: 0.1970
2025-02-19 01:07:23,372 - Epoch [57/1000], Step [4100/4367], Loss: 0.3012
2025-02-19 01:07:49,707 - Epoch [57/1000], Step [4200/4367], Loss: 0.4799
2025-02-19 01:08:16,041 - Epoch [57/1000], Step [4300/4367], Loss: 0.5231
2025-02-19 01:08:45,794 - Epoch [57/1000], Validation Step [100/1090], Val Loss: 0.0000
2025-02-19 01:08:54,056 - Epoch [57/1000], Validation Step [200/1090], Val Loss: 0.0000
2025-02-19 01:09:02,789 - Epoch [57/1000], Validation Step [300/1090], Val Loss: 0.6467
2025-02-19 01:09:11,202 - Epoch [57/1000], Validation Step [400/1090], Val Loss: 0.2192
2025-02-19 01:09:19,738 - Epoch [57/1000], Validation Step [500/1090], Val Loss: 0.4343
2025-02-19 01:09:28,396 - Epoch [57/1000], Validation Step [600/1090], Val Loss: 0.5578
2025-02-19 01:09:37,135 - Epoch [57/1000], Validation Step [700/1090], Val Loss: 0.3557
2025-02-19 01:09:44,921 - Epoch [57/1000], Validation Step [800/1090], Val Loss: 0.1240
2025-02-19 01:09:52,330 - Epoch [57/1000], Validation Step [900/1090], Val Loss: 0.2086
2025-02-19 01:10:00,884 - Epoch [57/1000], Validation Step [1000/1090], Val Loss: 0.0741
2025-02-19 01:10:08,448 - Epoch 57/1000, Train Loss: 0.3484, Val Loss: 0.3701, Accuracy: 86.49%
2025-02-19 01:10:38,650 - Epoch [58/1000], Step [100/4367], Loss: 0.5461
2025-02-19 01:11:04,764 - Epoch [58/1000], Step [200/4367], Loss: 0.5607
2025-02-19 01:11:30,990 - Epoch [58/1000], Step [300/4367], Loss: 0.3075
2025-02-19 01:11:56,737 - Epoch [58/1000], Step [400/4367], Loss: 0.4104
2025-02-19 01:12:22,617 - Epoch [58/1000], Step [500/4367], Loss: 0.3893
2025-02-19 01:12:48,390 - Epoch [58/1000], Step [600/4367], Loss: 0.3183
2025-02-19 01:13:14,330 - Epoch [58/1000], Step [700/4367], Loss: 0.2490
2025-02-19 01:13:40,350 - Epoch [58/1000], Step [800/4367], Loss: 0.3231
2025-02-19 01:14:06,497 - Epoch [58/1000], Step [900/4367], Loss: 0.1994
2025-02-19 01:14:32,414 - Epoch [58/1000], Step [1000/4367], Loss: 0.5302
2025-02-19 01:14:58,638 - Epoch [58/1000], Step [1100/4367], Loss: 0.6551
2025-02-19 01:15:24,776 - Epoch [58/1000], Step [1200/4367], Loss: 0.2748
2025-02-19 01:15:51,112 - Epoch [58/1000], Step [1300/4367], Loss: 0.6349
2025-02-19 01:16:17,043 - Epoch [58/1000], Step [1400/4367], Loss: 0.1565
2025-02-19 01:16:43,912 - Epoch [58/1000], Step [1500/4367], Loss: 0.2821
2025-02-19 01:17:09,920 - Epoch [58/1000], Step [1600/4367], Loss: 0.2426
2025-02-19 01:17:36,265 - Epoch [58/1000], Step [1700/4367], Loss: 0.3715
2025-02-19 01:18:02,332 - Epoch [58/1000], Step [1800/4367], Loss: 0.2106
2025-02-19 01:18:28,527 - Epoch [58/1000], Step [1900/4367], Loss: 0.4173
2025-02-19 01:18:54,623 - Epoch [58/1000], Step [2000/4367], Loss: 0.2085
2025-02-19 01:19:21,002 - Epoch [58/1000], Step [2100/4367], Loss: 0.3871
2025-02-19 01:19:47,268 - Epoch [58/1000], Step [2200/4367], Loss: 0.3549
2025-02-19 01:20:13,314 - Epoch [58/1000], Step [2300/4367], Loss: 0.4315
2025-02-19 01:20:39,803 - Epoch [58/1000], Step [2400/4367], Loss: 0.3198
2025-02-19 01:21:05,719 - Epoch [58/1000], Step [2500/4367], Loss: 0.1368
2025-02-19 01:21:31,921 - Epoch [58/1000], Step [2600/4367], Loss: 0.1793
2025-02-19 01:21:57,930 - Epoch [58/1000], Step [2700/4367], Loss: 0.4208
2025-02-19 01:22:23,853 - Epoch [58/1000], Step [2800/4367], Loss: 0.3276
2025-02-19 01:22:50,116 - Epoch [58/1000], Step [2900/4367], Loss: 0.2464
2025-02-19 01:23:16,298 - Epoch [58/1000], Step [3000/4367], Loss: 0.1800
2025-02-19 01:23:42,373 - Epoch [58/1000], Step [3100/4367], Loss: 0.7227
2025-02-19 01:24:08,145 - Epoch [58/1000], Step [3200/4367], Loss: 0.3203
2025-02-19 01:24:34,445 - Epoch [58/1000], Step [3300/4367], Loss: 0.4547
2025-02-19 01:25:00,636 - Epoch [58/1000], Step [3400/4367], Loss: 0.0986
2025-02-19 01:25:26,406 - Epoch [58/1000], Step [3500/4367], Loss: 0.2477
2025-02-19 01:25:52,263 - Epoch [58/1000], Step [3600/4367], Loss: 0.4674
2025-02-19 01:26:18,093 - Epoch [58/1000], Step [3700/4367], Loss: 0.2486
2025-02-19 01:26:44,234 - Epoch [58/1000], Step [3800/4367], Loss: 0.2233
2025-02-19 01:27:10,291 - Epoch [58/1000], Step [3900/4367], Loss: 0.3477
2025-02-19 01:27:36,395 - Epoch [58/1000], Step [4000/4367], Loss: 0.3652
2025-02-19 01:28:02,458 - Epoch [58/1000], Step [4100/4367], Loss: 0.2749
2025-02-19 01:28:28,441 - Epoch [58/1000], Step [4200/4367], Loss: 0.2774
2025-02-19 01:28:54,210 - Epoch [58/1000], Step [4300/4367], Loss: 0.2525
2025-02-19 01:29:21,034 - Epoch [58/1000], Validation Step [100/1090], Val Loss: 0.0000
2025-02-19 01:29:28,407 - Epoch [58/1000], Validation Step [200/1090], Val Loss: 0.0000
2025-02-19 01:29:35,846 - Epoch [58/1000], Validation Step [300/1090], Val Loss: 0.6890
2025-02-19 01:29:43,535 - Epoch [58/1000], Validation Step [400/1090], Val Loss: 0.1947
2025-02-19 01:29:50,840 - Epoch [58/1000], Validation Step [500/1090], Val Loss: 0.3711
2025-02-19 01:29:58,410 - Epoch [58/1000], Validation Step [600/1090], Val Loss: 0.5688
2025-02-19 01:30:06,060 - Epoch [58/1000], Validation Step [700/1090], Val Loss: 0.3439
2025-02-19 01:30:13,087 - Epoch [58/1000], Validation Step [800/1090], Val Loss: 0.1318
2025-02-19 01:30:19,935 - Epoch [58/1000], Validation Step [900/1090], Val Loss: 0.1988
2025-02-19 01:30:27,342 - Epoch [58/1000], Validation Step [1000/1090], Val Loss: 0.0737
2025-02-19 01:30:34,248 - Epoch 58/1000, Train Loss: 0.3484, Val Loss: 0.3791, Accuracy: 86.16%
2025-02-19 01:30:34,744 - Model saved at /data2/personal/sungjin/korean_dialects/classification_second_checkpoint/checkpoint_epoch_58.pth
2025-02-19 01:31:02,930 - Epoch [59/1000], Step [100/4367], Loss: 0.2474
2025-02-19 01:31:28,746 - Epoch [59/1000], Step [200/4367], Loss: 0.2524
2025-02-19 01:31:54,629 - Epoch [59/1000], Step [300/4367], Loss: 0.4107
2025-02-19 01:32:20,487 - Epoch [59/1000], Step [400/4367], Loss: 0.2023
2025-02-19 01:32:46,516 - Epoch [59/1000], Step [500/4367], Loss: 0.3902
2025-02-19 01:33:12,193 - Epoch [59/1000], Step [600/4367], Loss: 0.6511
2025-02-19 01:33:38,183 - Epoch [59/1000], Step [700/4367], Loss: 0.1610
2025-02-19 01:34:03,656 - Epoch [59/1000], Step [800/4367], Loss: 0.3522
2025-02-19 01:34:29,486 - Epoch [59/1000], Step [900/4367], Loss: 0.3600
2025-02-19 01:34:55,105 - Epoch [59/1000], Step [1000/4367], Loss: 0.3442
2025-02-19 01:35:21,025 - Epoch [59/1000], Step [1100/4367], Loss: 0.3652
2025-02-19 01:35:46,605 - Epoch [59/1000], Step [1200/4367], Loss: 0.3504
2025-02-19 01:36:12,017 - Epoch [59/1000], Step [1300/4367], Loss: 0.2192
2025-02-19 01:36:38,080 - Epoch [59/1000], Step [1400/4367], Loss: 0.4766
2025-02-19 01:37:03,827 - Epoch [59/1000], Step [1500/4367], Loss: 0.3349
2025-02-19 01:37:29,494 - Epoch [59/1000], Step [1600/4367], Loss: 0.2641
2025-02-19 01:37:55,080 - Epoch [59/1000], Step [1700/4367], Loss: 0.2444
2025-02-19 01:38:21,052 - Epoch [59/1000], Step [1800/4367], Loss: 0.2848
2025-02-19 01:38:46,787 - Epoch [59/1000], Step [1900/4367], Loss: 0.3104
2025-02-19 01:39:12,764 - Epoch [59/1000], Step [2000/4367], Loss: 0.3296
2025-02-19 01:39:38,704 - Epoch [59/1000], Step [2100/4367], Loss: 0.3272
2025-02-19 01:40:04,397 - Epoch [59/1000], Step [2200/4367], Loss: 0.3370
2025-02-19 01:40:30,207 - Epoch [59/1000], Step [2300/4367], Loss: 0.2935
2025-02-19 01:40:55,970 - Epoch [59/1000], Step [2400/4367], Loss: 0.1980
2025-02-19 01:41:22,077 - Epoch [59/1000], Step [2500/4367], Loss: 0.4993
2025-02-19 01:41:47,924 - Epoch [59/1000], Step [2600/4367], Loss: 0.1464
2025-02-19 01:42:13,263 - Epoch [59/1000], Step [2700/4367], Loss: 0.4317
2025-02-19 01:42:39,489 - Epoch [59/1000], Step [2800/4367], Loss: 0.4330
2025-02-19 01:43:05,694 - Epoch [59/1000], Step [2900/4367], Loss: 0.2421
2025-02-19 01:43:31,945 - Epoch [59/1000], Step [3000/4367], Loss: 0.7546
2025-02-19 01:43:57,743 - Epoch [59/1000], Step [3100/4367], Loss: 0.2504
2025-02-19 01:44:23,550 - Epoch [59/1000], Step [3200/4367], Loss: 0.5008
2025-02-19 01:44:49,543 - Epoch [59/1000], Step [3300/4367], Loss: 0.4527
2025-02-19 01:45:15,417 - Epoch [59/1000], Step [3400/4367], Loss: 0.6389
2025-02-19 01:45:41,523 - Epoch [59/1000], Step [3500/4367], Loss: 0.4978
2025-02-19 01:46:07,403 - Epoch [59/1000], Step [3600/4367], Loss: 0.1708
2025-02-19 01:46:33,160 - Epoch [59/1000], Step [3700/4367], Loss: 0.4564
2025-02-19 01:46:59,133 - Epoch [59/1000], Step [3800/4367], Loss: 0.1903
2025-02-19 01:47:25,055 - Epoch [59/1000], Step [3900/4367], Loss: 0.1538
2025-02-19 01:47:50,690 - Epoch [59/1000], Step [4000/4367], Loss: 0.3519
2025-02-19 01:48:16,401 - Epoch [59/1000], Step [4100/4367], Loss: 0.1136
2025-02-19 01:48:42,112 - Epoch [59/1000], Step [4200/4367], Loss: 0.3414
2025-02-19 01:49:08,392 - Epoch [59/1000], Step [4300/4367], Loss: 0.4064
2025-02-19 01:49:35,129 - Epoch [59/1000], Validation Step [100/1090], Val Loss: 0.0001
2025-02-19 01:49:42,493 - Epoch [59/1000], Validation Step [200/1090], Val Loss: 0.0000
2025-02-19 01:49:49,925 - Epoch [59/1000], Validation Step [300/1090], Val Loss: 0.6428
2025-02-19 01:49:57,639 - Epoch [59/1000], Validation Step [400/1090], Val Loss: 0.1735
2025-02-19 01:50:04,906 - Epoch [59/1000], Validation Step [500/1090], Val Loss: 0.3863
2025-02-19 01:50:12,500 - Epoch [59/1000], Validation Step [600/1090], Val Loss: 0.5936
2025-02-19 01:50:20,123 - Epoch [59/1000], Validation Step [700/1090], Val Loss: 0.3931
2025-02-19 01:50:27,148 - Epoch [59/1000], Validation Step [800/1090], Val Loss: 0.1209
2025-02-19 01:50:34,009 - Epoch [59/1000], Validation Step [900/1090], Val Loss: 0.1887
2025-02-19 01:50:41,404 - Epoch [59/1000], Validation Step [1000/1090], Val Loss: 0.0679
2025-02-19 01:50:48,337 - Epoch 59/1000, Train Loss: 0.3491, Val Loss: 0.3739, Accuracy: 86.43%
2025-02-19 01:51:16,431 - Epoch [60/1000], Step [100/4367], Loss: 0.4460
2025-02-19 01:51:42,227 - Epoch [60/1000], Step [200/4367], Loss: 0.2449
2025-02-19 01:52:08,068 - Epoch [60/1000], Step [300/4367], Loss: 0.4042
2025-02-19 01:52:33,970 - Epoch [60/1000], Step [400/4367], Loss: 0.2737
2025-02-19 01:52:59,621 - Epoch [60/1000], Step [500/4367], Loss: 0.4360
2025-02-19 01:53:25,461 - Epoch [60/1000], Step [600/4367], Loss: 0.1149
2025-02-19 01:53:51,117 - Epoch [60/1000], Step [700/4367], Loss: 0.3318
2025-02-19 01:54:16,912 - Epoch [60/1000], Step [800/4367], Loss: 0.3902
2025-02-19 01:54:42,865 - Epoch [60/1000], Step [900/4367], Loss: 0.2556
2025-02-19 01:55:08,764 - Epoch [60/1000], Step [1000/4367], Loss: 0.3087
2025-02-19 01:55:34,438 - Epoch [60/1000], Step [1100/4367], Loss: 0.3392
2025-02-19 01:56:00,358 - Epoch [60/1000], Step [1200/4367], Loss: 0.2731
2025-02-19 01:56:26,273 - Epoch [60/1000], Step [1300/4367], Loss: 0.2983
2025-02-19 01:56:52,050 - Epoch [60/1000], Step [1400/4367], Loss: 0.4748
2025-02-19 01:57:17,807 - Epoch [60/1000], Step [1500/4367], Loss: 0.4046
2025-02-19 01:57:43,288 - Epoch [60/1000], Step [1600/4367], Loss: 0.2473
2025-02-19 01:58:09,177 - Epoch [60/1000], Step [1700/4367], Loss: 0.5250
2025-02-19 01:58:35,296 - Epoch [60/1000], Step [1800/4367], Loss: 0.3508
2025-02-19 01:59:01,121 - Epoch [60/1000], Step [1900/4367], Loss: 0.3462
2025-02-19 01:59:27,046 - Epoch [60/1000], Step [2000/4367], Loss: 0.2189
2025-02-19 01:59:52,826 - Epoch [60/1000], Step [2100/4367], Loss: 0.3597
2025-02-19 02:00:18,585 - Epoch [60/1000], Step [2200/4367], Loss: 0.5236
2025-02-19 02:00:44,341 - Epoch [60/1000], Step [2300/4367], Loss: 0.2875
2025-02-19 02:01:10,253 - Epoch [60/1000], Step [2400/4367], Loss: 0.2420
2025-02-19 02:01:36,174 - Epoch [60/1000], Step [2500/4367], Loss: 0.3353
2025-02-19 02:02:01,980 - Epoch [60/1000], Step [2600/4367], Loss: 0.1259
2025-02-19 02:02:28,062 - Epoch [60/1000], Step [2700/4367], Loss: 0.2538
2025-02-19 02:02:54,373 - Epoch [60/1000], Step [2800/4367], Loss: 0.3316
2025-02-19 02:03:20,488 - Epoch [60/1000], Step [2900/4367], Loss: 0.3936
2025-02-19 02:03:46,294 - Epoch [60/1000], Step [3000/4367], Loss: 0.3749
2025-02-19 02:04:12,326 - Epoch [60/1000], Step [3100/4367], Loss: 0.3018
2025-02-19 02:04:38,078 - Epoch [60/1000], Step [3200/4367], Loss: 0.5071
2025-02-19 02:05:04,138 - Epoch [60/1000], Step [3300/4367], Loss: 0.7719
2025-02-19 02:05:29,906 - Epoch [60/1000], Step [3400/4367], Loss: 0.1002
2025-02-19 02:05:55,868 - Epoch [60/1000], Step [3500/4367], Loss: 0.3271
2025-02-19 02:06:21,996 - Epoch [60/1000], Step [3600/4367], Loss: 0.1685
2025-02-19 02:06:48,101 - Epoch [60/1000], Step [3700/4367], Loss: 0.2567
2025-02-19 02:07:13,909 - Epoch [60/1000], Step [3800/4367], Loss: 0.4280
2025-02-19 02:07:39,917 - Epoch [60/1000], Step [3900/4367], Loss: 0.2137
2025-02-19 02:08:05,967 - Epoch [60/1000], Step [4000/4367], Loss: 0.3233
2025-02-19 02:08:32,043 - Epoch [60/1000], Step [4100/4367], Loss: 0.2730
2025-02-19 02:08:57,687 - Epoch [60/1000], Step [4200/4367], Loss: 0.4890
2025-02-19 02:09:23,713 - Epoch [60/1000], Step [4300/4367], Loss: 0.1868
2025-02-19 02:09:50,648 - Epoch [60/1000], Validation Step [100/1090], Val Loss: 0.0000
2025-02-19 02:09:58,020 - Epoch [60/1000], Validation Step [200/1090], Val Loss: 0.0000
2025-02-19 02:10:05,491 - Epoch [60/1000], Validation Step [300/1090], Val Loss: 0.6553
2025-02-19 02:10:13,205 - Epoch [60/1000], Validation Step [400/1090], Val Loss: 0.2498
2025-02-19 02:10:20,456 - Epoch [60/1000], Validation Step [500/1090], Val Loss: 0.4278
2025-02-19 02:10:28,061 - Epoch [60/1000], Validation Step [600/1090], Val Loss: 0.5502
2025-02-19 02:10:35,714 - Epoch [60/1000], Validation Step [700/1090], Val Loss: 0.3332
2025-02-19 02:10:42,721 - Epoch [60/1000], Validation Step [800/1090], Val Loss: 0.1283
2025-02-19 02:10:49,577 - Epoch [60/1000], Validation Step [900/1090], Val Loss: 0.2012
2025-02-19 02:10:56,947 - Epoch [60/1000], Validation Step [1000/1090], Val Loss: 0.0679
2025-02-19 02:11:03,853 - Epoch 60/1000, Train Loss: 0.3480, Val Loss: 0.3706, Accuracy: 86.50%
2025-02-19 02:11:04,367 - Model saved at /data2/personal/sungjin/korean_dialects/classification_second_checkpoint/checkpoint_epoch_60.pth
2025-02-19 02:11:32,302 - Epoch [61/1000], Step [100/4367], Loss: 0.4074
2025-02-19 02:11:58,263 - Epoch [61/1000], Step [200/4367], Loss: 0.5906
2025-02-19 02:12:24,115 - Epoch [61/1000], Step [300/4367], Loss: 0.4247
2025-02-19 02:12:49,975 - Epoch [61/1000], Step [400/4367], Loss: 0.1739
2025-02-19 02:13:15,887 - Epoch [61/1000], Step [500/4367], Loss: 0.4999
2025-02-19 02:13:41,854 - Epoch [61/1000], Step [600/4367], Loss: 0.3663
2025-02-19 02:14:08,140 - Epoch [61/1000], Step [700/4367], Loss: 0.1296
2025-02-19 02:14:33,975 - Epoch [61/1000], Step [800/4367], Loss: 0.4768
2025-02-19 02:15:00,248 - Epoch [61/1000], Step [900/4367], Loss: 0.3748
2025-02-19 02:15:26,291 - Epoch [61/1000], Step [1000/4367], Loss: 0.2986
2025-02-19 02:15:52,011 - Epoch [61/1000], Step [1100/4367], Loss: 0.2828
2025-02-19 02:16:17,883 - Epoch [61/1000], Step [1200/4367], Loss: 0.2365
2025-02-19 02:16:44,037 - Epoch [61/1000], Step [1300/4367], Loss: 0.2268
2025-02-19 02:17:10,316 - Epoch [61/1000], Step [1400/4367], Loss: 0.2470
2025-02-19 02:17:36,063 - Epoch [61/1000], Step [1500/4367], Loss: 0.2741
2025-02-19 02:18:02,239 - Epoch [61/1000], Step [1600/4367], Loss: 0.3019
2025-02-19 02:18:28,213 - Epoch [61/1000], Step [1700/4367], Loss: 0.5556
2025-02-19 02:18:54,010 - Epoch [61/1000], Step [1800/4367], Loss: 0.3437
2025-02-19 02:19:20,300 - Epoch [61/1000], Step [1900/4367], Loss: 0.2301
2025-02-19 02:19:46,388 - Epoch [61/1000], Step [2000/4367], Loss: 0.3486
2025-02-19 02:20:12,108 - Epoch [61/1000], Step [2100/4367], Loss: 0.2876
2025-02-19 02:20:38,076 - Epoch [61/1000], Step [2200/4367], Loss: 0.3470
2025-02-19 02:21:03,739 - Epoch [61/1000], Step [2300/4367], Loss: 0.4674
2025-02-19 02:21:29,646 - Epoch [61/1000], Step [2400/4367], Loss: 0.4157
2025-02-19 02:21:55,439 - Epoch [61/1000], Step [2500/4367], Loss: 0.3193
2025-02-19 02:22:21,233 - Epoch [61/1000], Step [2600/4367], Loss: 0.1178
2025-02-19 02:22:47,107 - Epoch [61/1000], Step [2700/4367], Loss: 0.3693
2025-02-19 02:23:13,124 - Epoch [61/1000], Step [2800/4367], Loss: 0.3145
2025-02-19 02:23:38,936 - Epoch [61/1000], Step [2900/4367], Loss: 0.2405
2025-02-19 02:24:04,889 - Epoch [61/1000], Step [3000/4367], Loss: 0.2395
2025-02-19 02:24:31,320 - Epoch [61/1000], Step [3100/4367], Loss: 0.3514
2025-02-19 02:24:56,773 - Epoch [61/1000], Step [3200/4367], Loss: 0.2833
2025-02-19 02:25:23,022 - Epoch [61/1000], Step [3300/4367], Loss: 0.3965
2025-02-19 02:25:48,992 - Epoch [61/1000], Step [3400/4367], Loss: 0.2259
2025-02-19 02:26:15,285 - Epoch [61/1000], Step [3500/4367], Loss: 0.4742
2025-02-19 02:26:41,315 - Epoch [61/1000], Step [3600/4367], Loss: 0.3225
2025-02-19 02:27:07,176 - Epoch [61/1000], Step [3700/4367], Loss: 0.3206
2025-02-19 02:27:33,308 - Epoch [61/1000], Step [3800/4367], Loss: 0.1923
2025-02-19 02:27:59,248 - Epoch [61/1000], Step [3900/4367], Loss: 0.3421
2025-02-19 02:28:24,962 - Epoch [61/1000], Step [4000/4367], Loss: 0.2240
2025-02-19 02:28:50,828 - Epoch [61/1000], Step [4100/4367], Loss: 0.2264
2025-02-19 02:29:16,979 - Epoch [61/1000], Step [4200/4367], Loss: 0.6939
2025-02-19 02:29:42,977 - Epoch [61/1000], Step [4300/4367], Loss: 0.3060
2025-02-19 02:30:09,836 - Epoch [61/1000], Validation Step [100/1090], Val Loss: 0.0001
2025-02-19 02:30:17,219 - Epoch [61/1000], Validation Step [200/1090], Val Loss: 0.0000
2025-02-19 02:30:24,670 - Epoch [61/1000], Validation Step [300/1090], Val Loss: 0.6648
2025-02-19 02:30:32,375 - Epoch [61/1000], Validation Step [400/1090], Val Loss: 0.1881
2025-02-19 02:30:39,639 - Epoch [61/1000], Validation Step [500/1090], Val Loss: 0.3798
2025-02-19 02:30:47,258 - Epoch [61/1000], Validation Step [600/1090], Val Loss: 0.5418
2025-02-19 02:30:54,885 - Epoch [61/1000], Validation Step [700/1090], Val Loss: 0.3369
2025-02-19 02:31:01,913 - Epoch [61/1000], Validation Step [800/1090], Val Loss: 0.1509
2025-02-19 02:31:08,787 - Epoch [61/1000], Validation Step [900/1090], Val Loss: 0.2335
2025-02-19 02:31:16,196 - Epoch [61/1000], Validation Step [1000/1090], Val Loss: 0.0868
2025-02-19 02:31:23,138 - Epoch 61/1000, Train Loss: 0.3480, Val Loss: 0.3771, Accuracy: 86.17%
2025-02-19 02:31:51,434 - Epoch [62/1000], Step [100/4367], Loss: 0.4317
2025-02-19 02:32:17,543 - Epoch [62/1000], Step [200/4367], Loss: 0.3837
2025-02-19 02:32:43,471 - Epoch [62/1000], Step [300/4367], Loss: 0.4347
2025-02-19 02:33:09,196 - Epoch [62/1000], Step [400/4367], Loss: 0.3918
2025-02-19 02:33:34,844 - Epoch [62/1000], Step [500/4367], Loss: 0.5535
2025-02-19 02:34:00,634 - Epoch [62/1000], Step [600/4367], Loss: 0.6929
2025-02-19 02:34:26,744 - Epoch [62/1000], Step [700/4367], Loss: 0.2688
2025-02-19 02:34:52,461 - Epoch [62/1000], Step [800/4367], Loss: 0.5645
2025-02-19 02:35:18,556 - Epoch [62/1000], Step [900/4367], Loss: 0.3201
2025-02-19 02:35:44,226 - Epoch [62/1000], Step [1000/4367], Loss: 0.2794
2025-02-19 02:36:10,183 - Epoch [62/1000], Step [1100/4367], Loss: 0.2833
2025-02-19 02:36:36,239 - Epoch [62/1000], Step [1200/4367], Loss: 0.2817
2025-02-19 02:37:01,926 - Epoch [62/1000], Step [1300/4367], Loss: 0.3109
2025-02-19 02:37:27,857 - Epoch [62/1000], Step [1400/4367], Loss: 0.2339
2025-02-19 02:37:53,637 - Epoch [62/1000], Step [1500/4367], Loss: 0.2745
2025-02-19 02:38:19,641 - Epoch [62/1000], Step [1600/4367], Loss: 0.3933
2025-02-19 02:38:45,396 - Epoch [62/1000], Step [1700/4367], Loss: 0.3697
2025-02-19 02:39:11,395 - Epoch [62/1000], Step [1800/4367], Loss: 0.3557
2025-02-19 02:39:37,434 - Epoch [62/1000], Step [1900/4367], Loss: 0.4175
2025-02-19 02:40:03,552 - Epoch [62/1000], Step [2000/4367], Loss: 0.2804
2025-02-19 02:40:29,673 - Epoch [62/1000], Step [2100/4367], Loss: 0.1643
2025-02-19 02:40:55,573 - Epoch [62/1000], Step [2200/4367], Loss: 0.2500
2025-02-19 02:41:21,698 - Epoch [62/1000], Step [2300/4367], Loss: 0.3275
2025-02-19 02:41:47,546 - Epoch [62/1000], Step [2400/4367], Loss: 0.3809
2025-02-19 02:42:13,706 - Epoch [62/1000], Step [2500/4367], Loss: 0.3968
2025-02-19 02:42:39,366 - Epoch [62/1000], Step [2600/4367], Loss: 0.2718
2025-02-19 02:43:05,329 - Epoch [62/1000], Step [2700/4367], Loss: 0.2930
2025-02-19 02:43:31,124 - Epoch [62/1000], Step [2800/4367], Loss: 0.2072
2025-02-19 02:43:57,176 - Epoch [62/1000], Step [2900/4367], Loss: 0.6518
2025-02-19 02:44:23,141 - Epoch [62/1000], Step [3000/4367], Loss: 0.3042
2025-02-19 02:44:49,132 - Epoch [62/1000], Step [3100/4367], Loss: 0.1409
2025-02-19 02:45:15,124 - Epoch [62/1000], Step [3200/4367], Loss: 0.1933
2025-02-19 02:45:41,248 - Epoch [62/1000], Step [3300/4367], Loss: 0.5124
2025-02-19 02:46:07,060 - Epoch [62/1000], Step [3400/4367], Loss: 0.4498
2025-02-19 02:46:32,925 - Epoch [62/1000], Step [3500/4367], Loss: 0.1433
2025-02-19 02:46:58,724 - Epoch [62/1000], Step [3600/4367], Loss: 0.2452
2025-02-19 02:47:24,918 - Epoch [62/1000], Step [3700/4367], Loss: 0.3450
2025-02-19 02:47:50,790 - Epoch [62/1000], Step [3800/4367], Loss: 0.3433
2025-02-19 02:48:16,829 - Epoch [62/1000], Step [3900/4367], Loss: 0.2454
2025-02-19 02:48:42,752 - Epoch [62/1000], Step [4000/4367], Loss: 0.2581
2025-02-19 02:49:08,936 - Epoch [62/1000], Step [4100/4367], Loss: 0.5088
2025-02-19 02:49:34,494 - Epoch [62/1000], Step [4200/4367], Loss: 0.4629
2025-02-19 02:50:00,724 - Epoch [62/1000], Step [4300/4367], Loss: 0.2771
2025-02-19 02:50:27,501 - Epoch [62/1000], Validation Step [100/1090], Val Loss: 0.0001
2025-02-19 02:50:34,861 - Epoch [62/1000], Validation Step [200/1090], Val Loss: 0.0000
2025-02-19 02:50:42,315 - Epoch [62/1000], Validation Step [300/1090], Val Loss: 0.6894
2025-02-19 02:50:50,018 - Epoch [62/1000], Validation Step [400/1090], Val Loss: 0.2093
2025-02-19 02:50:57,283 - Epoch [62/1000], Validation Step [500/1090], Val Loss: 0.4317
2025-02-19 02:51:04,870 - Epoch [62/1000], Validation Step [600/1090], Val Loss: 0.5446
2025-02-19 02:51:12,521 - Epoch [62/1000], Validation Step [700/1090], Val Loss: 0.3478
2025-02-19 02:51:19,542 - Epoch [62/1000], Validation Step [800/1090], Val Loss: 0.1082
2025-02-19 02:51:26,405 - Epoch [62/1000], Validation Step [900/1090], Val Loss: 0.1797
2025-02-19 02:51:33,802 - Epoch [62/1000], Validation Step [1000/1090], Val Loss: 0.0658
2025-02-19 02:51:40,730 - Epoch 62/1000, Train Loss: 0.3470, Val Loss: 0.3660, Accuracy: 86.68%
2025-02-19 02:51:41,248 - Model saved at /data2/personal/sungjin/korean_dialects/classification_second_checkpoint/checkpoint_epoch_62.pth
2025-02-19 02:52:09,696 - Epoch [63/1000], Step [100/4367], Loss: 0.1862
2025-02-19 02:52:35,714 - Epoch [63/1000], Step [200/4367], Loss: 0.2400
2025-02-19 02:53:01,780 - Epoch [63/1000], Step [300/4367], Loss: 0.3002
2025-02-19 02:53:28,086 - Epoch [63/1000], Step [400/4367], Loss: 0.5591
2025-02-19 02:53:53,963 - Epoch [63/1000], Step [500/4367], Loss: 0.1311
2025-02-19 02:54:20,265 - Epoch [63/1000], Step [600/4367], Loss: 0.1481
2025-02-19 02:54:46,084 - Epoch [63/1000], Step [700/4367], Loss: 0.5199
2025-02-19 02:55:12,053 - Epoch [63/1000], Step [800/4367], Loss: 0.4526
2025-02-19 02:55:37,903 - Epoch [63/1000], Step [900/4367], Loss: 0.7796
2025-02-19 02:56:03,977 - Epoch [63/1000], Step [1000/4367], Loss: 0.3086
2025-02-19 02:56:29,656 - Epoch [63/1000], Step [1100/4367], Loss: 0.3345
2025-02-19 02:56:55,413 - Epoch [63/1000], Step [1200/4367], Loss: 0.3731
2025-02-19 02:57:21,822 - Epoch [63/1000], Step [1300/4367], Loss: 0.6850
2025-02-19 02:57:48,175 - Epoch [63/1000], Step [1400/4367], Loss: 0.2449
2025-02-19 02:58:13,920 - Epoch [63/1000], Step [1500/4367], Loss: 0.3908
2025-02-19 02:58:39,930 - Epoch [63/1000], Step [1600/4367], Loss: 0.5336
2025-02-19 02:59:05,851 - Epoch [63/1000], Step [1700/4367], Loss: 0.3912
2025-02-19 02:59:32,008 - Epoch [63/1000], Step [1800/4367], Loss: 0.3228
2025-02-19 02:59:57,819 - Epoch [63/1000], Step [1900/4367], Loss: 0.2403
2025-02-19 03:00:23,520 - Epoch [63/1000], Step [2000/4367], Loss: 0.2183
2025-02-19 03:00:49,596 - Epoch [63/1000], Step [2100/4367], Loss: 0.5935
2025-02-19 03:01:15,350 - Epoch [63/1000], Step [2200/4367], Loss: 0.3400
2025-02-19 03:01:41,845 - Epoch [63/1000], Step [2300/4367], Loss: 0.3749
2025-02-19 03:02:07,437 - Epoch [63/1000], Step [2400/4367], Loss: 0.3426
2025-02-19 03:02:33,662 - Epoch [63/1000], Step [2500/4367], Loss: 0.4383
2025-02-19 03:02:59,830 - Epoch [63/1000], Step [2600/4367], Loss: 0.1209
2025-02-19 03:03:25,666 - Epoch [63/1000], Step [2700/4367], Loss: 0.2344
2025-02-19 03:03:51,288 - Epoch [63/1000], Step [2800/4367], Loss: 0.3038
2025-02-19 03:04:16,945 - Epoch [63/1000], Step [2900/4367], Loss: 0.3533
2025-02-19 03:04:42,652 - Epoch [63/1000], Step [3000/4367], Loss: 0.1360
2025-02-19 03:05:08,629 - Epoch [63/1000], Step [3100/4367], Loss: 0.5950
2025-02-19 03:05:34,624 - Epoch [63/1000], Step [3200/4367], Loss: 0.1946
2025-02-19 03:06:00,484 - Epoch [63/1000], Step [3300/4367], Loss: 0.5070
2025-02-19 03:06:26,373 - Epoch [63/1000], Step [3400/4367], Loss: 0.3455
2025-02-19 03:06:52,247 - Epoch [63/1000], Step [3500/4367], Loss: 0.3945
2025-02-19 03:07:18,040 - Epoch [63/1000], Step [3600/4367], Loss: 0.4919
2025-02-19 03:07:44,198 - Epoch [63/1000], Step [3700/4367], Loss: 0.3149
2025-02-19 03:08:09,912 - Epoch [63/1000], Step [3800/4367], Loss: 0.3928
2025-02-19 03:08:35,892 - Epoch [63/1000], Step [3900/4367], Loss: 0.3449
2025-02-19 03:09:01,498 - Epoch [63/1000], Step [4000/4367], Loss: 0.1892
2025-02-19 03:09:27,150 - Epoch [63/1000], Step [4100/4367], Loss: 0.2926
2025-02-19 03:09:52,830 - Epoch [63/1000], Step [4200/4367], Loss: 0.3055
2025-02-19 03:10:18,665 - Epoch [63/1000], Step [4300/4367], Loss: 0.3462
2025-02-19 03:10:45,386 - Epoch [63/1000], Validation Step [100/1090], Val Loss: 0.0000
2025-02-19 03:10:52,682 - Epoch [63/1000], Validation Step [200/1090], Val Loss: 0.0000
2025-02-19 03:11:00,049 - Epoch [63/1000], Validation Step [300/1090], Val Loss: 0.7025
2025-02-19 03:11:07,672 - Epoch [63/1000], Validation Step [400/1090], Val Loss: 0.1971
2025-02-19 03:11:14,869 - Epoch [63/1000], Validation Step [500/1090], Val Loss: 0.3883
2025-02-19 03:11:22,401 - Epoch [63/1000], Validation Step [600/1090], Val Loss: 0.5540
2025-02-19 03:11:29,992 - Epoch [63/1000], Validation Step [700/1090], Val Loss: 0.3396
2025-02-19 03:11:36,952 - Epoch [63/1000], Validation Step [800/1090], Val Loss: 0.1224
2025-02-19 03:11:43,841 - Epoch [63/1000], Validation Step [900/1090], Val Loss: 0.2200
2025-02-19 03:11:51,197 - Epoch [63/1000], Validation Step [1000/1090], Val Loss: 0.0632
2025-02-19 03:11:58,039 - Epoch 63/1000, Train Loss: 0.3469, Val Loss: 0.3705, Accuracy: 86.49%
2025-02-19 03:12:26,080 - Epoch [64/1000], Step [100/4367], Loss: 0.3084
2025-02-19 03:12:51,919 - Epoch [64/1000], Step [200/4367], Loss: 0.2111
2025-02-19 03:13:17,899 - Epoch [64/1000], Step [300/4367], Loss: 0.2969
2025-02-19 03:13:43,623 - Epoch [64/1000], Step [400/4367], Loss: 0.2294
2025-02-19 03:14:09,858 - Epoch [64/1000], Step [500/4367], Loss: 0.1761
2025-02-19 03:14:35,601 - Epoch [64/1000], Step [600/4367], Loss: 0.4084
2025-02-19 03:15:01,352 - Epoch [64/1000], Step [700/4367], Loss: 0.5300
2025-02-19 03:15:27,114 - Epoch [64/1000], Step [800/4367], Loss: 0.3891
2025-02-19 03:15:53,255 - Epoch [64/1000], Step [900/4367], Loss: 0.2364
2025-02-19 03:16:19,333 - Epoch [64/1000], Step [1000/4367], Loss: 0.6491
2025-02-19 03:16:44,897 - Epoch [64/1000], Step [1100/4367], Loss: 0.3317
2025-02-19 03:17:10,677 - Epoch [64/1000], Step [1200/4367], Loss: 0.4286
2025-02-19 03:17:36,229 - Epoch [64/1000], Step [1300/4367], Loss: 0.4665
2025-02-19 03:18:02,101 - Epoch [64/1000], Step [1400/4367], Loss: 0.6109
2025-02-19 03:18:27,670 - Epoch [64/1000], Step [1500/4367], Loss: 0.3075
2025-02-19 03:18:53,127 - Epoch [64/1000], Step [1600/4367], Loss: 0.3044
2025-02-19 03:19:18,820 - Epoch [64/1000], Step [1700/4367], Loss: 0.3465
2025-02-19 03:19:44,677 - Epoch [64/1000], Step [1800/4367], Loss: 0.4814
2025-02-19 03:20:10,582 - Epoch [64/1000], Step [1900/4367], Loss: 0.1931
2025-02-19 03:20:36,638 - Epoch [64/1000], Step [2000/4367], Loss: 0.2404
2025-02-19 03:21:02,578 - Epoch [64/1000], Step [2100/4367], Loss: 0.4240
2025-02-19 03:21:28,197 - Epoch [64/1000], Step [2200/4367], Loss: 0.3482
2025-02-19 03:21:54,490 - Epoch [64/1000], Step [2300/4367], Loss: 0.2062
2025-02-19 03:22:20,325 - Epoch [64/1000], Step [2400/4367], Loss: 0.1963
2025-02-19 03:22:46,273 - Epoch [64/1000], Step [2500/4367], Loss: 0.7166
2025-02-19 03:23:12,490 - Epoch [64/1000], Step [2600/4367], Loss: 0.4827
2025-02-19 03:23:38,298 - Epoch [64/1000], Step [2700/4367], Loss: 0.3004
2025-02-19 03:24:04,262 - Epoch [64/1000], Step [2800/4367], Loss: 0.1847
2025-02-19 03:24:30,143 - Epoch [64/1000], Step [2900/4367], Loss: 0.2519
2025-02-19 03:24:56,508 - Epoch [64/1000], Step [3000/4367], Loss: 0.3575
2025-02-19 03:25:22,197 - Epoch [64/1000], Step [3100/4367], Loss: 0.3071
2025-02-19 03:25:48,035 - Epoch [64/1000], Step [3200/4367], Loss: 0.5951
2025-02-19 03:26:14,115 - Epoch [64/1000], Step [3300/4367], Loss: 0.2617
2025-02-19 03:26:40,011 - Epoch [64/1000], Step [3400/4367], Loss: 0.4229
2025-02-19 03:27:05,803 - Epoch [64/1000], Step [3500/4367], Loss: 0.2375
2025-02-19 03:27:31,706 - Epoch [64/1000], Step [3600/4367], Loss: 0.2800
2025-02-19 03:27:57,766 - Epoch [64/1000], Step [3700/4367], Loss: 0.6404
2025-02-19 03:28:23,658 - Epoch [64/1000], Step [3800/4367], Loss: 0.4363
2025-02-19 03:28:49,888 - Epoch [64/1000], Step [3900/4367], Loss: 0.4377
2025-02-19 03:29:16,216 - Epoch [64/1000], Step [4000/4367], Loss: 0.3101
2025-02-19 03:29:42,116 - Epoch [64/1000], Step [4100/4367], Loss: 0.3829
2025-02-19 03:30:08,268 - Epoch [64/1000], Step [4200/4367], Loss: 0.2699
2025-02-19 03:30:33,952 - Epoch [64/1000], Step [4300/4367], Loss: 0.3387
2025-02-19 03:31:01,191 - Epoch [64/1000], Validation Step [100/1090], Val Loss: 0.0001
2025-02-19 03:31:10,009 - Epoch [64/1000], Validation Step [200/1090], Val Loss: 0.0000
2025-02-19 03:31:17,442 - Epoch [64/1000], Validation Step [300/1090], Val Loss: 0.6644
2025-02-19 03:31:25,148 - Epoch [64/1000], Validation Step [400/1090], Val Loss: 0.2109
2025-02-19 03:31:32,421 - Epoch [64/1000], Validation Step [500/1090], Val Loss: 0.4215
2025-02-19 03:31:40,006 - Epoch [64/1000], Validation Step [600/1090], Val Loss: 0.5493
2025-02-19 03:31:47,646 - Epoch [64/1000], Validation Step [700/1090], Val Loss: 0.3427
2025-02-19 03:31:54,693 - Epoch [64/1000], Validation Step [800/1090], Val Loss: 0.1227
2025-02-19 03:32:01,558 - Epoch [64/1000], Validation Step [900/1090], Val Loss: 0.1971
2025-02-19 03:32:08,953 - Epoch [64/1000], Validation Step [1000/1090], Val Loss: 0.0771
2025-02-19 03:32:15,868 - Epoch 64/1000, Train Loss: 0.3460, Val Loss: 0.3719, Accuracy: 86.53%
2025-02-19 03:32:16,419 - Model saved at /data2/personal/sungjin/korean_dialects/classification_second_checkpoint/checkpoint_epoch_64.pth
2025-02-19 03:32:44,912 - Epoch [65/1000], Step [100/4367], Loss: 0.1421
2025-02-19 03:33:11,075 - Epoch [65/1000], Step [200/4367], Loss: 0.1559
2025-02-19 03:33:36,647 - Epoch [65/1000], Step [300/4367], Loss: 0.2661
2025-02-19 03:34:02,261 - Epoch [65/1000], Step [400/4367], Loss: 0.2055
2025-02-19 03:34:28,036 - Epoch [65/1000], Step [500/4367], Loss: 0.3523
2025-02-19 03:34:54,096 - Epoch [65/1000], Step [600/4367], Loss: 0.6638
2025-02-19 03:35:20,016 - Epoch [65/1000], Step [700/4367], Loss: 0.5644
2025-02-19 03:35:46,043 - Epoch [65/1000], Step [800/4367], Loss: 0.3806
2025-02-19 03:36:11,958 - Epoch [65/1000], Step [900/4367], Loss: 0.4356
2025-02-19 03:36:37,936 - Epoch [65/1000], Step [1000/4367], Loss: 0.1528
2025-02-19 03:37:03,691 - Epoch [65/1000], Step [1100/4367], Loss: 0.2625
2025-02-19 03:37:29,469 - Epoch [65/1000], Step [1200/4367], Loss: 0.2569
2025-02-19 03:37:55,475 - Epoch [65/1000], Step [1300/4367], Loss: 0.3952
2025-02-19 03:38:21,512 - Epoch [65/1000], Step [1400/4367], Loss: 0.5131
2025-02-19 03:38:47,593 - Epoch [65/1000], Step [1500/4367], Loss: 0.1590
2025-02-19 03:39:13,914 - Epoch [65/1000], Step [1600/4367], Loss: 0.4151
2025-02-19 03:39:39,727 - Epoch [65/1000], Step [1700/4367], Loss: 0.3715
2025-02-19 03:40:06,111 - Epoch [65/1000], Step [1800/4367], Loss: 0.2577
2025-02-19 03:40:31,880 - Epoch [65/1000], Step [1900/4367], Loss: 0.3185
2025-02-19 03:40:58,183 - Epoch [65/1000], Step [2000/4367], Loss: 0.3346
2025-02-19 03:41:24,308 - Epoch [65/1000], Step [2100/4367], Loss: 0.5628
2025-02-19 03:41:50,017 - Epoch [65/1000], Step [2200/4367], Loss: 0.2296
2025-02-19 03:42:15,658 - Epoch [65/1000], Step [2300/4367], Loss: 0.1566
2025-02-19 03:42:41,455 - Epoch [65/1000], Step [2400/4367], Loss: 0.1248
2025-02-19 03:43:07,546 - Epoch [65/1000], Step [2500/4367], Loss: 0.3629
2025-02-19 03:43:33,448 - Epoch [65/1000], Step [2600/4367], Loss: 0.3695
2025-02-19 03:43:59,030 - Epoch [65/1000], Step [2700/4367], Loss: 0.2170
2025-02-19 03:44:25,183 - Epoch [65/1000], Step [2800/4367], Loss: 0.2213
2025-02-19 03:44:51,207 - Epoch [65/1000], Step [2900/4367], Loss: 0.6699
2025-02-19 03:45:17,143 - Epoch [65/1000], Step [3000/4367], Loss: 0.2182
2025-02-19 03:45:42,928 - Epoch [65/1000], Step [3100/4367], Loss: 0.2959
2025-02-19 03:46:08,835 - Epoch [65/1000], Step [3200/4367], Loss: 0.3716
2025-02-19 03:46:34,774 - Epoch [65/1000], Step [3300/4367], Loss: 0.1998
2025-02-19 03:47:00,688 - Epoch [65/1000], Step [3400/4367], Loss: 0.3197
2025-02-19 03:47:26,931 - Epoch [65/1000], Step [3500/4367], Loss: 0.4043
2025-02-19 03:47:52,976 - Epoch [65/1000], Step [3600/4367], Loss: 0.4926
2025-02-19 03:48:18,657 - Epoch [65/1000], Step [3700/4367], Loss: 0.5768
2025-02-19 03:48:44,693 - Epoch [65/1000], Step [3800/4367], Loss: 0.2838
2025-02-19 03:49:10,378 - Epoch [65/1000], Step [3900/4367], Loss: 0.2211
2025-02-19 03:49:36,633 - Epoch [65/1000], Step [4000/4367], Loss: 0.3219
2025-02-19 03:50:02,548 - Epoch [65/1000], Step [4100/4367], Loss: 0.1683
2025-02-19 03:50:28,178 - Epoch [65/1000], Step [4200/4367], Loss: 0.1537
2025-02-19 03:50:54,196 - Epoch [65/1000], Step [4300/4367], Loss: 0.2793
2025-02-19 03:51:21,262 - Epoch [65/1000], Validation Step [100/1090], Val Loss: 0.0000
2025-02-19 03:51:28,604 - Epoch [65/1000], Validation Step [200/1090], Val Loss: 0.0000
2025-02-19 03:51:36,061 - Epoch [65/1000], Validation Step [300/1090], Val Loss: 0.6402
2025-02-19 03:51:43,788 - Epoch [65/1000], Validation Step [400/1090], Val Loss: 0.2308
2025-02-19 03:51:51,040 - Epoch [65/1000], Validation Step [500/1090], Val Loss: 0.4342
2025-02-19 03:51:58,619 - Epoch [65/1000], Validation Step [600/1090], Val Loss: 0.5541
2025-02-19 03:52:06,232 - Epoch [65/1000], Validation Step [700/1090], Val Loss: 0.3444
2025-02-19 03:52:13,256 - Epoch [65/1000], Validation Step [800/1090], Val Loss: 0.1180
2025-02-19 03:52:20,084 - Epoch [65/1000], Validation Step [900/1090], Val Loss: 0.2007
2025-02-19 03:52:27,435 - Epoch [65/1000], Validation Step [1000/1090], Val Loss: 0.0765
2025-02-19 03:52:34,329 - Epoch 65/1000, Train Loss: 0.3461, Val Loss: 0.3712, Accuracy: 86.57%
2025-02-19 03:53:02,972 - Epoch [66/1000], Step [100/4367], Loss: 0.3251
2025-02-19 03:53:28,713 - Epoch [66/1000], Step [200/4367], Loss: 0.2652
2025-02-19 03:53:54,821 - Epoch [66/1000], Step [300/4367], Loss: 0.2665
2025-02-19 03:54:20,868 - Epoch [66/1000], Step [400/4367], Loss: 0.4890
2025-02-19 03:54:46,538 - Epoch [66/1000], Step [500/4367], Loss: 0.3859
2025-02-19 03:55:12,327 - Epoch [66/1000], Step [600/4367], Loss: 0.3238
2025-02-19 03:55:38,255 - Epoch [66/1000], Step [700/4367], Loss: 0.4273
2025-02-19 03:56:03,956 - Epoch [66/1000], Step [800/4367], Loss: 0.3112
2025-02-19 03:56:30,059 - Epoch [66/1000], Step [900/4367], Loss: 0.1974
2025-02-19 03:56:55,867 - Epoch [66/1000], Step [1000/4367], Loss: 0.3599
2025-02-19 03:57:21,680 - Epoch [66/1000], Step [1100/4367], Loss: 0.3913
2025-02-19 03:57:47,495 - Epoch [66/1000], Step [1200/4367], Loss: 0.3794
2025-02-19 03:58:13,606 - Epoch [66/1000], Step [1300/4367], Loss: 0.3278
2025-02-19 03:58:39,321 - Epoch [66/1000], Step [1400/4367], Loss: 0.5213
2025-02-19 03:59:05,019 - Epoch [66/1000], Step [1500/4367], Loss: 0.4014
2025-02-19 03:59:31,209 - Epoch [66/1000], Step [1600/4367], Loss: 0.4622
2025-02-19 03:59:57,034 - Epoch [66/1000], Step [1700/4367], Loss: 0.4226
2025-02-19 04:00:23,013 - Epoch [66/1000], Step [1800/4367], Loss: 0.2340
2025-02-19 04:00:48,932 - Epoch [66/1000], Step [1900/4367], Loss: 0.2687
2025-02-19 04:01:14,564 - Epoch [66/1000], Step [2000/4367], Loss: 0.4317
2025-02-19 04:01:40,288 - Epoch [66/1000], Step [2100/4367], Loss: 0.2999
2025-02-19 04:02:06,286 - Epoch [66/1000], Step [2200/4367], Loss: 0.3188
2025-02-19 04:02:32,174 - Epoch [66/1000], Step [2300/4367], Loss: 0.3414
2025-02-19 04:02:58,069 - Epoch [66/1000], Step [2400/4367], Loss: 0.5225
2025-02-19 04:03:23,891 - Epoch [66/1000], Step [2500/4367], Loss: 0.3301
2025-02-19 04:03:49,729 - Epoch [66/1000], Step [2600/4367], Loss: 0.3322
2025-02-19 04:04:15,637 - Epoch [66/1000], Step [2700/4367], Loss: 0.2263
2025-02-19 04:04:41,655 - Epoch [66/1000], Step [2800/4367], Loss: 0.2013
2025-02-19 04:05:07,326 - Epoch [66/1000], Step [2900/4367], Loss: 0.2538
2025-02-19 04:05:33,246 - Epoch [66/1000], Step [3000/4367], Loss: 0.4582
2025-02-19 04:05:59,063 - Epoch [66/1000], Step [3100/4367], Loss: 0.1963
2025-02-19 04:06:24,741 - Epoch [66/1000], Step [3200/4367], Loss: 0.2192
2025-02-19 04:06:50,772 - Epoch [66/1000], Step [3300/4367], Loss: 0.4486
2025-02-19 04:07:16,755 - Epoch [66/1000], Step [3400/4367], Loss: 0.2692
2025-02-19 04:07:42,421 - Epoch [66/1000], Step [3500/4367], Loss: 0.2007
2025-02-19 04:08:08,536 - Epoch [66/1000], Step [3600/4367], Loss: 0.2960
2025-02-19 04:08:34,497 - Epoch [66/1000], Step [3700/4367], Loss: 0.5664
2025-02-19 04:09:00,095 - Epoch [66/1000], Step [3800/4367], Loss: 0.4286
2025-02-19 04:09:25,952 - Epoch [66/1000], Step [3900/4367], Loss: 0.5132
2025-02-19 04:09:52,026 - Epoch [66/1000], Step [4000/4367], Loss: 0.3017
2025-02-19 04:10:17,755 - Epoch [66/1000], Step [4100/4367], Loss: 0.1962
2025-02-19 04:10:43,537 - Epoch [66/1000], Step [4200/4367], Loss: 0.4851
2025-02-19 04:11:09,196 - Epoch [66/1000], Step [4300/4367], Loss: 0.2300
2025-02-19 04:11:36,013 - Epoch [66/1000], Validation Step [100/1090], Val Loss: 0.0001
2025-02-19 04:11:43,318 - Epoch [66/1000], Validation Step [200/1090], Val Loss: 0.0000
2025-02-19 04:11:50,713 - Epoch [66/1000], Validation Step [300/1090], Val Loss: 0.6738
2025-02-19 04:11:58,336 - Epoch [66/1000], Validation Step [400/1090], Val Loss: 0.1968
2025-02-19 04:12:05,530 - Epoch [66/1000], Validation Step [500/1090], Val Loss: 0.4121
2025-02-19 04:12:13,044 - Epoch [66/1000], Validation Step [600/1090], Val Loss: 0.5479
2025-02-19 04:12:20,605 - Epoch [66/1000], Validation Step [700/1090], Val Loss: 0.3369
2025-02-19 04:12:27,566 - Epoch [66/1000], Validation Step [800/1090], Val Loss: 0.1269
2025-02-19 04:12:34,346 - Epoch [66/1000], Validation Step [900/1090], Val Loss: 0.2032
2025-02-19 04:12:41,672 - Epoch [66/1000], Validation Step [1000/1090], Val Loss: 0.0911
2025-02-19 04:12:48,522 - Epoch 66/1000, Train Loss: 0.3452, Val Loss: 0.3752, Accuracy: 86.38%
2025-02-19 04:32:30,696 - Model saved at /data2/personal/sungjin/korean_dialects/classification_second_checkpoint/checkpoint_epoch_66.pth
2025-02-19 04:32:58,826 - Epoch [67/1000], Step [100/4367], Loss: 0.5999
2025-02-19 04:33:24,557 - Epoch [67/1000], Step [200/4367], Loss: 0.4825
2025-02-19 04:33:50,627 - Epoch [67/1000], Step [300/4367], Loss: 0.2520
2025-02-19 04:34:16,662 - Epoch [67/1000], Step [400/4367], Loss: 0.3399
2025-02-19 04:34:42,787 - Epoch [67/1000], Step [500/4367], Loss: 0.2761
2025-02-19 04:35:09,364 - Epoch [67/1000], Step [600/4367], Loss: 0.3077
2025-02-19 04:35:35,645 - Epoch [67/1000], Step [700/4367], Loss: 0.1354
2025-02-19 04:36:01,707 - Epoch [67/1000], Step [800/4367], Loss: 0.4008
2025-02-19 04:36:28,189 - Epoch [67/1000], Step [900/4367], Loss: 0.3923
2025-02-19 04:36:54,578 - Epoch [67/1000], Step [1000/4367], Loss: 0.3996
2025-02-19 04:37:20,787 - Epoch [67/1000], Step [1100/4367], Loss: 0.2897
2025-02-19 04:37:47,264 - Epoch [67/1000], Step [1200/4367], Loss: 0.2315
2025-02-19 04:38:12,954 - Epoch [67/1000], Step [1300/4367], Loss: 0.2021
2025-02-19 04:38:39,046 - Epoch [67/1000], Step [1400/4367], Loss: 0.2427
2025-02-19 04:39:04,674 - Epoch [67/1000], Step [1500/4367], Loss: 0.1520
2025-02-19 04:39:31,314 - Epoch [67/1000], Step [1600/4367], Loss: 0.5218
2025-02-19 04:39:57,254 - Epoch [67/1000], Step [1700/4367], Loss: 0.4741
2025-02-19 04:40:23,695 - Epoch [67/1000], Step [1800/4367], Loss: 0.4636
2025-02-19 04:40:49,873 - Epoch [67/1000], Step [1900/4367], Loss: 0.3250
2025-02-19 04:41:15,618 - Epoch [67/1000], Step [2000/4367], Loss: 0.2534
2025-02-19 04:41:41,508 - Epoch [67/1000], Step [2100/4367], Loss: 0.2796
2025-02-19 04:42:08,012 - Epoch [67/1000], Step [2200/4367], Loss: 0.6345
2025-02-19 04:42:34,520 - Epoch [67/1000], Step [2300/4367], Loss: 0.3441
2025-02-19 04:43:00,801 - Epoch [67/1000], Step [2400/4367], Loss: 0.4603
2025-02-19 04:43:27,052 - Epoch [67/1000], Step [2500/4367], Loss: 0.2608
2025-02-19 04:43:53,437 - Epoch [67/1000], Step [2600/4367], Loss: 0.6807
2025-02-19 04:44:19,660 - Epoch [67/1000], Step [2700/4367], Loss: 0.4924
2025-02-19 04:44:45,793 - Epoch [67/1000], Step [2800/4367], Loss: 0.2598
2025-02-19 04:45:12,222 - Epoch [67/1000], Step [2900/4367], Loss: 0.5363
2025-02-19 04:45:38,343 - Epoch [67/1000], Step [3000/4367], Loss: 0.3491
2025-02-19 04:46:04,253 - Epoch [67/1000], Step [3100/4367], Loss: 0.1804
2025-02-19 04:46:30,402 - Epoch [67/1000], Step [3200/4367], Loss: 0.1673
2025-02-19 04:46:56,548 - Epoch [67/1000], Step [3300/4367], Loss: 0.2212
2025-02-19 04:47:22,942 - Epoch [67/1000], Step [3400/4367], Loss: 0.3066
2025-02-19 04:47:48,954 - Epoch [67/1000], Step [3500/4367], Loss: 0.4327
2025-02-19 04:48:15,230 - Epoch [67/1000], Step [3600/4367], Loss: 0.3186
2025-02-19 04:48:41,781 - Epoch [67/1000], Step [3700/4367], Loss: 0.4325
2025-02-19 04:49:08,194 - Epoch [67/1000], Step [3800/4367], Loss: 0.3742
2025-02-19 04:49:34,353 - Epoch [67/1000], Step [3900/4367], Loss: 0.1945
2025-02-19 04:50:00,697 - Epoch [67/1000], Step [4000/4367], Loss: 0.5274
2025-02-19 04:50:26,686 - Epoch [67/1000], Step [4100/4367], Loss: 0.5907
2025-02-19 04:50:53,107 - Epoch [67/1000], Step [4200/4367], Loss: 0.2100
2025-02-19 04:51:19,298 - Epoch [67/1000], Step [4300/4367], Loss: 0.2454
2025-02-19 04:51:46,932 - Epoch [67/1000], Validation Step [100/1090], Val Loss: 0.0000
2025-02-19 04:51:54,253 - Epoch [67/1000], Validation Step [200/1090], Val Loss: 0.0000
2025-02-19 04:52:01,631 - Epoch [67/1000], Validation Step [300/1090], Val Loss: 0.6617
2025-02-19 04:52:09,274 - Epoch [67/1000], Validation Step [400/1090], Val Loss: 0.2060
2025-02-19 04:52:16,481 - Epoch [67/1000], Validation Step [500/1090], Val Loss: 0.4135
2025-02-19 04:52:24,034 - Epoch [67/1000], Validation Step [600/1090], Val Loss: 0.5407
2025-02-19 04:52:31,627 - Epoch [67/1000], Validation Step [700/1090], Val Loss: 0.3283
2025-02-19 04:52:38,608 - Epoch [67/1000], Validation Step [800/1090], Val Loss: 0.1222
2025-02-19 04:52:45,411 - Epoch [67/1000], Validation Step [900/1090], Val Loss: 0.2133
2025-02-19 04:52:52,744 - Epoch [67/1000], Validation Step [1000/1090], Val Loss: 0.0696
2025-02-19 04:52:59,630 - Epoch 67/1000, Train Loss: 0.3459, Val Loss: 0.3718, Accuracy: 86.49%
2025-02-19 04:53:28,329 - Epoch [68/1000], Step [100/4367], Loss: 0.4224
2025-02-19 04:53:54,459 - Epoch [68/1000], Step [200/4367], Loss: 0.3186
2025-02-19 04:54:20,179 - Epoch [68/1000], Step [300/4367], Loss: 0.3222
2025-02-19 04:54:46,076 - Epoch [68/1000], Step [400/4367], Loss: 0.4068
2025-02-19 04:55:12,352 - Epoch [68/1000], Step [500/4367], Loss: 0.4125
2025-02-19 04:55:38,755 - Epoch [68/1000], Step [600/4367], Loss: 0.3427
2025-02-19 04:56:05,057 - Epoch [68/1000], Step [700/4367], Loss: 0.1693
2025-02-19 04:56:31,098 - Epoch [68/1000], Step [800/4367], Loss: 0.4383
2025-02-19 04:56:57,282 - Epoch [68/1000], Step [900/4367], Loss: 0.4164
2025-02-19 04:57:23,694 - Epoch [68/1000], Step [1000/4367], Loss: 0.3196
2025-02-19 04:57:50,345 - Epoch [68/1000], Step [1100/4367], Loss: 0.3714
2025-02-19 04:58:16,116 - Epoch [68/1000], Step [1200/4367], Loss: 0.2132
2025-02-19 04:58:42,467 - Epoch [68/1000], Step [1300/4367], Loss: 0.1788
2025-02-19 04:59:08,581 - Epoch [68/1000], Step [1400/4367], Loss: 0.3218
2025-02-19 04:59:34,727 - Epoch [68/1000], Step [1500/4367], Loss: 0.2267
2025-02-19 05:00:00,548 - Epoch [68/1000], Step [1600/4367], Loss: 0.4271
2025-02-19 05:00:26,761 - Epoch [68/1000], Step [1700/4367], Loss: 0.2847
2025-02-19 05:00:53,144 - Epoch [68/1000], Step [1800/4367], Loss: 0.5270
2025-02-19 05:01:19,455 - Epoch [68/1000], Step [1900/4367], Loss: 0.2456
2025-02-19 05:01:45,861 - Epoch [68/1000], Step [2000/4367], Loss: 0.4281
2025-02-19 05:02:11,932 - Epoch [68/1000], Step [2100/4367], Loss: 0.3725
2025-02-19 05:02:37,895 - Epoch [68/1000], Step [2200/4367], Loss: 0.4163
2025-02-19 05:03:04,181 - Epoch [68/1000], Step [2300/4367], Loss: 0.5661
2025-02-19 05:03:30,636 - Epoch [68/1000], Step [2400/4367], Loss: 0.1968
2025-02-19 05:03:56,648 - Epoch [68/1000], Step [2500/4367], Loss: 0.3208
2025-02-19 05:04:22,730 - Epoch [68/1000], Step [2600/4367], Loss: 0.1773
2025-02-19 05:04:49,246 - Epoch [68/1000], Step [2700/4367], Loss: 0.3867
2025-02-19 05:05:15,760 - Epoch [68/1000], Step [2800/4367], Loss: 0.1571
2025-02-19 05:05:42,371 - Epoch [68/1000], Step [2900/4367], Loss: 0.3403
2025-02-19 05:06:08,139 - Epoch [68/1000], Step [3000/4367], Loss: 0.4833
2025-02-19 05:06:34,718 - Epoch [68/1000], Step [3100/4367], Loss: 0.3837
2025-02-19 05:07:00,379 - Epoch [68/1000], Step [3200/4367], Loss: 0.1866
2025-02-19 05:07:26,641 - Epoch [68/1000], Step [3300/4367], Loss: 0.4488
2025-02-19 05:07:52,982 - Epoch [68/1000], Step [3400/4367], Loss: 0.3241
2025-02-19 05:08:19,438 - Epoch [68/1000], Step [3500/4367], Loss: 0.2665
2025-02-19 05:08:45,799 - Epoch [68/1000], Step [3600/4367], Loss: 0.3980
2025-02-19 05:09:11,746 - Epoch [68/1000], Step [3700/4367], Loss: 0.2212
2025-02-19 05:09:38,237 - Epoch [68/1000], Step [3800/4367], Loss: 0.3055
2025-02-19 05:10:04,739 - Epoch [68/1000], Step [3900/4367], Loss: 0.3284
2025-02-19 05:10:30,973 - Epoch [68/1000], Step [4000/4367], Loss: 0.2211
2025-02-19 05:10:57,401 - Epoch [68/1000], Step [4100/4367], Loss: 0.4004
2025-02-19 05:11:23,666 - Epoch [68/1000], Step [4200/4367], Loss: 0.3642
2025-02-19 05:11:49,982 - Epoch [68/1000], Step [4300/4367], Loss: 0.2519
2025-02-19 05:12:17,661 - Epoch [68/1000], Validation Step [100/1090], Val Loss: 0.0000
2025-02-19 05:12:25,360 - Epoch [68/1000], Validation Step [200/1090], Val Loss: 0.0000
2025-02-19 05:12:33,127 - Epoch [68/1000], Validation Step [300/1090], Val Loss: 0.6642
2025-02-19 05:12:41,176 - Epoch [68/1000], Validation Step [400/1090], Val Loss: 0.2114
2025-02-19 05:12:48,769 - Epoch [68/1000], Validation Step [500/1090], Val Loss: 0.3806
2025-02-19 05:12:56,706 - Epoch [68/1000], Validation Step [600/1090], Val Loss: 0.5706
2025-02-19 05:13:04,689 - Epoch [68/1000], Validation Step [700/1090], Val Loss: 0.3466
2025-02-19 05:13:12,065 - Epoch [68/1000], Validation Step [800/1090], Val Loss: 0.1346
2025-02-19 05:13:19,272 - Epoch [68/1000], Validation Step [900/1090], Val Loss: 0.2331
2025-02-19 05:13:27,009 - Epoch [68/1000], Validation Step [1000/1090], Val Loss: 0.0599
2025-02-19 05:13:34,310 - Epoch 68/1000, Train Loss: 0.3445, Val Loss: 0.3709, Accuracy: 86.49%
2025-02-19 05:37:46,739 - Model saved at /data2/personal/sungjin/korean_dialects/classification_second_checkpoint/checkpoint_epoch_68.pth
2025-02-19 05:38:15,391 - Epoch [69/1000], Step [100/4367], Loss: 0.2903
2025-02-19 05:38:41,465 - Epoch [69/1000], Step [200/4367], Loss: 0.3874
2025-02-19 05:39:07,633 - Epoch [69/1000], Step [300/4367], Loss: 0.2229
2025-02-19 05:39:33,389 - Epoch [69/1000], Step [400/4367], Loss: 0.3133
2025-02-19 05:39:59,380 - Epoch [69/1000], Step [500/4367], Loss: 0.2882
2025-02-19 05:40:25,345 - Epoch [69/1000], Step [600/4367], Loss: 0.2609
2025-02-19 05:40:51,594 - Epoch [69/1000], Step [700/4367], Loss: 0.4037
2025-02-19 05:41:17,817 - Epoch [69/1000], Step [800/4367], Loss: 0.3807
2025-02-19 05:41:43,797 - Epoch [69/1000], Step [900/4367], Loss: 0.3656
2025-02-19 05:42:09,199 - Epoch [69/1000], Step [1000/4367], Loss: 0.0880
2025-02-19 05:42:35,614 - Epoch [69/1000], Step [1100/4367], Loss: 0.3867
2025-02-19 05:43:01,466 - Epoch [69/1000], Step [1200/4367], Loss: 0.3875
2025-02-19 05:43:27,539 - Epoch [69/1000], Step [1300/4367], Loss: 0.3124
2025-02-19 05:43:53,901 - Epoch [69/1000], Step [1400/4367], Loss: 0.3819
2025-02-19 05:44:20,500 - Epoch [69/1000], Step [1500/4367], Loss: 0.3731
2025-02-19 05:44:46,632 - Epoch [69/1000], Step [1600/4367], Loss: 0.3053
2025-02-19 05:45:12,553 - Epoch [69/1000], Step [1700/4367], Loss: 0.4065
2025-02-19 05:45:38,903 - Epoch [69/1000], Step [1800/4367], Loss: 0.2937
2025-02-19 05:46:04,959 - Epoch [69/1000], Step [1900/4367], Loss: 0.2535
2025-02-19 05:46:31,003 - Epoch [69/1000], Step [2000/4367], Loss: 0.3841
2025-02-19 05:46:56,951 - Epoch [69/1000], Step [2100/4367], Loss: 0.2721
2025-02-19 05:47:23,459 - Epoch [69/1000], Step [2200/4367], Loss: 0.1347
2025-02-19 05:47:50,436 - Epoch [69/1000], Step [2300/4367], Loss: 0.4949
2025-02-19 05:48:16,950 - Epoch [69/1000], Step [2400/4367], Loss: 0.1796
2025-02-19 05:48:43,539 - Epoch [69/1000], Step [2500/4367], Loss: 0.3534
2025-02-19 05:49:10,246 - Epoch [69/1000], Step [2600/4367], Loss: 0.2080
2025-02-19 05:49:36,762 - Epoch [69/1000], Step [2700/4367], Loss: 0.3934
2025-02-19 05:50:03,187 - Epoch [69/1000], Step [2800/4367], Loss: 0.3928
2025-02-19 05:50:30,080 - Epoch [69/1000], Step [2900/4367], Loss: 0.1206
2025-02-19 05:50:56,499 - Epoch [69/1000], Step [3000/4367], Loss: 0.4323
2025-02-19 05:51:22,941 - Epoch [69/1000], Step [3100/4367], Loss: 0.2231
2025-02-19 05:51:49,100 - Epoch [69/1000], Step [3200/4367], Loss: 0.3173
2025-02-19 05:52:15,701 - Epoch [69/1000], Step [3300/4367], Loss: 0.3811
2025-02-19 05:52:42,286 - Epoch [69/1000], Step [3400/4367], Loss: 0.4883
2025-02-19 05:53:08,924 - Epoch [69/1000], Step [3500/4367], Loss: 0.4445
2025-02-19 05:53:35,427 - Epoch [69/1000], Step [3600/4367], Loss: 0.2776
2025-02-19 05:54:01,908 - Epoch [69/1000], Step [3700/4367], Loss: 0.1939
2025-02-19 05:54:28,001 - Epoch [69/1000], Step [3800/4367], Loss: 0.1244
2025-02-19 05:54:53,940 - Epoch [69/1000], Step [3900/4367], Loss: 0.5658
2025-02-19 05:55:20,448 - Epoch [69/1000], Step [4000/4367], Loss: 0.3077
2025-02-19 05:55:46,727 - Epoch [69/1000], Step [4100/4367], Loss: 0.2273
2025-02-19 05:56:13,448 - Epoch [69/1000], Step [4200/4367], Loss: 0.2461
2025-02-19 05:56:39,785 - Epoch [69/1000], Step [4300/4367], Loss: 0.4401
2025-02-19 05:57:08,892 - Epoch [69/1000], Validation Step [100/1090], Val Loss: 0.0000
2025-02-19 05:57:17,093 - Epoch [69/1000], Validation Step [200/1090], Val Loss: 0.0000
2025-02-19 05:57:25,473 - Epoch [69/1000], Validation Step [300/1090], Val Loss: 0.6972
2025-02-19 05:57:34,094 - Epoch [69/1000], Validation Step [400/1090], Val Loss: 0.1778
2025-02-19 05:57:42,278 - Epoch [69/1000], Validation Step [500/1090], Val Loss: 0.3406
2025-02-19 05:57:50,602 - Epoch [69/1000], Validation Step [600/1090], Val Loss: 0.5602
2025-02-19 05:57:59,064 - Epoch [69/1000], Validation Step [700/1090], Val Loss: 0.3583
2025-02-19 05:58:06,731 - Epoch [69/1000], Validation Step [800/1090], Val Loss: 0.1524
2025-02-19 05:58:14,380 - Epoch [69/1000], Validation Step [900/1090], Val Loss: 0.2471
2025-02-19 05:58:22,516 - Epoch [69/1000], Validation Step [1000/1090], Val Loss: 0.0692
2025-02-19 05:58:30,271 - Epoch 69/1000, Train Loss: 0.3451, Val Loss: 0.3771, Accuracy: 86.29%
2025-02-19 05:59:00,412 - Epoch [70/1000], Step [100/4367], Loss: 0.1674
2025-02-19 05:59:26,904 - Epoch [70/1000], Step [200/4367], Loss: 0.2244
2025-02-19 05:59:53,086 - Epoch [70/1000], Step [300/4367], Loss: 0.3802
2025-02-19 06:00:19,522 - Epoch [70/1000], Step [400/4367], Loss: 0.3952
2025-02-19 06:00:45,909 - Epoch [70/1000], Step [500/4367], Loss: 0.5252
2025-02-19 06:01:12,365 - Epoch [70/1000], Step [600/4367], Loss: 0.2428
2025-02-19 06:01:39,156 - Epoch [70/1000], Step [700/4367], Loss: 0.5931
2025-02-19 06:02:06,071 - Epoch [70/1000], Step [800/4367], Loss: 0.3294
2025-02-19 06:02:32,621 - Epoch [70/1000], Step [900/4367], Loss: 0.2705
2025-02-19 06:02:59,206 - Epoch [70/1000], Step [1000/4367], Loss: 0.1892
2025-02-19 06:03:25,728 - Epoch [70/1000], Step [1100/4367], Loss: 0.2919
2025-02-19 06:03:52,134 - Epoch [70/1000], Step [1200/4367], Loss: 0.4544
2025-02-19 06:04:18,554 - Epoch [70/1000], Step [1300/4367], Loss: 0.4193
2025-02-19 06:04:44,944 - Epoch [70/1000], Step [1400/4367], Loss: 0.2328
2025-02-19 06:05:11,838 - Epoch [70/1000], Step [1500/4367], Loss: 0.6199
2025-02-19 06:05:38,366 - Epoch [70/1000], Step [1600/4367], Loss: 0.4086
2025-02-19 06:06:05,213 - Epoch [70/1000], Step [1700/4367], Loss: 0.2811
2025-02-19 06:06:31,348 - Epoch [70/1000], Step [1800/4367], Loss: 0.5671
2025-02-19 06:06:57,798 - Epoch [70/1000], Step [1900/4367], Loss: 0.3556
2025-02-19 06:07:24,086 - Epoch [70/1000], Step [2000/4367], Loss: 0.3693
2025-02-19 06:07:51,011 - Epoch [70/1000], Step [2100/4367], Loss: 0.3971
2025-02-19 06:08:17,487 - Epoch [70/1000], Step [2200/4367], Loss: 0.4738
2025-02-19 06:08:43,851 - Epoch [70/1000], Step [2300/4367], Loss: 0.2687
2025-02-19 06:09:10,594 - Epoch [70/1000], Step [2400/4367], Loss: 0.3463
2025-02-19 06:09:37,092 - Epoch [70/1000], Step [2500/4367], Loss: 0.1560
2025-02-19 06:10:03,667 - Epoch [70/1000], Step [2600/4367], Loss: 0.3361
2025-02-19 06:10:30,573 - Epoch [70/1000], Step [2700/4367], Loss: 0.5140
2025-02-19 06:10:57,227 - Epoch [70/1000], Step [2800/4367], Loss: 0.5222
2025-02-19 06:11:23,733 - Epoch [70/1000], Step [2900/4367], Loss: 0.3992
2025-02-19 06:11:50,583 - Epoch [70/1000], Step [3000/4367], Loss: 0.5994
2025-02-19 06:12:17,370 - Epoch [70/1000], Step [3100/4367], Loss: 0.2611
2025-02-19 06:12:44,017 - Epoch [70/1000], Step [3200/4367], Loss: 0.3222
2025-02-19 06:13:10,558 - Epoch [70/1000], Step [3300/4367], Loss: 0.5123
2025-02-19 06:13:37,545 - Epoch [70/1000], Step [3400/4367], Loss: 0.4519
2025-02-19 06:14:04,003 - Epoch [70/1000], Step [3500/4367], Loss: 0.3822
2025-02-19 06:14:30,447 - Epoch [70/1000], Step [3600/4367], Loss: 0.3628
2025-02-19 06:14:57,223 - Epoch [70/1000], Step [3700/4367], Loss: 0.4866
2025-02-19 06:15:24,021 - Epoch [70/1000], Step [3800/4367], Loss: 0.3441
2025-02-19 06:15:50,691 - Epoch [70/1000], Step [3900/4367], Loss: 0.1645
2025-02-19 06:16:17,284 - Epoch [70/1000], Step [4000/4367], Loss: 0.4228
2025-02-19 06:16:43,985 - Epoch [70/1000], Step [4100/4367], Loss: 0.2669
2025-02-19 06:17:10,644 - Epoch [70/1000], Step [4200/4367], Loss: 0.3591
2025-02-19 06:17:37,377 - Epoch [70/1000], Step [4300/4367], Loss: 0.1295
2025-02-19 06:18:07,074 - Epoch [70/1000], Validation Step [100/1090], Val Loss: 0.0000
2025-02-19 06:18:15,443 - Epoch [70/1000], Validation Step [200/1090], Val Loss: 0.0000
2025-02-19 06:18:23,958 - Epoch [70/1000], Validation Step [300/1090], Val Loss: 0.6759
2025-02-19 06:18:32,460 - Epoch [70/1000], Validation Step [400/1090], Val Loss: 0.2644
2025-02-19 06:18:40,854 - Epoch [70/1000], Validation Step [500/1090], Val Loss: 0.4022
2025-02-19 06:18:49,205 - Epoch [70/1000], Validation Step [600/1090], Val Loss: 0.5263
2025-02-19 06:18:57,657 - Epoch [70/1000], Validation Step [700/1090], Val Loss: 0.3213
2025-02-19 06:19:05,464 - Epoch [70/1000], Validation Step [800/1090], Val Loss: 0.1367
2025-02-19 06:19:13,310 - Epoch [70/1000], Validation Step [900/1090], Val Loss: 0.2263
2025-02-19 06:19:22,093 - Epoch [70/1000], Validation Step [1000/1090], Val Loss: 0.0908
2025-02-19 06:19:29,577 - Epoch 70/1000, Train Loss: 0.3444, Val Loss: 0.3806, Accuracy: 86.19%
2025-02-19 08:06:58,633 - Model saved at /data2/personal/sungjin/korean_dialects/classification_second_checkpoint/checkpoint_epoch_70.pth
2025-02-19 08:07:28,011 - Epoch [71/1000], Step [100/4367], Loss: 0.1705
2025-02-19 08:07:54,188 - Epoch [71/1000], Step [200/4367], Loss: 0.2325
2025-02-19 08:08:20,699 - Epoch [71/1000], Step [300/4367], Loss: 0.3836
2025-02-19 08:08:47,011 - Epoch [71/1000], Step [400/4367], Loss: 0.4204
2025-02-19 08:09:13,655 - Epoch [71/1000], Step [500/4367], Loss: 0.2981
2025-02-19 08:09:39,857 - Epoch [71/1000], Step [600/4367], Loss: 0.2465
2025-02-19 08:10:07,102 - Epoch [71/1000], Step [700/4367], Loss: 0.4937
2025-02-19 08:10:33,258 - Epoch [71/1000], Step [800/4367], Loss: 0.4218
2025-02-19 08:10:59,679 - Epoch [71/1000], Step [900/4367], Loss: 0.7119
2025-02-19 08:11:26,189 - Epoch [71/1000], Step [1000/4367], Loss: 0.4378
2025-02-19 08:11:52,615 - Epoch [71/1000], Step [1100/4367], Loss: 0.2647
2025-02-19 08:12:19,082 - Epoch [71/1000], Step [1200/4367], Loss: 0.2680
2025-02-19 08:12:45,395 - Epoch [71/1000], Step [1300/4367], Loss: 0.4184
2025-02-19 08:13:11,564 - Epoch [71/1000], Step [1400/4367], Loss: 0.3730
2025-02-19 08:13:38,019 - Epoch [71/1000], Step [1500/4367], Loss: 0.3253
2025-02-19 08:14:04,208 - Epoch [71/1000], Step [1600/4367], Loss: 0.2430
2025-02-19 08:14:30,641 - Epoch [71/1000], Step [1700/4367], Loss: 0.4439
2025-02-19 08:14:56,857 - Epoch [71/1000], Step [1800/4367], Loss: 0.3187
2025-02-19 08:15:23,397 - Epoch [71/1000], Step [1900/4367], Loss: 0.2019
2025-02-19 08:15:49,667 - Epoch [71/1000], Step [2000/4367], Loss: 0.1456
2025-02-19 08:16:16,213 - Epoch [71/1000], Step [2100/4367], Loss: 0.3578
2025-02-19 08:16:42,343 - Epoch [71/1000], Step [2200/4367], Loss: 0.4974
2025-02-19 08:17:08,286 - Epoch [71/1000], Step [2300/4367], Loss: 0.4524
2025-02-19 08:17:34,801 - Epoch [71/1000], Step [2400/4367], Loss: 0.1796
2025-02-19 08:18:01,006 - Epoch [71/1000], Step [2500/4367], Loss: 0.2069
2025-02-19 08:18:27,617 - Epoch [71/1000], Step [2600/4367], Loss: 0.0992
2025-02-19 08:18:53,991 - Epoch [71/1000], Step [2700/4367], Loss: 0.3598
2025-02-19 08:19:20,383 - Epoch [71/1000], Step [2800/4367], Loss: 0.3498
2025-02-19 08:19:46,452 - Epoch [71/1000], Step [2900/4367], Loss: 0.4715
2025-02-19 08:20:12,845 - Epoch [71/1000], Step [3000/4367], Loss: 0.5735
2025-02-19 08:20:39,201 - Epoch [71/1000], Step [3100/4367], Loss: 0.2137
2025-02-19 08:21:05,289 - Epoch [71/1000], Step [3200/4367], Loss: 0.4237
2025-02-19 08:21:31,624 - Epoch [71/1000], Step [3300/4367], Loss: 0.7340
2025-02-19 08:21:58,270 - Epoch [71/1000], Step [3400/4367], Loss: 0.2962
2025-02-19 08:22:24,369 - Epoch [71/1000], Step [3500/4367], Loss: 0.1409
2025-02-19 08:22:50,725 - Epoch [71/1000], Step [3600/4367], Loss: 0.2251
2025-02-19 08:23:17,067 - Epoch [71/1000], Step [3700/4367], Loss: 0.2197
2025-02-19 08:23:43,771 - Epoch [71/1000], Step [3800/4367], Loss: 0.3746
2025-02-19 08:24:10,305 - Epoch [71/1000], Step [3900/4367], Loss: 0.1752
2025-02-19 08:24:36,569 - Epoch [71/1000], Step [4000/4367], Loss: 0.4871
2025-02-19 08:25:03,131 - Epoch [71/1000], Step [4100/4367], Loss: 0.6106
2025-02-19 08:25:29,552 - Epoch [71/1000], Step [4200/4367], Loss: 0.5408
2025-02-19 08:25:56,100 - Epoch [71/1000], Step [4300/4367], Loss: 0.3887
2025-02-19 08:26:25,402 - Epoch [71/1000], Validation Step [100/1090], Val Loss: 0.0000
2025-02-19 08:26:33,445 - Epoch [71/1000], Validation Step [200/1090], Val Loss: 0.0000
2025-02-19 08:26:41,436 - Epoch [71/1000], Validation Step [300/1090], Val Loss: 0.7159
2025-02-19 08:26:49,528 - Epoch [71/1000], Validation Step [400/1090], Val Loss: 0.1937
2025-02-19 08:26:57,666 - Epoch [71/1000], Validation Step [500/1090], Val Loss: 0.3872
2025-02-19 08:27:05,808 - Epoch [71/1000], Validation Step [600/1090], Val Loss: 0.5503
2025-02-19 08:27:13,804 - Epoch [71/1000], Validation Step [700/1090], Val Loss: 0.3460
2025-02-19 08:27:21,347 - Epoch [71/1000], Validation Step [800/1090], Val Loss: 0.1197
2025-02-19 08:27:28,601 - Epoch [71/1000], Validation Step [900/1090], Val Loss: 0.2147
2025-02-19 08:27:36,515 - Epoch [71/1000], Validation Step [1000/1090], Val Loss: 0.0597
2025-02-19 08:27:43,876 - Epoch 71/1000, Train Loss: 0.3442, Val Loss: 0.3682, Accuracy: 86.59%
2025-02-19 08:28:13,275 - Epoch [72/1000], Step [100/4367], Loss: 0.2127
2025-02-19 08:28:39,607 - Epoch [72/1000], Step [200/4367], Loss: 0.3803
2025-02-19 08:29:05,840 - Epoch [72/1000], Step [300/4367], Loss: 0.4123
2025-02-19 08:29:32,200 - Epoch [72/1000], Step [400/4367], Loss: 0.4095
2025-02-19 08:29:58,485 - Epoch [72/1000], Step [500/4367], Loss: 0.2662
2025-02-19 08:30:24,777 - Epoch [72/1000], Step [600/4367], Loss: 0.4862
2025-02-19 08:30:51,111 - Epoch [72/1000], Step [700/4367], Loss: 0.2584
2025-02-19 08:31:17,460 - Epoch [72/1000], Step [800/4367], Loss: 0.2781
2025-02-19 08:31:44,302 - Epoch [72/1000], Step [900/4367], Loss: 0.3469
2025-02-19 08:32:10,891 - Epoch [72/1000], Step [1000/4367], Loss: 0.3468
2025-02-19 08:32:37,585 - Epoch [72/1000], Step [1100/4367], Loss: 0.4941
2025-02-19 08:33:03,699 - Epoch [72/1000], Step [1200/4367], Loss: 0.1553
2025-02-19 08:33:29,884 - Epoch [72/1000], Step [1300/4367], Loss: 0.1096
2025-02-19 08:33:56,438 - Epoch [72/1000], Step [1400/4367], Loss: 0.1196
2025-02-19 08:34:22,586 - Epoch [72/1000], Step [1500/4367], Loss: 0.3544
2025-02-19 08:34:48,914 - Epoch [72/1000], Step [1600/4367], Loss: 0.4289
2025-02-19 08:35:15,071 - Epoch [72/1000], Step [1700/4367], Loss: 0.4021
2025-02-19 08:35:41,454 - Epoch [72/1000], Step [1800/4367], Loss: 0.2009
2025-02-19 08:36:07,850 - Epoch [72/1000], Step [1900/4367], Loss: 0.1928
2025-02-19 08:36:33,989 - Epoch [72/1000], Step [2000/4367], Loss: 0.5886
2025-02-19 08:37:00,120 - Epoch [72/1000], Step [2100/4367], Loss: 0.2608
2025-02-19 08:37:26,316 - Epoch [72/1000], Step [2200/4367], Loss: 0.2119
2025-02-19 08:37:52,954 - Epoch [72/1000], Step [2300/4367], Loss: 0.3345
2025-02-19 08:38:19,100 - Epoch [72/1000], Step [2400/4367], Loss: 0.2366
2025-02-19 08:38:45,766 - Epoch [72/1000], Step [2500/4367], Loss: 0.2593
2025-02-19 08:39:12,258 - Epoch [72/1000], Step [2600/4367], Loss: 0.3123
2025-02-19 08:39:38,697 - Epoch [72/1000], Step [2700/4367], Loss: 0.2829
2025-02-19 08:40:04,605 - Epoch [72/1000], Step [2800/4367], Loss: 0.1892
2025-02-19 08:40:30,871 - Epoch [72/1000], Step [2900/4367], Loss: 0.4595
2025-02-19 08:40:56,929 - Epoch [72/1000], Step [3000/4367], Loss: 0.1876
2025-02-19 08:41:22,828 - Epoch [72/1000], Step [3100/4367], Loss: 0.7376
2025-02-19 08:41:49,336 - Epoch [72/1000], Step [3200/4367], Loss: 0.2632
2025-02-19 08:42:15,844 - Epoch [72/1000], Step [3300/4367], Loss: 0.3973
2025-02-19 08:42:41,832 - Epoch [72/1000], Step [3400/4367], Loss: 0.4155
2025-02-19 08:43:08,284 - Epoch [72/1000], Step [3500/4367], Loss: 0.7719
2025-02-19 08:43:34,655 - Epoch [72/1000], Step [3600/4367], Loss: 0.2046
2025-02-19 08:44:01,373 - Epoch [72/1000], Step [3700/4367], Loss: 0.2556
2025-02-19 08:44:27,908 - Epoch [72/1000], Step [3800/4367], Loss: 0.2574
2025-02-19 08:44:53,881 - Epoch [72/1000], Step [3900/4367], Loss: 0.2862
2025-02-19 08:45:20,483 - Epoch [72/1000], Step [4000/4367], Loss: 0.2610
2025-02-19 08:45:46,520 - Epoch [72/1000], Step [4100/4367], Loss: 0.3352
2025-02-19 08:46:12,713 - Epoch [72/1000], Step [4200/4367], Loss: 0.2352
2025-02-19 08:46:38,897 - Epoch [72/1000], Step [4300/4367], Loss: 0.4103
2025-02-19 08:47:07,771 - Epoch [72/1000], Validation Step [100/1090], Val Loss: 0.0000
2025-02-19 08:47:15,428 - Epoch [72/1000], Validation Step [200/1090], Val Loss: 0.0000
2025-02-19 08:47:23,335 - Epoch [72/1000], Validation Step [300/1090], Val Loss: 0.6824
2025-02-19 08:47:31,141 - Epoch [72/1000], Validation Step [400/1090], Val Loss: 0.1972
2025-02-19 08:47:38,752 - Epoch [72/1000], Validation Step [500/1090], Val Loss: 0.3807
2025-02-19 08:47:46,584 - Epoch [72/1000], Validation Step [600/1090], Val Loss: 0.5750
2025-02-19 08:47:54,473 - Epoch [72/1000], Validation Step [700/1090], Val Loss: 0.3518
2025-02-19 08:48:01,877 - Epoch [72/1000], Validation Step [800/1090], Val Loss: 0.1227
2025-02-19 08:48:09,066 - Epoch [72/1000], Validation Step [900/1090], Val Loss: 0.2070
2025-02-19 08:48:16,592 - Epoch [72/1000], Validation Step [1000/1090], Val Loss: 0.0661
2025-02-19 08:48:23,749 - Epoch 72/1000, Train Loss: 0.3457, Val Loss: 0.3705, Accuracy: 86.49%
2025-02-19 09:05:55,190 - Model saved at /data2/personal/sungjin/korean_dialects/classification_second_checkpoint/checkpoint_epoch_72.pth
2025-02-19 09:06:25,929 - Epoch [73/1000], Step [100/4367], Loss: 0.4563
2025-02-19 09:06:52,259 - Epoch [73/1000], Step [200/4367], Loss: 0.1825
2025-02-19 09:07:18,725 - Epoch [73/1000], Step [300/4367], Loss: 0.1985
2025-02-19 09:07:45,068 - Epoch [73/1000], Step [400/4367], Loss: 0.5110
2025-02-19 09:08:11,457 - Epoch [73/1000], Step [500/4367], Loss: 0.3114
2025-02-19 09:08:37,613 - Epoch [73/1000], Step [600/4367], Loss: 0.3155
2025-02-19 09:09:04,473 - Epoch [73/1000], Step [700/4367], Loss: 0.3752
2025-02-19 09:09:30,917 - Epoch [73/1000], Step [800/4367], Loss: 0.3070
2025-02-19 09:09:57,545 - Epoch [73/1000], Step [900/4367], Loss: 0.3267
2025-02-19 09:10:24,227 - Epoch [73/1000], Step [1000/4367], Loss: 0.4240
2025-02-19 09:10:51,243 - Epoch [73/1000], Step [1100/4367], Loss: 0.2664
2025-02-19 09:11:18,121 - Epoch [73/1000], Step [1200/4367], Loss: 0.3060
2025-02-19 09:11:44,768 - Epoch [73/1000], Step [1300/4367], Loss: 0.1744
2025-02-19 09:12:11,184 - Epoch [73/1000], Step [1400/4367], Loss: 0.1836
2025-02-19 09:12:38,061 - Epoch [73/1000], Step [1500/4367], Loss: 0.2915
2025-02-19 09:13:04,390 - Epoch [73/1000], Step [1600/4367], Loss: 0.5480
2025-02-19 09:13:30,563 - Epoch [73/1000], Step [1700/4367], Loss: 0.2542
2025-02-19 09:13:57,548 - Epoch [73/1000], Step [1800/4367], Loss: 0.2300
2025-02-19 09:14:23,816 - Epoch [73/1000], Step [1900/4367], Loss: 0.3655
2025-02-19 09:14:50,571 - Epoch [73/1000], Step [2000/4367], Loss: 0.2478
2025-02-19 09:15:17,289 - Epoch [73/1000], Step [2100/4367], Loss: 0.2935
2025-02-19 09:15:44,075 - Epoch [73/1000], Step [2200/4367], Loss: 0.3312
2025-02-19 09:16:10,619 - Epoch [73/1000], Step [2300/4367], Loss: 0.3532
2025-02-19 09:16:37,392 - Epoch [73/1000], Step [2400/4367], Loss: 0.2787
2025-02-19 09:17:03,978 - Epoch [73/1000], Step [2500/4367], Loss: 0.1668
2025-02-19 09:17:30,243 - Epoch [73/1000], Step [2600/4367], Loss: 0.1964
2025-02-19 09:17:56,532 - Epoch [73/1000], Step [2700/4367], Loss: 0.2412
2025-02-19 09:18:22,746 - Epoch [73/1000], Step [2800/4367], Loss: 0.3088
2025-02-19 09:18:49,426 - Epoch [73/1000], Step [2900/4367], Loss: 0.6780
2025-02-19 09:19:16,109 - Epoch [73/1000], Step [3000/4367], Loss: 0.6048
2025-02-19 09:19:42,713 - Epoch [73/1000], Step [3100/4367], Loss: 0.2384
2025-02-19 09:20:09,128 - Epoch [73/1000], Step [3200/4367], Loss: 0.1158
2025-02-19 09:20:35,996 - Epoch [73/1000], Step [3300/4367], Loss: 0.3101
2025-02-19 09:21:02,692 - Epoch [73/1000], Step [3400/4367], Loss: 0.3464
2025-02-19 09:21:29,420 - Epoch [73/1000], Step [3500/4367], Loss: 0.3035
2025-02-19 09:21:56,378 - Epoch [73/1000], Step [3600/4367], Loss: 0.2701
2025-02-19 09:22:23,164 - Epoch [73/1000], Step [3700/4367], Loss: 0.2038
2025-02-19 09:22:50,189 - Epoch [73/1000], Step [3800/4367], Loss: 0.4731
2025-02-19 09:23:17,263 - Epoch [73/1000], Step [3900/4367], Loss: 0.2355
2025-02-19 09:23:43,634 - Epoch [73/1000], Step [4000/4367], Loss: 0.3300
2025-02-19 09:24:10,156 - Epoch [73/1000], Step [4100/4367], Loss: 0.1926
2025-02-19 09:24:36,685 - Epoch [73/1000], Step [4200/4367], Loss: 0.2694
2025-02-19 09:25:03,136 - Epoch [73/1000], Step [4300/4367], Loss: 0.4201
2025-02-19 09:25:32,170 - Epoch [73/1000], Validation Step [100/1090], Val Loss: 0.0001
2025-02-19 09:25:40,392 - Epoch [73/1000], Validation Step [200/1090], Val Loss: 0.0000
2025-02-19 09:25:48,645 - Epoch [73/1000], Validation Step [300/1090], Val Loss: 0.6745
2025-02-19 09:25:57,377 - Epoch [73/1000], Validation Step [400/1090], Val Loss: 0.1656
2025-02-19 09:26:05,793 - Epoch [73/1000], Validation Step [500/1090], Val Loss: 0.3653
2025-02-19 09:26:14,042 - Epoch [73/1000], Validation Step [600/1090], Val Loss: 0.5670
2025-02-19 09:26:22,484 - Epoch [73/1000], Validation Step [700/1090], Val Loss: 0.3599
2025-02-19 09:26:30,319 - Epoch [73/1000], Validation Step [800/1090], Val Loss: 0.1376
2025-02-19 09:26:38,173 - Epoch [73/1000], Validation Step [900/1090], Val Loss: 0.2163
2025-02-19 09:26:46,611 - Epoch [73/1000], Validation Step [1000/1090], Val Loss: 0.0687
2025-02-19 09:26:54,248 - Epoch 73/1000, Train Loss: 0.3453, Val Loss: 0.3739, Accuracy: 86.42%
2025-02-19 09:27:24,035 - Epoch [74/1000], Step [100/4367], Loss: 0.3775
2025-02-19 09:27:50,842 - Epoch [74/1000], Step [200/4367], Loss: 0.4441
2025-02-19 09:28:17,380 - Epoch [74/1000], Step [300/4367], Loss: 0.6140
2025-02-19 09:28:43,991 - Epoch [74/1000], Step [400/4367], Loss: 0.4010
2025-02-19 09:29:10,182 - Epoch [74/1000], Step [500/4367], Loss: 0.2156
2025-02-19 09:29:36,637 - Epoch [74/1000], Step [600/4367], Loss: 0.3772
2025-02-19 09:30:03,074 - Epoch [74/1000], Step [700/4367], Loss: 0.5757
2025-02-19 09:30:30,248 - Epoch [74/1000], Step [800/4367], Loss: 0.4547
2025-02-19 09:30:57,446 - Epoch [74/1000], Step [900/4367], Loss: 0.3012
2025-02-19 09:31:24,614 - Epoch [74/1000], Step [1000/4367], Loss: 0.2600
2025-02-19 09:31:51,630 - Epoch [74/1000], Step [1100/4367], Loss: 0.4186
2025-02-19 09:32:18,086 - Epoch [74/1000], Step [1200/4367], Loss: 0.3331
2025-02-19 09:32:44,265 - Epoch [74/1000], Step [1300/4367], Loss: 0.5811
2025-02-19 09:33:10,736 - Epoch [74/1000], Step [1400/4367], Loss: 0.2407
2025-02-19 09:33:37,163 - Epoch [74/1000], Step [1500/4367], Loss: 0.2186
2025-02-19 09:34:03,467 - Epoch [74/1000], Step [1600/4367], Loss: 0.2903
2025-02-19 09:34:30,216 - Epoch [74/1000], Step [1700/4367], Loss: 0.2837
2025-02-19 09:34:56,581 - Epoch [74/1000], Step [1800/4367], Loss: 0.4447
2025-02-19 09:35:23,555 - Epoch [74/1000], Step [1900/4367], Loss: 0.4092
2025-02-19 09:35:50,293 - Epoch [74/1000], Step [2000/4367], Loss: 0.6051
2025-02-19 09:36:17,058 - Epoch [74/1000], Step [2100/4367], Loss: 0.5043
2025-02-19 09:36:43,616 - Epoch [74/1000], Step [2200/4367], Loss: 0.4200
2025-02-19 09:37:10,475 - Epoch [74/1000], Step [2300/4367], Loss: 0.3233
2025-02-19 09:37:36,954 - Epoch [74/1000], Step [2400/4367], Loss: 0.4008
2025-02-19 09:38:03,880 - Epoch [74/1000], Step [2500/4367], Loss: 0.3039
2025-02-19 09:38:30,656 - Epoch [74/1000], Step [2600/4367], Loss: 0.3975
2025-02-19 09:38:57,879 - Epoch [74/1000], Step [2700/4367], Loss: 0.3361
2025-02-19 09:39:24,780 - Epoch [74/1000], Step [2800/4367], Loss: 0.3344
2025-02-19 09:39:52,180 - Epoch [74/1000], Step [2900/4367], Loss: 0.5986
2025-02-19 09:40:18,988 - Epoch [74/1000], Step [3000/4367], Loss: 0.3927
2025-02-19 09:40:46,507 - Epoch [74/1000], Step [3100/4367], Loss: 0.3550
2025-02-19 09:41:13,289 - Epoch [74/1000], Step [3200/4367], Loss: 0.5408
2025-02-19 09:41:40,158 - Epoch [74/1000], Step [3300/4367], Loss: 0.3124
2025-02-19 09:42:06,811 - Epoch [74/1000], Step [3400/4367], Loss: 0.6201
2025-02-19 09:42:33,785 - Epoch [74/1000], Step [3500/4367], Loss: 0.2110
2025-02-19 09:43:00,665 - Epoch [74/1000], Step [3600/4367], Loss: 0.2768
2025-02-19 09:43:27,976 - Epoch [74/1000], Step [3700/4367], Loss: 0.2544
2025-02-19 09:43:54,825 - Epoch [74/1000], Step [3800/4367], Loss: 0.2996
2025-02-19 09:44:21,650 - Epoch [74/1000], Step [3900/4367], Loss: 0.3573
2025-02-19 09:44:48,688 - Epoch [74/1000], Step [4000/4367], Loss: 0.3758
2025-02-19 09:45:16,161 - Epoch [74/1000], Step [4100/4367], Loss: 0.2567
2025-02-19 09:45:42,898 - Epoch [74/1000], Step [4200/4367], Loss: 0.5300
2025-02-19 09:46:09,750 - Epoch [74/1000], Step [4300/4367], Loss: 0.3255
2025-02-19 09:46:39,858 - Epoch [74/1000], Validation Step [100/1090], Val Loss: 0.0002
2025-02-19 09:46:48,127 - Epoch [74/1000], Validation Step [200/1090], Val Loss: 0.0000
2025-02-19 09:46:57,260 - Epoch [74/1000], Validation Step [300/1090], Val Loss: 0.6648
2025-02-19 09:47:06,216 - Epoch [74/1000], Validation Step [400/1090], Val Loss: 0.1912
2025-02-19 09:47:14,398 - Epoch [74/1000], Validation Step [500/1090], Val Loss: 0.4194
2025-02-19 09:47:22,856 - Epoch [74/1000], Validation Step [600/1090], Val Loss: 0.5604
2025-02-19 09:47:31,418 - Epoch [74/1000], Validation Step [700/1090], Val Loss: 0.3430
2025-02-19 09:47:39,348 - Epoch [74/1000], Validation Step [800/1090], Val Loss: 0.1114
2025-02-19 09:47:47,299 - Epoch [74/1000], Validation Step [900/1090], Val Loss: 0.1854
2025-02-19 09:47:55,757 - Epoch [74/1000], Validation Step [1000/1090], Val Loss: 0.0691
2025-02-19 09:48:03,242 - Epoch 74/1000, Train Loss: 0.3446, Val Loss: 0.3686, Accuracy: 86.67%
2025-02-19 09:48:04,041 - Model saved at /data2/personal/sungjin/korean_dialects/classification_second_checkpoint/checkpoint_epoch_74.pth
2025-02-19 09:48:35,036 - Epoch [75/1000], Step [100/4367], Loss: 0.4570
2025-02-19 09:49:02,112 - Epoch [75/1000], Step [200/4367], Loss: 0.3022
2025-02-19 09:49:29,174 - Epoch [75/1000], Step [300/4367], Loss: 0.3694
2025-02-19 09:49:56,092 - Epoch [75/1000], Step [400/4367], Loss: 0.2294
2025-02-19 09:50:23,239 - Epoch [75/1000], Step [500/4367], Loss: 0.2836
2025-02-19 09:50:50,015 - Epoch [75/1000], Step [600/4367], Loss: 0.2204
2025-02-19 09:51:17,535 - Epoch [75/1000], Step [700/4367], Loss: 0.3777
2025-02-19 09:51:44,937 - Epoch [75/1000], Step [800/4367], Loss: 0.2262
2025-02-19 09:52:12,064 - Epoch [75/1000], Step [900/4367], Loss: 0.2276
