import torch
import torchaudio
import os
import torch.nn.functional as F
import numpy as np
from conformer import CustomConformer2
import torchaudio.transforms as T

# ì„¤ì •
region = 'Jeju'
phoneme_dir = f'/data2/personal/sungjin/korean_standard/phoneme'
trainset_file = f'/data2/personal/sungjin/korean_dialects/modified/{region}/{region}_classification2_testset.txt'

# í…ŒìŠ¤íŠ¸í•  WAV íŒŒì¼ ëª©ë¡ ë¶ˆëŸ¬ì˜¤ê¸°
with open(trainset_file, 'r') as f:
    test_files = [os.path.join(phoneme_dir, line.strip().replace(".wav", ".npy")) for line in f.readlines() if line.strip().endswith(".wav")]

# ë””ë°”ì´ìŠ¤ ì„¤ì •
device = torch.device('cuda:11')

# ë ˆì´ë¸” ë§¤í•‘
DIALECT_LABELS = {
    0: 'standard',
    1: 'Chungcheong',
    2: 'Gangwon',
    3: 'Gyeongsang',
    4: 'Jeju',
    5: 'Jeolla'
}

# ë°˜ë³µì ìœ¼ë¡œ ì²´í¬í¬ì¸íŠ¸ ë³€ê²½í•˜ë©´ì„œ í‰ê°€
for epoch in range(6, 20, 2):  # 2, 4, 6, ..., 74 ì—í­ ë°˜ë³µ
    checkpoint_path = f'/data2/personal/sungjin/korean_dialects/classification_third_checkpoint/checkpoint_epoch_{epoch}.pth'
    
    # ëª¨ë¸ ë¡œë“œ
    num_classes = 6
    model = CustomConformer2(num_classes=num_classes).to(device)
    checkpoint = torch.load(checkpoint_path, map_location=device)
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()

    print(f"\nğŸ”¹ ì²´í¬í¬ì¸íŠ¸: Epoch {epoch} ì‹œì‘!")

    standard_count = 0
    for phoneme_path in test_files:
        wav_path = phoneme_path.replace("korean_standard/phoneme/", f"korean_dialects/modified/{region}/wav/").replace(".npy", ".wav")
        inputs = torch.from_numpy(np.load(phoneme_path)).unsqueeze(0)
        inputs = inputs.to(device)

        with torch.no_grad():
            output = model(inputs)
            probabilities = F.softmax(output, dim=1)
            probabilities_list = [f"{p:.2f}" for p in probabilities.squeeze(0).tolist()]
            predicted_label = torch.argmax(probabilities, dim=1).item()

        if DIALECT_LABELS[predicted_label] == region:
            standard_count += 1

        print(f"í™•ë¥ : {probabilities_list}, ì˜ˆì¸¡ëœ ë°©ì–¸: {DIALECT_LABELS[predicted_label]}, íŒŒì¼ : {wav_path}")

    # ì •ë‹µ ë¹„ìœ¨ ì¶œë ¥
    standard_ratio = standard_count / len(test_files) * 100
    print(f"âœ… Epoch {epoch}: ì •ë‹µ ë¹„ìœ¨: {standard_ratio:.2f}%")
