2025-02-14 07:18:55,008 - Epoch [1/1000], Step [100/4367], Loss: 1.4822
2025-02-14 07:19:29,623 - Epoch [1/1000], Step [200/4367], Loss: 1.0482
2025-02-14 07:20:04,759 - Epoch [1/1000], Step [300/4367], Loss: 0.7531
2025-02-14 07:20:40,031 - Epoch [1/1000], Step [400/4367], Loss: 0.7664
2025-02-14 07:21:14,768 - Epoch [1/1000], Step [500/4367], Loss: 0.6288
2025-02-14 07:21:49,459 - Epoch [1/1000], Step [600/4367], Loss: 0.7414
2025-02-14 07:22:24,100 - Epoch [1/1000], Step [700/4367], Loss: 0.5378
2025-02-14 07:22:58,818 - Epoch [1/1000], Step [800/4367], Loss: 0.6990
2025-02-14 07:23:33,771 - Epoch [1/1000], Step [900/4367], Loss: 0.6925
2025-02-14 07:24:08,880 - Epoch [1/1000], Step [1000/4367], Loss: 0.5868
2025-02-14 07:24:43,984 - Epoch [1/1000], Step [1100/4367], Loss: 0.4788
2025-02-14 07:25:18,524 - Epoch [1/1000], Step [1200/4367], Loss: 0.2685
2025-02-14 07:25:53,540 - Epoch [1/1000], Step [1300/4367], Loss: 0.5820
2025-02-14 07:26:28,205 - Epoch [1/1000], Step [1400/4367], Loss: 0.7743
2025-02-14 07:27:02,963 - Epoch [1/1000], Step [1500/4367], Loss: 0.5576
2025-02-14 07:27:38,136 - Epoch [1/1000], Step [1600/4367], Loss: 1.2279
2025-02-14 07:28:12,770 - Epoch [1/1000], Step [1700/4367], Loss: 0.7060
2025-02-14 07:28:47,411 - Epoch [1/1000], Step [1800/4367], Loss: 0.3491
2025-02-14 07:29:21,991 - Epoch [1/1000], Step [1900/4367], Loss: 0.5066
2025-02-14 07:29:57,244 - Epoch [1/1000], Step [2000/4367], Loss: 0.3607
2025-02-14 07:30:31,767 - Epoch [1/1000], Step [2100/4367], Loss: 0.5460
2025-02-14 07:31:06,706 - Epoch [1/1000], Step [2200/4367], Loss: 0.6728
2025-02-14 07:31:41,553 - Epoch [1/1000], Step [2300/4367], Loss: 0.6052
2025-02-14 07:32:16,100 - Epoch [1/1000], Step [2400/4367], Loss: 0.7354
2025-02-14 07:32:51,365 - Epoch [1/1000], Step [2500/4367], Loss: 0.3132
2025-02-14 07:33:26,316 - Epoch [1/1000], Step [2600/4367], Loss: 0.4512
2025-02-14 07:34:00,665 - Epoch [1/1000], Step [2700/4367], Loss: 0.3353
2025-02-14 07:34:34,970 - Epoch [1/1000], Step [2800/4367], Loss: 0.7099
2025-02-14 07:35:09,577 - Epoch [1/1000], Step [2900/4367], Loss: 0.5716
2025-02-14 07:35:44,756 - Epoch [1/1000], Step [3000/4367], Loss: 0.3293
2025-02-14 07:36:19,482 - Epoch [1/1000], Step [3100/4367], Loss: 0.2552
2025-02-14 07:36:54,194 - Epoch [1/1000], Step [3200/4367], Loss: 0.3389
2025-02-14 07:37:28,834 - Epoch [1/1000], Step [3300/4367], Loss: 0.3662
2025-02-14 07:38:03,210 - Epoch [1/1000], Step [3400/4367], Loss: 0.3085
2025-02-14 07:38:38,105 - Epoch [1/1000], Step [3500/4367], Loss: 0.5133
2025-02-14 07:39:12,929 - Epoch [1/1000], Step [3600/4367], Loss: 0.4614
2025-02-14 07:39:47,509 - Epoch [1/1000], Step [3700/4367], Loss: 0.7474
2025-02-14 07:40:22,474 - Epoch [1/1000], Step [3800/4367], Loss: 0.6837
2025-02-14 07:40:56,990 - Epoch [1/1000], Step [3900/4367], Loss: 0.6915
2025-02-14 07:41:31,802 - Epoch [1/1000], Step [4000/4367], Loss: 0.2713
2025-02-14 07:42:06,575 - Epoch [1/1000], Step [4100/4367], Loss: 0.5279
2025-02-14 07:42:41,360 - Epoch [1/1000], Step [4200/4367], Loss: 0.5624
2025-02-14 07:43:16,014 - Epoch [1/1000], Step [4300/4367], Loss: 0.3383
2025-02-14 07:43:48,988 - Epoch [1/1000], Validation Step [100/1090], Val Loss: 17.6249
2025-02-14 07:43:58,208 - Epoch [1/1000], Validation Step [200/1090], Val Loss: 18.5715
2025-02-14 07:44:07,570 - Epoch [1/1000], Validation Step [300/1090], Val Loss: 13.6040
2025-02-14 07:44:17,251 - Epoch [1/1000], Validation Step [400/1090], Val Loss: 19.2554
2025-02-14 07:44:26,386 - Epoch [1/1000], Validation Step [500/1090], Val Loss: 16.8500
2025-02-14 07:44:35,935 - Epoch [1/1000], Validation Step [600/1090], Val Loss: 25.0211
2025-02-14 07:44:45,527 - Epoch [1/1000], Validation Step [700/1090], Val Loss: 25.0854
2025-02-14 07:44:54,341 - Epoch [1/1000], Validation Step [800/1090], Val Loss: 4.9062
2025-02-14 07:45:02,931 - Epoch [1/1000], Validation Step [900/1090], Val Loss: 5.7617
2025-02-14 07:45:12,206 - Epoch [1/1000], Validation Step [1000/1090], Val Loss: 0.0000
2025-02-14 07:45:20,879 - Epoch 1/1000, Train Loss: 0.6028, Val Loss: 14.0849, Accuracy: 14.69%
2025-02-14 07:45:56,388 - Epoch [2/1000], Step [100/4367], Loss: 0.4567
2025-02-14 07:46:31,348 - Epoch [2/1000], Step [200/4367], Loss: 0.2911
2025-02-14 07:47:06,116 - Epoch [2/1000], Step [300/4367], Loss: 0.4222
2025-02-14 07:47:41,026 - Epoch [2/1000], Step [400/4367], Loss: 0.6283
2025-02-14 07:48:15,834 - Epoch [2/1000], Step [500/4367], Loss: 0.3264
2025-02-14 07:48:50,432 - Epoch [2/1000], Step [600/4367], Loss: 0.6799
2025-02-14 07:49:25,546 - Epoch [2/1000], Step [700/4367], Loss: 0.4029
2025-02-14 07:50:00,343 - Epoch [2/1000], Step [800/4367], Loss: 0.4511
2025-02-14 07:50:35,095 - Epoch [2/1000], Step [900/4367], Loss: 0.3767
2025-02-14 07:51:09,752 - Epoch [2/1000], Step [1000/4367], Loss: 0.3711
2025-02-14 07:51:44,545 - Epoch [2/1000], Step [1100/4367], Loss: 0.2150
2025-02-14 07:52:20,077 - Epoch [2/1000], Step [1200/4367], Loss: 0.5524
2025-02-14 07:52:55,643 - Epoch [2/1000], Step [1300/4367], Loss: 0.2218
2025-02-14 07:53:30,566 - Epoch [2/1000], Step [1400/4367], Loss: 0.4292
2025-02-14 07:54:05,322 - Epoch [2/1000], Step [1500/4367], Loss: 0.1382
2025-02-14 07:54:40,121 - Epoch [2/1000], Step [1600/4367], Loss: 0.4169
2025-02-14 07:55:15,000 - Epoch [2/1000], Step [1700/4367], Loss: 0.7099
2025-02-14 07:55:49,286 - Epoch [2/1000], Step [1800/4367], Loss: 0.3927
2025-02-14 07:56:23,994 - Epoch [2/1000], Step [1900/4367], Loss: 0.4269
2025-02-14 07:56:58,022 - Epoch [2/1000], Step [2000/4367], Loss: 0.3162
2025-02-14 07:57:32,767 - Epoch [2/1000], Step [2100/4367], Loss: 0.4568
2025-02-14 07:58:07,933 - Epoch [2/1000], Step [2200/4367], Loss: 0.4456
2025-02-14 07:58:42,524 - Epoch [2/1000], Step [2300/4367], Loss: 0.1299
2025-02-14 07:59:17,173 - Epoch [2/1000], Step [2400/4367], Loss: 0.1712
2025-02-14 07:59:51,555 - Epoch [2/1000], Step [2500/4367], Loss: 0.3035
2025-02-14 08:00:25,814 - Epoch [2/1000], Step [2600/4367], Loss: 0.2503
2025-02-14 08:01:00,647 - Epoch [2/1000], Step [2700/4367], Loss: 0.4967
2025-02-14 08:01:35,273 - Epoch [2/1000], Step [2800/4367], Loss: 0.5751
2025-02-14 08:02:10,052 - Epoch [2/1000], Step [2900/4367], Loss: 0.3034
2025-02-14 08:02:44,846 - Epoch [2/1000], Step [3000/4367], Loss: 0.1396
2025-02-14 08:03:19,959 - Epoch [2/1000], Step [3100/4367], Loss: 0.2211
2025-02-14 08:03:54,788 - Epoch [2/1000], Step [3200/4367], Loss: 0.2942
2025-02-14 08:04:29,257 - Epoch [2/1000], Step [3300/4367], Loss: 0.2622
2025-02-14 08:05:03,366 - Epoch [2/1000], Step [3400/4367], Loss: 0.2114
2025-02-14 08:05:37,884 - Epoch [2/1000], Step [3500/4367], Loss: 0.7372
2025-02-14 08:06:12,400 - Epoch [2/1000], Step [3600/4367], Loss: 0.3924
2025-02-14 08:06:47,190 - Epoch [2/1000], Step [3700/4367], Loss: 0.4281
2025-02-14 08:07:22,154 - Epoch [2/1000], Step [3800/4367], Loss: 0.3628
2025-02-14 08:07:56,789 - Epoch [2/1000], Step [3900/4367], Loss: 0.2887
2025-02-14 08:08:31,521 - Epoch [2/1000], Step [4000/4367], Loss: 0.2022
2025-02-14 08:09:06,632 - Epoch [2/1000], Step [4100/4367], Loss: 0.4739
2025-02-14 08:09:41,802 - Epoch [2/1000], Step [4200/4367], Loss: 0.2962
2025-02-14 08:10:16,639 - Epoch [2/1000], Step [4300/4367], Loss: 0.3177
2025-02-14 08:10:49,787 - Epoch [2/1000], Validation Step [100/1090], Val Loss: 7.0062
2025-02-14 08:10:59,036 - Epoch [2/1000], Validation Step [200/1090], Val Loss: 3.4568
2025-02-14 08:11:08,429 - Epoch [2/1000], Validation Step [300/1090], Val Loss: 2.5329
2025-02-14 08:11:18,138 - Epoch [2/1000], Validation Step [400/1090], Val Loss: 1.7843
2025-02-14 08:11:27,298 - Epoch [2/1000], Validation Step [500/1090], Val Loss: 1.6145
2025-02-14 08:11:36,867 - Epoch [2/1000], Validation Step [600/1090], Val Loss: 1.8561
2025-02-14 08:11:46,476 - Epoch [2/1000], Validation Step [700/1090], Val Loss: 2.4012
2025-02-14 08:11:55,298 - Epoch [2/1000], Validation Step [800/1090], Val Loss: 0.1164
2025-02-14 08:12:03,898 - Epoch [2/1000], Validation Step [900/1090], Val Loss: 0.1038
2025-02-14 08:12:13,196 - Epoch [2/1000], Validation Step [1000/1090], Val Loss: 0.0097
2025-02-14 08:12:21,872 - Epoch 2/1000, Train Loss: 0.3836, Val Loss: 2.6181, Accuracy: 43.72%
2025-02-14 08:12:22,333 - Model saved at /data2/personal/sungjin/korean_dialects/checkpoint_real/checkpoint_epoch_2.pth
2025-02-14 08:12:58,449 - Epoch [3/1000], Step [100/4367], Loss: 0.3828
2025-02-14 08:13:33,268 - Epoch [3/1000], Step [200/4367], Loss: 0.2184
2025-02-14 08:14:07,909 - Epoch [3/1000], Step [300/4367], Loss: 0.1897
2025-02-14 08:14:42,955 - Epoch [3/1000], Step [400/4367], Loss: 0.5168
2025-02-14 08:15:17,422 - Epoch [3/1000], Step [500/4367], Loss: 0.2600
2025-02-14 08:15:52,382 - Epoch [3/1000], Step [600/4367], Loss: 0.6091
2025-02-14 08:16:27,370 - Epoch [3/1000], Step [700/4367], Loss: 0.3383
2025-02-14 08:17:02,475 - Epoch [3/1000], Step [800/4367], Loss: 0.2626
2025-02-14 08:17:37,536 - Epoch [3/1000], Step [900/4367], Loss: 0.3114
2025-02-14 08:18:12,090 - Epoch [3/1000], Step [1000/4367], Loss: 0.2730
2025-02-14 08:18:47,031 - Epoch [3/1000], Step [1100/4367], Loss: 0.6613
2025-02-14 08:19:21,651 - Epoch [3/1000], Step [1200/4367], Loss: 0.1457
2025-02-14 08:19:56,784 - Epoch [3/1000], Step [1300/4367], Loss: 0.3564
2025-02-14 08:20:31,545 - Epoch [3/1000], Step [1400/4367], Loss: 0.3911
2025-02-14 08:21:06,354 - Epoch [3/1000], Step [1500/4367], Loss: 0.2674
2025-02-14 08:21:40,821 - Epoch [3/1000], Step [1600/4367], Loss: 0.3013
2025-02-14 08:22:15,402 - Epoch [3/1000], Step [1700/4367], Loss: 0.2040
2025-02-14 08:22:49,727 - Epoch [3/1000], Step [1800/4367], Loss: 0.4209
2025-02-14 08:23:24,608 - Epoch [3/1000], Step [1900/4367], Loss: 0.1624
2025-02-14 08:23:59,467 - Epoch [3/1000], Step [2000/4367], Loss: 0.2833
2025-02-14 08:24:34,168 - Epoch [3/1000], Step [2100/4367], Loss: 0.5752
2025-02-14 08:25:08,318 - Epoch [3/1000], Step [2200/4367], Loss: 0.4142
2025-02-14 08:25:43,057 - Epoch [3/1000], Step [2300/4367], Loss: 0.7111
2025-02-14 08:26:17,764 - Epoch [3/1000], Step [2400/4367], Loss: 0.2291
2025-02-14 08:26:52,358 - Epoch [3/1000], Step [2500/4367], Loss: 0.3209
2025-02-14 08:27:26,524 - Epoch [3/1000], Step [2600/4367], Loss: 0.5088
2025-02-14 08:28:01,544 - Epoch [3/1000], Step [2700/4367], Loss: 0.2821
2025-02-14 08:28:36,557 - Epoch [3/1000], Step [2800/4367], Loss: 0.2150
2025-02-14 08:29:11,391 - Epoch [3/1000], Step [2900/4367], Loss: 0.2865
2025-02-14 08:29:46,109 - Epoch [3/1000], Step [3000/4367], Loss: 0.3232
2025-02-14 08:30:20,932 - Epoch [3/1000], Step [3100/4367], Loss: 0.1107
2025-02-14 08:30:56,048 - Epoch [3/1000], Step [3200/4367], Loss: 0.2048
2025-02-14 08:31:31,305 - Epoch [3/1000], Step [3300/4367], Loss: 0.2834
2025-02-14 08:32:06,369 - Epoch [3/1000], Step [3400/4367], Loss: 0.1097
2025-02-14 08:32:41,886 - Epoch [3/1000], Step [3500/4367], Loss: 0.2670
2025-02-14 08:33:16,313 - Epoch [3/1000], Step [3600/4367], Loss: 0.2803
2025-02-14 08:33:51,182 - Epoch [3/1000], Step [3700/4367], Loss: 0.3556
2025-02-14 08:34:25,706 - Epoch [3/1000], Step [3800/4367], Loss: 0.3950
2025-02-14 08:35:00,468 - Epoch [3/1000], Step [3900/4367], Loss: 0.2872
2025-02-14 08:35:35,709 - Epoch [3/1000], Step [4000/4367], Loss: 0.2278
2025-02-14 08:36:10,762 - Epoch [3/1000], Step [4100/4367], Loss: 0.7117
2025-02-14 08:36:46,131 - Epoch [3/1000], Step [4200/4367], Loss: 0.2557
2025-02-14 08:37:20,511 - Epoch [3/1000], Step [4300/4367], Loss: 0.2709
2025-02-14 08:37:53,657 - Epoch [3/1000], Validation Step [100/1090], Val Loss: 2.0114
2025-02-14 08:38:02,871 - Epoch [3/1000], Validation Step [200/1090], Val Loss: 0.9710
2025-02-14 08:38:12,241 - Epoch [3/1000], Validation Step [300/1090], Val Loss: 0.4913
2025-02-14 08:38:21,925 - Epoch [3/1000], Validation Step [400/1090], Val Loss: 0.2218
2025-02-14 08:38:31,050 - Epoch [3/1000], Validation Step [500/1090], Val Loss: 0.4700
2025-02-14 08:38:40,599 - Epoch [3/1000], Validation Step [600/1090], Val Loss: 1.2431
2025-02-14 08:38:50,188 - Epoch [3/1000], Validation Step [700/1090], Val Loss: 0.8928
2025-02-14 08:38:58,984 - Epoch [3/1000], Validation Step [800/1090], Val Loss: 0.3540
2025-02-14 08:39:07,555 - Epoch [3/1000], Validation Step [900/1090], Val Loss: 0.3066
2025-02-14 08:39:16,825 - Epoch [3/1000], Validation Step [1000/1090], Val Loss: 7.5987
2025-02-14 08:39:25,441 - Epoch 3/1000, Train Loss: 0.3197, Val Loss: 2.1980, Accuracy: 52.21%
2025-02-14 08:40:00,953 - Epoch [4/1000], Step [100/4367], Loss: 0.2528
2025-02-14 08:40:35,792 - Epoch [4/1000], Step [200/4367], Loss: 0.2549
2025-02-14 08:41:10,366 - Epoch [4/1000], Step [300/4367], Loss: 0.1711
2025-02-14 08:41:45,065 - Epoch [4/1000], Step [400/4367], Loss: 0.3919
2025-02-14 08:42:19,524 - Epoch [4/1000], Step [500/4367], Loss: 0.2953
2025-02-14 08:42:54,327 - Epoch [4/1000], Step [600/4367], Loss: 0.2032
2025-02-14 08:43:29,278 - Epoch [4/1000], Step [700/4367], Loss: 0.2091
2025-02-14 08:44:03,988 - Epoch [4/1000], Step [800/4367], Loss: 0.1968
2025-02-14 08:44:38,403 - Epoch [4/1000], Step [900/4367], Loss: 0.3000
2025-02-14 08:45:13,228 - Epoch [4/1000], Step [1000/4367], Loss: 0.5661
2025-02-14 08:45:47,899 - Epoch [4/1000], Step [1100/4367], Loss: 0.1772
2025-02-14 08:46:22,945 - Epoch [4/1000], Step [1200/4367], Loss: 0.2197
2025-02-14 08:46:57,510 - Epoch [4/1000], Step [1300/4367], Loss: 0.1937
2025-02-14 08:47:32,181 - Epoch [4/1000], Step [1400/4367], Loss: 0.2378
2025-02-14 08:48:06,809 - Epoch [4/1000], Step [1500/4367], Loss: 0.3934
2025-02-14 08:48:41,485 - Epoch [4/1000], Step [1600/4367], Loss: 0.5704
2025-02-14 08:49:16,046 - Epoch [4/1000], Step [1700/4367], Loss: 0.2400
2025-02-14 08:49:50,805 - Epoch [4/1000], Step [1800/4367], Loss: 0.2343
2025-02-14 08:50:25,327 - Epoch [4/1000], Step [1900/4367], Loss: 0.3846
2025-02-14 08:51:00,335 - Epoch [4/1000], Step [2000/4367], Loss: 0.2667
2025-02-14 08:51:35,215 - Epoch [4/1000], Step [2100/4367], Loss: 0.4047
2025-02-14 08:52:10,015 - Epoch [4/1000], Step [2200/4367], Loss: 0.0911
2025-02-14 08:52:45,014 - Epoch [4/1000], Step [2300/4367], Loss: 0.2135
2025-02-14 08:53:20,179 - Epoch [4/1000], Step [2400/4367], Loss: 0.2453
2025-02-14 08:53:54,747 - Epoch [4/1000], Step [2500/4367], Loss: 0.3484
2025-02-14 08:54:29,398 - Epoch [4/1000], Step [2600/4367], Loss: 0.1354
2025-02-14 08:55:03,978 - Epoch [4/1000], Step [2700/4367], Loss: 0.4400
2025-02-14 08:55:38,555 - Epoch [4/1000], Step [2800/4367], Loss: 0.0883
2025-02-14 08:56:13,547 - Epoch [4/1000], Step [2900/4367], Loss: 0.2736
2025-02-14 08:56:48,381 - Epoch [4/1000], Step [3000/4367], Loss: 0.6160
2025-02-14 08:57:23,337 - Epoch [4/1000], Step [3100/4367], Loss: 0.2534
2025-02-14 08:57:58,148 - Epoch [4/1000], Step [3200/4367], Loss: 0.3326
2025-02-14 08:58:32,592 - Epoch [4/1000], Step [3300/4367], Loss: 0.5601
2025-02-14 08:59:07,612 - Epoch [4/1000], Step [3400/4367], Loss: 0.2130
2025-02-14 08:59:42,402 - Epoch [4/1000], Step [3500/4367], Loss: 0.2270
2025-02-14 09:00:16,820 - Epoch [4/1000], Step [3600/4367], Loss: 0.3443
2025-02-14 09:00:51,532 - Epoch [4/1000], Step [3700/4367], Loss: 0.1854
2025-02-14 09:01:26,403 - Epoch [4/1000], Step [3800/4367], Loss: 0.1126
2025-02-14 09:02:01,387 - Epoch [4/1000], Step [3900/4367], Loss: 0.1229
2025-02-14 09:02:36,310 - Epoch [4/1000], Step [4000/4367], Loss: 0.2164
2025-02-14 09:03:11,235 - Epoch [4/1000], Step [4100/4367], Loss: 0.1163
2025-02-14 09:03:45,919 - Epoch [4/1000], Step [4200/4367], Loss: 0.2363
2025-02-14 09:04:20,852 - Epoch [4/1000], Step [4300/4367], Loss: 0.4573
2025-02-14 09:04:53,827 - Epoch [4/1000], Validation Step [100/1090], Val Loss: 7.5264
2025-02-14 09:05:03,049 - Epoch [4/1000], Validation Step [200/1090], Val Loss: 6.8677
2025-02-14 09:05:12,413 - Epoch [4/1000], Validation Step [300/1090], Val Loss: 0.2667
2025-02-14 09:05:22,105 - Epoch [4/1000], Validation Step [400/1090], Val Loss: 0.6288
2025-02-14 09:05:31,244 - Epoch [4/1000], Validation Step [500/1090], Val Loss: 2.0739
2025-02-14 09:05:40,798 - Epoch [4/1000], Validation Step [600/1090], Val Loss: 5.3680
2025-02-14 09:05:50,390 - Epoch [4/1000], Validation Step [700/1090], Val Loss: 5.3438
2025-02-14 09:05:59,198 - Epoch [4/1000], Validation Step [800/1090], Val Loss: 0.0017
2025-02-14 09:06:07,788 - Epoch [4/1000], Validation Step [900/1090], Val Loss: 0.0041
2025-02-14 09:06:17,063 - Epoch [4/1000], Validation Step [1000/1090], Val Loss: 8.1384
2025-02-14 09:06:25,718 - Epoch 4/1000, Train Loss: 0.2834, Val Loss: 4.2146, Accuracy: 35.34%
2025-02-14 09:06:26,162 - Model saved at /data2/personal/sungjin/korean_dialects/checkpoint_real/checkpoint_epoch_4.pth
2025-02-14 09:07:02,288 - Epoch [5/1000], Step [100/4367], Loss: 0.3242
2025-02-14 09:07:37,174 - Epoch [5/1000], Step [200/4367], Loss: 0.3078
2025-02-14 09:08:11,794 - Epoch [5/1000], Step [300/4367], Loss: 0.2311
2025-02-14 09:08:46,401 - Epoch [5/1000], Step [400/4367], Loss: 0.2167
2025-02-14 09:09:21,329 - Epoch [5/1000], Step [500/4367], Loss: 0.4124
2025-02-14 09:09:56,501 - Epoch [5/1000], Step [600/4367], Loss: 0.0688
2025-02-14 09:10:31,227 - Epoch [5/1000], Step [700/4367], Loss: 0.1199
2025-02-14 09:11:06,373 - Epoch [5/1000], Step [800/4367], Loss: 0.1944
2025-02-14 09:11:41,063 - Epoch [5/1000], Step [900/4367], Loss: 0.2780
2025-02-14 09:12:16,158 - Epoch [5/1000], Step [1000/4367], Loss: 0.1387
2025-02-14 09:12:50,260 - Epoch [5/1000], Step [1100/4367], Loss: 0.3325
2025-02-14 09:13:24,796 - Epoch [5/1000], Step [1200/4367], Loss: 0.2178
2025-02-14 09:13:59,360 - Epoch [5/1000], Step [1300/4367], Loss: 0.1353
2025-02-14 09:14:33,818 - Epoch [5/1000], Step [1400/4367], Loss: 0.3807
2025-02-14 09:15:08,563 - Epoch [5/1000], Step [1500/4367], Loss: 0.3785
2025-02-14 09:15:42,962 - Epoch [5/1000], Step [1600/4367], Loss: 0.1369
2025-02-14 09:16:17,738 - Epoch [5/1000], Step [1700/4367], Loss: 0.2108
2025-02-14 09:16:52,587 - Epoch [5/1000], Step [1800/4367], Loss: 0.2870
2025-02-14 09:17:26,922 - Epoch [5/1000], Step [1900/4367], Loss: 0.1684
2025-02-14 09:18:01,298 - Epoch [5/1000], Step [2000/4367], Loss: 0.1697
2025-02-14 09:18:35,831 - Epoch [5/1000], Step [2100/4367], Loss: 0.4265
2025-02-14 09:19:10,137 - Epoch [5/1000], Step [2200/4367], Loss: 0.2100
2025-02-14 09:19:45,052 - Epoch [5/1000], Step [2300/4367], Loss: 0.2466
2025-02-14 09:20:19,409 - Epoch [5/1000], Step [2400/4367], Loss: 0.3688
2025-02-14 09:20:54,153 - Epoch [5/1000], Step [2500/4367], Loss: 0.3299
2025-02-14 09:21:28,806 - Epoch [5/1000], Step [2600/4367], Loss: 0.1016
2025-02-14 09:22:03,303 - Epoch [5/1000], Step [2700/4367], Loss: 0.1069
2025-02-14 09:22:37,528 - Epoch [5/1000], Step [2800/4367], Loss: 0.3349
2025-02-14 09:23:12,302 - Epoch [5/1000], Step [2900/4367], Loss: 0.3804
2025-02-14 09:23:47,133 - Epoch [5/1000], Step [3000/4367], Loss: 0.2376
2025-02-14 09:24:21,927 - Epoch [5/1000], Step [3100/4367], Loss: 0.2344
2025-02-14 09:24:56,425 - Epoch [5/1000], Step [3200/4367], Loss: 0.3860
2025-02-14 09:25:30,670 - Epoch [5/1000], Step [3300/4367], Loss: 0.0906
2025-02-14 09:26:05,065 - Epoch [5/1000], Step [3400/4367], Loss: 0.3617
2025-02-14 09:26:40,020 - Epoch [5/1000], Step [3500/4367], Loss: 0.2432
2025-02-14 09:27:14,520 - Epoch [5/1000], Step [3600/4367], Loss: 0.3211
2025-02-14 09:27:49,389 - Epoch [5/1000], Step [3700/4367], Loss: 0.3826
2025-02-14 09:28:24,204 - Epoch [5/1000], Step [3800/4367], Loss: 0.0673
2025-02-14 09:28:58,846 - Epoch [5/1000], Step [3900/4367], Loss: 0.7210
2025-02-14 09:29:33,780 - Epoch [5/1000], Step [4000/4367], Loss: 0.1471
2025-02-14 09:30:08,575 - Epoch [5/1000], Step [4100/4367], Loss: 0.4279
2025-02-14 09:30:43,229 - Epoch [5/1000], Step [4200/4367], Loss: 0.1035
2025-02-14 09:31:17,807 - Epoch [5/1000], Step [4300/4367], Loss: 0.2077
2025-02-14 09:31:50,798 - Epoch [5/1000], Validation Step [100/1090], Val Loss: 1.3185
2025-02-14 09:31:59,992 - Epoch [5/1000], Validation Step [200/1090], Val Loss: 0.3931
2025-02-14 09:32:09,319 - Epoch [5/1000], Validation Step [300/1090], Val Loss: 0.1903
2025-02-14 09:32:18,975 - Epoch [5/1000], Validation Step [400/1090], Val Loss: 0.9176
2025-02-14 09:32:28,071 - Epoch [5/1000], Validation Step [500/1090], Val Loss: 0.8200
2025-02-14 09:32:37,596 - Epoch [5/1000], Validation Step [600/1090], Val Loss: 0.5570
2025-02-14 09:32:47,154 - Epoch [5/1000], Validation Step [700/1090], Val Loss: 0.5699
2025-02-14 09:32:55,939 - Epoch [5/1000], Validation Step [800/1090], Val Loss: 0.0844
2025-02-14 09:33:04,496 - Epoch [5/1000], Validation Step [900/1090], Val Loss: 0.5795
2025-02-14 09:33:13,746 - Epoch [5/1000], Validation Step [1000/1090], Val Loss: 1.4943
2025-02-14 09:33:22,366 - Epoch 5/1000, Train Loss: 0.2649, Val Loss: 1.1285, Accuracy: 64.46%
2025-02-14 09:33:58,433 - Epoch [6/1000], Step [100/4367], Loss: 0.3030
2025-02-14 09:34:32,849 - Epoch [6/1000], Step [200/4367], Loss: 0.5135
2025-02-14 09:35:07,150 - Epoch [6/1000], Step [300/4367], Loss: 0.1784
2025-02-14 09:35:41,593 - Epoch [6/1000], Step [400/4367], Loss: 0.2983
2025-02-14 09:36:16,295 - Epoch [6/1000], Step [500/4367], Loss: 0.4756
2025-02-14 09:36:50,791 - Epoch [6/1000], Step [600/4367], Loss: 0.1899
2025-02-14 09:37:25,273 - Epoch [6/1000], Step [700/4367], Loss: 0.1381
2025-02-14 09:38:00,148 - Epoch [6/1000], Step [800/4367], Loss: 0.2518
2025-02-14 09:38:34,613 - Epoch [6/1000], Step [900/4367], Loss: 0.2188
2025-02-14 09:39:09,172 - Epoch [6/1000], Step [1000/4367], Loss: 0.1948
2025-02-14 09:39:43,528 - Epoch [6/1000], Step [1100/4367], Loss: 0.2678
2025-02-14 09:40:18,248 - Epoch [6/1000], Step [1200/4367], Loss: 0.1813
2025-02-14 09:40:53,081 - Epoch [6/1000], Step [1300/4367], Loss: 0.2623
2025-02-14 09:41:27,565 - Epoch [6/1000], Step [1400/4367], Loss: 0.1835
2025-02-14 09:42:01,839 - Epoch [6/1000], Step [1500/4367], Loss: 0.2172
2025-02-14 09:42:36,389 - Epoch [6/1000], Step [1600/4367], Loss: 0.2045
2025-02-14 09:43:10,846 - Epoch [6/1000], Step [1700/4367], Loss: 0.1398
2025-02-14 09:43:44,923 - Epoch [6/1000], Step [1800/4367], Loss: 0.3452
2025-02-14 09:44:19,562 - Epoch [6/1000], Step [1900/4367], Loss: 0.0623
2025-02-14 09:44:54,073 - Epoch [6/1000], Step [2000/4367], Loss: 0.0641
2025-02-14 09:45:28,522 - Epoch [6/1000], Step [2100/4367], Loss: 0.2826
2025-02-14 09:46:03,583 - Epoch [6/1000], Step [2200/4367], Loss: 0.2396
2025-02-14 09:46:38,188 - Epoch [6/1000], Step [2300/4367], Loss: 0.1513
2025-02-14 09:47:13,187 - Epoch [6/1000], Step [2400/4367], Loss: 0.0833
2025-02-14 09:47:48,095 - Epoch [6/1000], Step [2500/4367], Loss: 0.2569
2025-02-14 09:48:22,668 - Epoch [6/1000], Step [2600/4367], Loss: 0.1452
2025-02-14 09:48:57,351 - Epoch [6/1000], Step [2700/4367], Loss: 0.1962
2025-02-14 09:49:32,432 - Epoch [6/1000], Step [2800/4367], Loss: 0.1765
2025-02-14 09:50:07,259 - Epoch [6/1000], Step [2900/4367], Loss: 0.1005
2025-02-14 09:50:41,896 - Epoch [6/1000], Step [3000/4367], Loss: 0.0954
2025-02-14 09:51:16,777 - Epoch [6/1000], Step [3100/4367], Loss: 0.1202
2025-02-14 09:51:51,485 - Epoch [6/1000], Step [3200/4367], Loss: 0.5085
2025-02-14 09:52:26,095 - Epoch [6/1000], Step [3300/4367], Loss: 0.1468
2025-02-14 09:53:00,872 - Epoch [6/1000], Step [3400/4367], Loss: 0.2648
2025-02-14 09:53:35,959 - Epoch [6/1000], Step [3500/4367], Loss: 0.2740
2025-02-14 09:54:10,775 - Epoch [6/1000], Step [3600/4367], Loss: 0.2414
2025-02-14 09:54:45,450 - Epoch [6/1000], Step [3700/4367], Loss: 0.0845
2025-02-14 09:55:20,307 - Epoch [6/1000], Step [3800/4367], Loss: 0.3433
2025-02-14 09:55:54,369 - Epoch [6/1000], Step [3900/4367], Loss: 0.2212
2025-02-14 09:56:29,270 - Epoch [6/1000], Step [4000/4367], Loss: 0.3490
2025-02-14 09:57:04,214 - Epoch [6/1000], Step [4100/4367], Loss: 0.4262
2025-02-14 09:57:39,064 - Epoch [6/1000], Step [4200/4367], Loss: 0.1391
2025-02-14 09:58:14,300 - Epoch [6/1000], Step [4300/4367], Loss: 0.1386
2025-02-14 09:58:46,837 - Epoch [6/1000], Validation Step [100/1090], Val Loss: 4.5690
2025-02-14 09:58:56,041 - Epoch [6/1000], Validation Step [200/1090], Val Loss: 5.3029
2025-02-14 09:59:05,387 - Epoch [6/1000], Validation Step [300/1090], Val Loss: 0.0824
2025-02-14 09:59:15,053 - Epoch [6/1000], Validation Step [400/1090], Val Loss: 3.9863
2025-02-14 09:59:24,161 - Epoch [6/1000], Validation Step [500/1090], Val Loss: 1.9266
2025-02-14 09:59:33,676 - Epoch [6/1000], Validation Step [600/1090], Val Loss: 1.4292
2025-02-14 09:59:43,244 - Epoch [6/1000], Validation Step [700/1090], Val Loss: 1.8399
2025-02-14 09:59:52,026 - Epoch [6/1000], Validation Step [800/1090], Val Loss: 1.1270
2025-02-14 10:00:00,577 - Epoch [6/1000], Validation Step [900/1090], Val Loss: 2.5949
2025-02-14 10:00:09,837 - Epoch [6/1000], Validation Step [1000/1090], Val Loss: 0.0009
2025-02-14 10:00:18,467 - Epoch 6/1000, Train Loss: 0.2408, Val Loss: 2.6938, Accuracy: 47.42%
2025-02-14 10:00:18,858 - Model saved at /data2/personal/sungjin/korean_dialects/checkpoint_real/checkpoint_epoch_6.pth
2025-02-14 10:00:55,006 - Epoch [7/1000], Step [100/4367], Loss: 0.1461
2025-02-14 10:01:29,469 - Epoch [7/1000], Step [200/4367], Loss: 0.2126
2025-02-14 10:02:03,945 - Epoch [7/1000], Step [300/4367], Loss: 0.0728
2025-02-14 10:02:38,910 - Epoch [7/1000], Step [400/4367], Loss: 0.0492
2025-02-14 10:03:13,637 - Epoch [7/1000], Step [500/4367], Loss: 0.1538
2025-02-14 10:03:48,767 - Epoch [7/1000], Step [600/4367], Loss: 0.2717
2025-02-14 10:04:23,764 - Epoch [7/1000], Step [700/4367], Loss: 0.3840
2025-02-14 10:04:58,437 - Epoch [7/1000], Step [800/4367], Loss: 0.3143
2025-02-14 10:05:33,180 - Epoch [7/1000], Step [900/4367], Loss: 0.1886
2025-02-14 10:06:08,028 - Epoch [7/1000], Step [1000/4367], Loss: 0.3647
2025-02-14 10:06:42,498 - Epoch [7/1000], Step [1100/4367], Loss: 0.4303
2025-02-14 10:07:17,253 - Epoch [7/1000], Step [1200/4367], Loss: 0.3751
2025-02-14 10:07:51,696 - Epoch [7/1000], Step [1300/4367], Loss: 0.3234
2025-02-14 10:08:26,362 - Epoch [7/1000], Step [1400/4367], Loss: 0.3014
2025-02-14 10:09:01,321 - Epoch [7/1000], Step [1500/4367], Loss: 0.3054
2025-02-14 10:09:36,267 - Epoch [7/1000], Step [1600/4367], Loss: 0.2945
2025-02-14 10:10:10,782 - Epoch [7/1000], Step [1700/4367], Loss: 0.2978
2025-02-14 10:10:45,989 - Epoch [7/1000], Step [1800/4367], Loss: 0.5084
2025-02-14 10:11:20,790 - Epoch [7/1000], Step [1900/4367], Loss: 0.1860
2025-02-14 10:11:55,668 - Epoch [7/1000], Step [2000/4367], Loss: 0.2188
2025-02-14 10:12:30,114 - Epoch [7/1000], Step [2100/4367], Loss: 0.2471
2025-02-14 10:13:04,901 - Epoch [7/1000], Step [2200/4367], Loss: 0.1347
2025-02-14 10:13:39,656 - Epoch [7/1000], Step [2300/4367], Loss: 0.2617
2025-02-14 10:14:14,186 - Epoch [7/1000], Step [2400/4367], Loss: 0.2321
2025-02-14 10:14:48,884 - Epoch [7/1000], Step [2500/4367], Loss: 0.2609
2025-02-14 10:15:23,251 - Epoch [7/1000], Step [2600/4367], Loss: 0.3430
2025-02-14 10:15:58,339 - Epoch [7/1000], Step [2700/4367], Loss: 0.1125
2025-02-14 10:16:32,870 - Epoch [7/1000], Step [2800/4367], Loss: 0.2994
2025-02-14 10:17:07,776 - Epoch [7/1000], Step [2900/4367], Loss: 0.2246
2025-02-14 10:17:42,276 - Epoch [7/1000], Step [3000/4367], Loss: 0.4344
2025-02-14 10:18:16,827 - Epoch [7/1000], Step [3100/4367], Loss: 0.1070
2025-02-14 10:18:51,338 - Epoch [7/1000], Step [3200/4367], Loss: 0.1536
2025-02-14 10:19:26,297 - Epoch [7/1000], Step [3300/4367], Loss: 0.4280
2025-02-14 10:20:00,802 - Epoch [7/1000], Step [3400/4367], Loss: 0.2749
2025-02-14 10:20:35,640 - Epoch [7/1000], Step [3500/4367], Loss: 0.1546
2025-02-14 10:21:10,000 - Epoch [7/1000], Step [3600/4367], Loss: 0.3375
2025-02-14 10:21:44,672 - Epoch [7/1000], Step [3700/4367], Loss: 0.1807
2025-02-14 10:22:18,972 - Epoch [7/1000], Step [3800/4367], Loss: 0.1231
2025-02-14 10:22:53,470 - Epoch [7/1000], Step [3900/4367], Loss: 0.2403
2025-02-14 10:23:28,353 - Epoch [7/1000], Step [4000/4367], Loss: 0.1978
2025-02-14 10:24:03,323 - Epoch [7/1000], Step [4100/4367], Loss: 0.1372
2025-02-14 10:24:37,869 - Epoch [7/1000], Step [4200/4367], Loss: 0.0416
2025-02-14 10:25:12,660 - Epoch [7/1000], Step [4300/4367], Loss: 0.1614
2025-02-14 10:25:45,328 - Epoch [7/1000], Validation Step [100/1090], Val Loss: 0.0001
2025-02-14 10:25:54,539 - Epoch [7/1000], Validation Step [200/1090], Val Loss: 0.0000
2025-02-14 10:26:03,889 - Epoch [7/1000], Validation Step [300/1090], Val Loss: 0.6093
2025-02-14 10:26:13,553 - Epoch [7/1000], Validation Step [400/1090], Val Loss: 0.1404
2025-02-14 10:26:22,647 - Epoch [7/1000], Validation Step [500/1090], Val Loss: 1.1862
2025-02-14 10:26:32,179 - Epoch [7/1000], Validation Step [600/1090], Val Loss: 0.1827
2025-02-14 10:26:41,743 - Epoch [7/1000], Validation Step [700/1090], Val Loss: 0.3657
2025-02-14 10:26:50,524 - Epoch [7/1000], Validation Step [800/1090], Val Loss: 0.0041
2025-02-14 10:26:59,072 - Epoch [7/1000], Validation Step [900/1090], Val Loss: 0.0317
2025-02-14 10:27:08,324 - Epoch [7/1000], Validation Step [1000/1090], Val Loss: 0.1074
2025-02-14 10:27:16,937 - Epoch 7/1000, Train Loss: 0.2323, Val Loss: 0.2799, Accuracy: 90.18%
2025-02-14 10:27:52,692 - Epoch [8/1000], Step [100/4367], Loss: 0.0824
2025-02-14 10:28:27,687 - Epoch [8/1000], Step [200/4367], Loss: 0.1826
2025-02-14 10:29:02,552 - Epoch [8/1000], Step [300/4367], Loss: 0.1109
2025-02-14 10:29:36,774 - Epoch [8/1000], Step [400/4367], Loss: 0.2162
2025-02-14 10:30:11,409 - Epoch [8/1000], Step [500/4367], Loss: 0.2937
2025-02-14 10:30:45,724 - Epoch [8/1000], Step [600/4367], Loss: 0.3933
2025-02-14 10:31:20,478 - Epoch [8/1000], Step [700/4367], Loss: 0.5179
2025-02-14 10:31:55,115 - Epoch [8/1000], Step [800/4367], Loss: 0.0960
2025-02-14 10:32:29,579 - Epoch [8/1000], Step [900/4367], Loss: 0.4845
2025-02-14 10:33:04,480 - Epoch [8/1000], Step [1000/4367], Loss: 0.2291
2025-02-14 10:33:39,080 - Epoch [8/1000], Step [1100/4367], Loss: 0.1929
2025-02-14 10:34:14,188 - Epoch [8/1000], Step [1200/4367], Loss: 0.1443
2025-02-14 10:34:48,996 - Epoch [8/1000], Step [1300/4367], Loss: 0.1911
2025-02-14 10:35:23,753 - Epoch [8/1000], Step [1400/4367], Loss: 0.1446
2025-02-14 10:35:58,602 - Epoch [8/1000], Step [1500/4367], Loss: 0.1161
2025-02-14 10:36:33,270 - Epoch [8/1000], Step [1600/4367], Loss: 0.1262
2025-02-14 10:37:07,722 - Epoch [8/1000], Step [1700/4367], Loss: 0.3209
2025-02-14 10:37:42,384 - Epoch [8/1000], Step [1800/4367], Loss: 0.4239
2025-02-14 10:38:16,716 - Epoch [8/1000], Step [1900/4367], Loss: 0.1312
2025-02-14 10:38:51,541 - Epoch [8/1000], Step [2000/4367], Loss: 0.0885
2025-02-14 10:39:26,463 - Epoch [8/1000], Step [2100/4367], Loss: 0.2425
2025-02-14 10:40:01,068 - Epoch [8/1000], Step [2200/4367], Loss: 0.3478
2025-02-14 10:40:35,675 - Epoch [8/1000], Step [2300/4367], Loss: 0.6287
2025-02-14 10:41:10,650 - Epoch [8/1000], Step [2400/4367], Loss: 0.1626
2025-02-14 10:41:44,943 - Epoch [8/1000], Step [2500/4367], Loss: 0.0971
2025-02-14 10:42:18,953 - Epoch [8/1000], Step [2600/4367], Loss: 0.1496
2025-02-14 10:42:54,025 - Epoch [8/1000], Step [2700/4367], Loss: 0.1629
2025-02-14 10:43:28,609 - Epoch [8/1000], Step [2800/4367], Loss: 0.3256
2025-02-14 10:44:02,773 - Epoch [8/1000], Step [2900/4367], Loss: 0.2596
2025-02-14 10:44:37,451 - Epoch [8/1000], Step [3000/4367], Loss: 0.2938
2025-02-14 10:45:12,196 - Epoch [8/1000], Step [3100/4367], Loss: 0.0730
2025-02-14 10:45:46,974 - Epoch [8/1000], Step [3200/4367], Loss: 0.3547
2025-02-14 10:46:21,340 - Epoch [8/1000], Step [3300/4367], Loss: 0.1783
2025-02-14 10:46:56,190 - Epoch [8/1000], Step [3400/4367], Loss: 0.1606
2025-02-14 10:47:31,333 - Epoch [8/1000], Step [3500/4367], Loss: 0.1716
2025-02-14 10:48:06,300 - Epoch [8/1000], Step [3600/4367], Loss: 0.0964
2025-02-14 10:48:41,020 - Epoch [8/1000], Step [3700/4367], Loss: 0.0473
2025-02-14 10:49:15,383 - Epoch [8/1000], Step [3800/4367], Loss: 0.1321
2025-02-14 10:49:50,440 - Epoch [8/1000], Step [3900/4367], Loss: 0.0824
2025-02-14 10:50:24,791 - Epoch [8/1000], Step [4000/4367], Loss: 0.3987
2025-02-14 10:50:59,983 - Epoch [8/1000], Step [4100/4367], Loss: 0.1601
2025-02-14 10:51:35,029 - Epoch [8/1000], Step [4200/4367], Loss: 0.2368
2025-02-14 10:52:09,585 - Epoch [8/1000], Step [4300/4367], Loss: 0.1915
2025-02-14 10:52:42,670 - Epoch [8/1000], Validation Step [100/1090], Val Loss: 0.0001
2025-02-14 10:52:51,885 - Epoch [8/1000], Validation Step [200/1090], Val Loss: 0.0000
2025-02-14 10:53:01,237 - Epoch [8/1000], Validation Step [300/1090], Val Loss: 0.8976
2025-02-14 10:53:10,900 - Epoch [8/1000], Validation Step [400/1090], Val Loss: 0.2590
2025-02-14 10:53:20,008 - Epoch [8/1000], Validation Step [500/1090], Val Loss: 0.5593
2025-02-14 10:53:29,539 - Epoch [8/1000], Validation Step [600/1090], Val Loss: 0.3246
2025-02-14 10:53:39,101 - Epoch [8/1000], Validation Step [700/1090], Val Loss: 0.0801
2025-02-14 10:53:47,891 - Epoch [8/1000], Validation Step [800/1090], Val Loss: 0.0151
2025-02-14 10:53:56,452 - Epoch [8/1000], Validation Step [900/1090], Val Loss: 0.1074
2025-02-14 10:54:05,703 - Epoch [8/1000], Validation Step [1000/1090], Val Loss: 0.0004
2025-02-14 10:54:14,329 - Epoch 8/1000, Train Loss: 0.2205, Val Loss: 0.2837, Accuracy: 89.94%
2025-02-14 10:54:14,741 - Model saved at /data2/personal/sungjin/korean_dialects/checkpoint_real/checkpoint_epoch_8.pth
2025-02-14 10:54:50,041 - Epoch [9/1000], Step [100/4367], Loss: 0.2377
2025-02-14 10:55:24,744 - Epoch [9/1000], Step [200/4367], Loss: 0.1022
2025-02-14 10:55:59,379 - Epoch [9/1000], Step [300/4367], Loss: 0.1684
2025-02-14 10:56:34,132 - Epoch [9/1000], Step [400/4367], Loss: 0.1531
2025-02-14 10:57:08,838 - Epoch [9/1000], Step [500/4367], Loss: 0.0811
2025-02-14 10:57:43,766 - Epoch [9/1000], Step [600/4367], Loss: 0.1399
2025-02-14 10:58:18,144 - Epoch [9/1000], Step [700/4367], Loss: 0.2827
2025-02-14 10:58:52,374 - Epoch [9/1000], Step [800/4367], Loss: 0.0739
2025-02-14 10:59:27,189 - Epoch [9/1000], Step [900/4367], Loss: 0.5161
2025-02-14 11:00:01,898 - Epoch [9/1000], Step [1000/4367], Loss: 0.2575
2025-02-14 11:00:36,260 - Epoch [9/1000], Step [1100/4367], Loss: 0.2515
2025-02-14 11:01:11,210 - Epoch [9/1000], Step [1200/4367], Loss: 0.2120
2025-02-14 11:01:45,904 - Epoch [9/1000], Step [1300/4367], Loss: 0.1931
2025-02-14 11:02:19,904 - Epoch [9/1000], Step [1400/4367], Loss: 0.1259
2025-02-14 11:02:54,813 - Epoch [9/1000], Step [1500/4367], Loss: 0.1754
2025-02-14 11:03:30,009 - Epoch [9/1000], Step [1600/4367], Loss: 0.2347
2025-02-14 11:04:04,782 - Epoch [9/1000], Step [1700/4367], Loss: 0.3306
2025-02-14 11:04:39,720 - Epoch [9/1000], Step [1800/4367], Loss: 0.1585
2025-02-14 11:05:14,068 - Epoch [9/1000], Step [1900/4367], Loss: 0.1251
2025-02-14 11:05:49,066 - Epoch [9/1000], Step [2000/4367], Loss: 0.0507
2025-02-14 11:06:23,762 - Epoch [9/1000], Step [2100/4367], Loss: 0.2125
2025-02-14 11:06:58,187 - Epoch [9/1000], Step [2200/4367], Loss: 0.0849
2025-02-14 11:07:33,189 - Epoch [9/1000], Step [2300/4367], Loss: 0.4600
2025-02-14 11:08:07,902 - Epoch [9/1000], Step [2400/4367], Loss: 0.1301
2025-02-14 11:08:42,582 - Epoch [9/1000], Step [2500/4367], Loss: 0.3304
2025-02-14 11:09:17,355 - Epoch [9/1000], Step [2600/4367], Loss: 0.0838
2025-02-14 11:09:51,980 - Epoch [9/1000], Step [2700/4367], Loss: 0.4686
2025-02-14 11:10:26,620 - Epoch [9/1000], Step [2800/4367], Loss: 0.0880
2025-02-14 11:11:01,075 - Epoch [9/1000], Step [2900/4367], Loss: 0.2666
2025-02-14 11:11:36,095 - Epoch [9/1000], Step [3000/4367], Loss: 0.2341
2025-02-14 11:12:11,113 - Epoch [9/1000], Step [3100/4367], Loss: 0.2075
2025-02-14 11:12:45,368 - Epoch [9/1000], Step [3200/4367], Loss: 0.2794
2025-02-14 11:13:19,844 - Epoch [9/1000], Step [3300/4367], Loss: 0.3082
2025-02-14 11:13:54,741 - Epoch [9/1000], Step [3400/4367], Loss: 0.1364
2025-02-14 11:14:29,252 - Epoch [9/1000], Step [3500/4367], Loss: 0.4776
2025-02-14 11:15:04,386 - Epoch [9/1000], Step [3600/4367], Loss: 0.1081
2025-02-14 11:15:38,789 - Epoch [9/1000], Step [3700/4367], Loss: 0.1212
2025-02-14 11:16:13,587 - Epoch [9/1000], Step [3800/4367], Loss: 0.1631
2025-02-14 11:16:48,189 - Epoch [9/1000], Step [3900/4367], Loss: 0.0530
2025-02-14 11:17:23,179 - Epoch [9/1000], Step [4000/4367], Loss: 0.2648
2025-02-14 11:17:57,622 - Epoch [9/1000], Step [4100/4367], Loss: 0.3631
2025-02-14 11:18:32,230 - Epoch [9/1000], Step [4200/4367], Loss: 0.1182
2025-02-14 11:19:06,823 - Epoch [9/1000], Step [4300/4367], Loss: 0.1660
2025-02-14 11:19:39,671 - Epoch [9/1000], Validation Step [100/1090], Val Loss: 0.1692
2025-02-14 11:19:48,854 - Epoch [9/1000], Validation Step [200/1090], Val Loss: 0.2649
2025-02-14 11:19:58,180 - Epoch [9/1000], Validation Step [300/1090], Val Loss: 0.4666
2025-02-14 11:20:07,828 - Epoch [9/1000], Validation Step [400/1090], Val Loss: 1.7931
2025-02-14 11:20:16,920 - Epoch [9/1000], Validation Step [500/1090], Val Loss: 2.3833
2025-02-14 11:20:26,431 - Epoch [9/1000], Validation Step [600/1090], Val Loss: 0.4629
2025-02-14 11:20:35,976 - Epoch [9/1000], Validation Step [700/1090], Val Loss: 0.4308
2025-02-14 11:20:44,750 - Epoch [9/1000], Validation Step [800/1090], Val Loss: 4.3919
2025-02-14 11:20:53,299 - Epoch [9/1000], Validation Step [900/1090], Val Loss: 3.7527
2025-02-14 11:21:02,538 - Epoch [9/1000], Validation Step [1000/1090], Val Loss: 3.6690
2025-02-14 11:21:11,145 - Epoch 9/1000, Train Loss: 0.2080, Val Loss: 1.9835, Accuracy: 39.85%
2025-02-14 11:21:46,845 - Epoch [10/1000], Step [100/4367], Loss: 0.1680
2025-02-14 11:22:21,682 - Epoch [10/1000], Step [200/4367], Loss: 0.1897
2025-02-14 11:22:56,473 - Epoch [10/1000], Step [300/4367], Loss: 0.4233
2025-02-14 11:23:30,835 - Epoch [10/1000], Step [400/4367], Loss: 0.2250
2025-02-14 11:24:05,347 - Epoch [10/1000], Step [500/4367], Loss: 0.0566
2025-02-14 11:24:39,874 - Epoch [10/1000], Step [600/4367], Loss: 0.1348
2025-02-14 11:25:14,650 - Epoch [10/1000], Step [700/4367], Loss: 0.1652
2025-02-14 11:25:49,510 - Epoch [10/1000], Step [800/4367], Loss: 0.0882
2025-02-14 11:26:24,339 - Epoch [10/1000], Step [900/4367], Loss: 0.1684
2025-02-14 11:26:58,792 - Epoch [10/1000], Step [1000/4367], Loss: 0.1843
2025-02-14 11:27:33,615 - Epoch [10/1000], Step [1100/4367], Loss: 0.3729
2025-02-14 11:28:08,030 - Epoch [10/1000], Step [1200/4367], Loss: 0.1375
2025-02-14 11:28:42,919 - Epoch [10/1000], Step [1300/4367], Loss: 0.3019
2025-02-14 11:29:17,581 - Epoch [10/1000], Step [1400/4367], Loss: 0.4412
2025-02-14 11:29:52,502 - Epoch [10/1000], Step [1500/4367], Loss: 0.2906
2025-02-14 11:30:27,463 - Epoch [10/1000], Step [1600/4367], Loss: 0.2451
2025-02-14 11:31:01,942 - Epoch [10/1000], Step [1700/4367], Loss: 0.1762
2025-02-14 11:31:36,291 - Epoch [10/1000], Step [1800/4367], Loss: 0.6958
2025-02-14 11:32:10,960 - Epoch [10/1000], Step [1900/4367], Loss: 0.5865
2025-02-14 11:32:45,305 - Epoch [10/1000], Step [2000/4367], Loss: 0.8867
2025-02-14 11:33:19,912 - Epoch [10/1000], Step [2100/4367], Loss: 0.8031
2025-02-14 11:33:54,748 - Epoch [10/1000], Step [2200/4367], Loss: 0.5921
2025-02-14 11:34:29,785 - Epoch [10/1000], Step [2300/4367], Loss: 0.4399
2025-02-14 11:35:04,256 - Epoch [10/1000], Step [2400/4367], Loss: 0.5919
2025-02-14 11:35:39,140 - Epoch [10/1000], Step [2500/4367], Loss: 0.6823
2025-02-14 11:36:13,602 - Epoch [10/1000], Step [2600/4367], Loss: 0.3519
2025-02-14 11:36:47,931 - Epoch [10/1000], Step [2700/4367], Loss: 0.6135
2025-02-14 11:37:22,834 - Epoch [10/1000], Step [2800/4367], Loss: 0.7066
2025-02-14 11:37:57,421 - Epoch [10/1000], Step [2900/4367], Loss: 0.5477
2025-02-14 11:38:32,143 - Epoch [10/1000], Step [3000/4367], Loss: 0.5573
2025-02-14 11:39:06,524 - Epoch [10/1000], Step [3100/4367], Loss: 0.5129
2025-02-14 11:39:41,134 - Epoch [10/1000], Step [3200/4367], Loss: 0.3125
2025-02-14 11:40:16,140 - Epoch [10/1000], Step [3300/4367], Loss: 0.4006
2025-02-14 11:40:50,644 - Epoch [10/1000], Step [3400/4367], Loss: 0.7177
2025-02-14 11:41:25,589 - Epoch [10/1000], Step [3500/4367], Loss: 0.5768
2025-02-14 11:41:59,956 - Epoch [10/1000], Step [3600/4367], Loss: 0.1813
2025-02-14 11:42:34,641 - Epoch [10/1000], Step [3700/4367], Loss: 0.2966
2025-02-14 11:43:09,079 - Epoch [10/1000], Step [3800/4367], Loss: 0.6284
2025-02-14 11:43:44,347 - Epoch [10/1000], Step [3900/4367], Loss: 0.3359
2025-02-14 11:44:19,009 - Epoch [10/1000], Step [4000/4367], Loss: 0.4123
2025-02-14 11:44:54,020 - Epoch [10/1000], Step [4100/4367], Loss: 0.2460
2025-02-14 11:45:29,280 - Epoch [10/1000], Step [4200/4367], Loss: 0.2361
2025-02-14 11:46:03,809 - Epoch [10/1000], Step [4300/4367], Loss: 0.4946
2025-02-14 11:46:36,859 - Epoch [10/1000], Validation Step [100/1090], Val Loss: 0.1167
2025-02-14 11:46:46,016 - Epoch [10/1000], Validation Step [200/1090], Val Loss: 0.0395
2025-02-14 11:46:55,314 - Epoch [10/1000], Validation Step [300/1090], Val Loss: 0.1782
2025-02-14 11:47:04,923 - Epoch [10/1000], Validation Step [400/1090], Val Loss: 1.1341
2025-02-14 11:47:13,978 - Epoch [10/1000], Validation Step [500/1090], Val Loss: 1.5012
2025-02-14 11:47:23,452 - Epoch [10/1000], Validation Step [600/1090], Val Loss: 0.8841
2025-02-14 11:47:32,983 - Epoch [10/1000], Validation Step [700/1090], Val Loss: 0.9981
2025-02-14 11:47:41,719 - Epoch [10/1000], Validation Step [800/1090], Val Loss: 0.0441
2025-02-14 11:47:50,236 - Epoch [10/1000], Validation Step [900/1090], Val Loss: 0.2144
2025-02-14 11:47:59,438 - Epoch [10/1000], Validation Step [1000/1090], Val Loss: 0.2496
2025-02-14 11:48:08,017 - Epoch 10/1000, Train Loss: 0.3795, Val Loss: 0.5940, Accuracy: 76.97%
2025-02-14 11:48:08,422 - Model saved at /data2/personal/sungjin/korean_dialects/checkpoint_real/checkpoint_epoch_10.pth
2025-02-14 11:48:44,768 - Epoch [11/1000], Step [100/4367], Loss: 0.2376
2025-02-14 11:49:19,243 - Epoch [11/1000], Step [200/4367], Loss: 0.3864
2025-02-14 11:49:54,091 - Epoch [11/1000], Step [300/4367], Loss: 0.2176
2025-02-14 11:50:28,823 - Epoch [11/1000], Step [400/4367], Loss: 0.3317
2025-02-14 11:51:03,161 - Epoch [11/1000], Step [500/4367], Loss: 0.1375
2025-02-14 11:51:37,682 - Epoch [11/1000], Step [600/4367], Loss: 0.1843
2025-02-14 11:52:12,174 - Epoch [11/1000], Step [700/4367], Loss: 0.3237
2025-02-14 11:52:47,216 - Epoch [11/1000], Step [800/4367], Loss: 0.2525
2025-02-14 11:53:21,738 - Epoch [11/1000], Step [900/4367], Loss: 0.2782
2025-02-14 11:53:56,391 - Epoch [11/1000], Step [1000/4367], Loss: 0.0631
2025-02-14 11:54:31,021 - Epoch [11/1000], Step [1100/4367], Loss: 0.1217
2025-02-14 11:55:05,756 - Epoch [11/1000], Step [1200/4367], Loss: 0.2494
2025-02-14 11:55:40,587 - Epoch [11/1000], Step [1300/4367], Loss: 0.0783
2025-02-14 11:56:14,830 - Epoch [11/1000], Step [1400/4367], Loss: 0.1194
2025-02-14 11:56:49,324 - Epoch [11/1000], Step [1500/4367], Loss: 0.1908
2025-02-14 11:57:24,333 - Epoch [11/1000], Step [1600/4367], Loss: 0.3073
2025-02-14 11:57:58,990 - Epoch [11/1000], Step [1700/4367], Loss: 0.2665
2025-02-14 11:58:33,623 - Epoch [11/1000], Step [1800/4367], Loss: 0.3957
2025-02-14 11:59:08,121 - Epoch [11/1000], Step [1900/4367], Loss: 0.3088
2025-02-14 11:59:42,858 - Epoch [11/1000], Step [2000/4367], Loss: 0.3456
2025-02-14 12:00:17,586 - Epoch [11/1000], Step [2100/4367], Loss: 0.3106
2025-02-14 12:00:52,100 - Epoch [11/1000], Step [2200/4367], Loss: 0.2614
2025-02-14 12:01:27,024 - Epoch [11/1000], Step [2300/4367], Loss: 0.1833
2025-02-14 12:02:01,635 - Epoch [11/1000], Step [2400/4367], Loss: 0.2367
2025-02-14 12:02:35,863 - Epoch [11/1000], Step [2500/4367], Loss: 0.2401
2025-02-14 12:03:10,292 - Epoch [11/1000], Step [2600/4367], Loss: 0.2646
2025-02-14 12:03:45,248 - Epoch [11/1000], Step [2700/4367], Loss: 0.2295
2025-02-14 12:04:19,832 - Epoch [11/1000], Step [2800/4367], Loss: 0.2693
2025-02-14 12:04:54,162 - Epoch [11/1000], Step [2900/4367], Loss: 0.2908
2025-02-14 12:05:28,804 - Epoch [11/1000], Step [3000/4367], Loss: 0.2096
2025-02-14 12:06:03,581 - Epoch [11/1000], Step [3100/4367], Loss: 0.4914
2025-02-14 12:06:38,424 - Epoch [11/1000], Step [3200/4367], Loss: 0.3424
2025-02-14 12:07:13,575 - Epoch [11/1000], Step [3300/4367], Loss: 0.1692
2025-02-14 12:07:48,445 - Epoch [11/1000], Step [3400/4367], Loss: 0.0618
2025-02-14 12:08:23,457 - Epoch [11/1000], Step [3500/4367], Loss: 0.2602
2025-02-14 12:08:57,561 - Epoch [11/1000], Step [3600/4367], Loss: 0.2763
2025-02-14 12:09:31,951 - Epoch [11/1000], Step [3700/4367], Loss: 0.2641
2025-02-14 12:10:06,926 - Epoch [11/1000], Step [3800/4367], Loss: 0.1405
2025-02-14 12:10:41,736 - Epoch [11/1000], Step [3900/4367], Loss: 0.4910
2025-02-14 12:11:16,684 - Epoch [11/1000], Step [4000/4367], Loss: 0.2238
2025-02-14 12:11:51,079 - Epoch [11/1000], Step [4100/4367], Loss: 0.5491
2025-02-14 12:12:25,797 - Epoch [11/1000], Step [4200/4367], Loss: 0.0961
2025-02-14 12:13:00,413 - Epoch [11/1000], Step [4300/4367], Loss: 0.0928
2025-02-14 12:13:33,100 - Epoch [11/1000], Validation Step [100/1090], Val Loss: 0.0147
2025-02-14 12:13:42,291 - Epoch [11/1000], Validation Step [200/1090], Val Loss: 0.3460
2025-02-14 12:13:51,635 - Epoch [11/1000], Validation Step [300/1090], Val Loss: 2.3088
2025-02-14 12:14:01,293 - Epoch [11/1000], Validation Step [400/1090], Val Loss: 0.3064
2025-02-14 12:14:10,393 - Epoch [11/1000], Validation Step [500/1090], Val Loss: 1.6391
2025-02-14 12:14:19,919 - Epoch [11/1000], Validation Step [600/1090], Val Loss: 0.7834
2025-02-14 12:14:29,475 - Epoch [11/1000], Validation Step [700/1090], Val Loss: 0.8602
2025-02-14 12:14:38,259 - Epoch [11/1000], Validation Step [800/1090], Val Loss: 0.0002
2025-02-14 12:14:46,813 - Epoch [11/1000], Validation Step [900/1090], Val Loss: 0.0018
2025-02-14 12:14:56,059 - Epoch [11/1000], Validation Step [1000/1090], Val Loss: 0.1383
2025-02-14 12:15:04,685 - Epoch 11/1000, Train Loss: 0.2580, Val Loss: 0.5179, Accuracy: 82.63%
2025-02-14 12:15:40,796 - Epoch [12/1000], Step [100/4367], Loss: 0.3069
2025-02-14 12:16:15,153 - Epoch [12/1000], Step [200/4367], Loss: 0.3199
2025-02-14 12:16:50,087 - Epoch [12/1000], Step [300/4367], Loss: 0.0878
2025-02-14 12:17:24,828 - Epoch [12/1000], Step [400/4367], Loss: 0.1650
2025-02-14 12:17:58,995 - Epoch [12/1000], Step [500/4367], Loss: 0.1281
2025-02-14 12:18:34,180 - Epoch [12/1000], Step [600/4367], Loss: 0.1822
2025-02-14 12:19:08,838 - Epoch [12/1000], Step [700/4367], Loss: 0.1594
2025-02-14 12:19:43,593 - Epoch [12/1000], Step [800/4367], Loss: 0.1596
2025-02-14 12:20:18,207 - Epoch [12/1000], Step [900/4367], Loss: 0.1701
2025-02-14 12:20:52,887 - Epoch [12/1000], Step [1000/4367], Loss: 0.3948
2025-02-14 12:21:27,167 - Epoch [12/1000], Step [1100/4367], Loss: 0.1866
2025-02-14 12:22:01,021 - Epoch [12/1000], Step [1200/4367], Loss: 0.1471
2025-02-14 12:22:36,030 - Epoch [12/1000], Step [1300/4367], Loss: 0.2334
2025-02-14 12:23:10,877 - Epoch [12/1000], Step [1400/4367], Loss: 0.2415
2025-02-14 12:23:45,760 - Epoch [12/1000], Step [1500/4367], Loss: 0.4587
2025-02-14 12:24:19,990 - Epoch [12/1000], Step [1600/4367], Loss: 0.1572
2025-02-14 12:24:54,885 - Epoch [12/1000], Step [1700/4367], Loss: 0.1645
2025-02-14 12:25:29,901 - Epoch [12/1000], Step [1800/4367], Loss: 0.1771
2025-02-14 12:26:04,862 - Epoch [12/1000], Step [1900/4367], Loss: 0.3559
2025-02-14 12:26:39,482 - Epoch [12/1000], Step [2000/4367], Loss: 0.2645
2025-02-14 12:27:14,269 - Epoch [12/1000], Step [2100/4367], Loss: 0.0953
2025-02-14 12:27:49,060 - Epoch [12/1000], Step [2200/4367], Loss: 0.0771
2025-02-14 12:28:23,961 - Epoch [12/1000], Step [2300/4367], Loss: 0.1689
2025-02-14 12:28:58,846 - Epoch [12/1000], Step [2400/4367], Loss: 0.1095
2025-02-14 12:29:33,318 - Epoch [12/1000], Step [2500/4367], Loss: 0.1374
2025-02-14 12:30:08,052 - Epoch [12/1000], Step [2600/4367], Loss: 0.1258
2025-02-14 12:30:42,683 - Epoch [12/1000], Step [2700/4367], Loss: 0.2133
2025-02-14 12:31:17,534 - Epoch [12/1000], Step [2800/4367], Loss: 0.2280
2025-02-14 12:31:52,166 - Epoch [12/1000], Step [2900/4367], Loss: 0.0552
2025-02-14 12:32:26,943 - Epoch [12/1000], Step [3000/4367], Loss: 0.1942
2025-02-14 12:33:01,465 - Epoch [12/1000], Step [3100/4367], Loss: 0.0568
2025-02-14 12:33:35,791 - Epoch [12/1000], Step [3200/4367], Loss: 0.0775
2025-02-14 12:34:10,452 - Epoch [12/1000], Step [3300/4367], Loss: 0.2488
2025-02-14 12:34:45,385 - Epoch [12/1000], Step [3400/4367], Loss: 0.2517
2025-02-14 12:35:19,647 - Epoch [12/1000], Step [3500/4367], Loss: 0.1134
2025-02-14 12:35:54,612 - Epoch [12/1000], Step [3600/4367], Loss: 0.1880
2025-02-14 12:36:29,102 - Epoch [12/1000], Step [3700/4367], Loss: 0.1610
2025-02-14 12:37:03,665 - Epoch [12/1000], Step [3800/4367], Loss: 0.1582
2025-02-14 12:37:38,091 - Epoch [12/1000], Step [3900/4367], Loss: 0.1353
2025-02-14 12:38:12,362 - Epoch [12/1000], Step [4000/4367], Loss: 0.2382
2025-02-14 12:38:47,207 - Epoch [12/1000], Step [4100/4367], Loss: 0.2088
2025-02-14 12:39:21,756 - Epoch [12/1000], Step [4200/4367], Loss: 0.0884
2025-02-14 12:39:56,302 - Epoch [12/1000], Step [4300/4367], Loss: 0.2385
2025-02-14 12:40:29,290 - Epoch [12/1000], Validation Step [100/1090], Val Loss: 0.8023
2025-02-14 12:40:38,485 - Epoch [12/1000], Validation Step [200/1090], Val Loss: 2.5289
2025-02-14 12:40:47,820 - Epoch [12/1000], Validation Step [300/1090], Val Loss: 0.4569
2025-02-14 12:40:57,477 - Epoch [12/1000], Validation Step [400/1090], Val Loss: 4.2907
2025-02-14 12:41:06,580 - Epoch [12/1000], Validation Step [500/1090], Val Loss: 3.3224
2025-02-14 12:41:16,101 - Epoch [12/1000], Validation Step [600/1090], Val Loss: 0.2128
2025-02-14 12:41:25,661 - Epoch [12/1000], Validation Step [700/1090], Val Loss: 0.2177
2025-02-14 12:41:34,439 - Epoch [12/1000], Validation Step [800/1090], Val Loss: 3.9643
2025-02-14 12:41:42,995 - Epoch [12/1000], Validation Step [900/1090], Val Loss: 6.2247
2025-02-14 12:41:52,245 - Epoch [12/1000], Validation Step [1000/1090], Val Loss: 4.0647
2025-02-14 12:42:00,856 - Epoch 12/1000, Train Loss: 0.2049, Val Loss: 2.7481, Accuracy: 44.38%
2025-02-14 12:42:01,270 - Model saved at /data2/personal/sungjin/korean_dialects/checkpoint_real/checkpoint_epoch_12.pth
2025-02-14 12:42:36,671 - Epoch [13/1000], Step [100/4367], Loss: 0.1049
2025-02-14 12:43:11,550 - Epoch [13/1000], Step [200/4367], Loss: 0.2018
2025-02-14 12:43:46,240 - Epoch [13/1000], Step [300/4367], Loss: 0.3771
2025-02-14 12:44:20,731 - Epoch [13/1000], Step [400/4367], Loss: 0.1895
2025-02-14 12:44:55,322 - Epoch [13/1000], Step [500/4367], Loss: 0.3856
2025-02-14 12:45:29,981 - Epoch [13/1000], Step [600/4367], Loss: 0.2075
2025-02-14 12:46:04,454 - Epoch [13/1000], Step [700/4367], Loss: 0.1331
2025-02-14 12:46:38,777 - Epoch [13/1000], Step [800/4367], Loss: 0.4569
2025-02-14 12:47:13,563 - Epoch [13/1000], Step [900/4367], Loss: 0.1665
2025-02-14 12:47:47,719 - Epoch [13/1000], Step [1000/4367], Loss: 0.1316
2025-02-14 12:48:22,268 - Epoch [13/1000], Step [1100/4367], Loss: 0.1315
2025-02-14 12:48:57,217 - Epoch [13/1000], Step [1200/4367], Loss: 0.2147
2025-02-14 12:49:31,788 - Epoch [13/1000], Step [1300/4367], Loss: 0.4185
2025-02-14 12:50:06,062 - Epoch [13/1000], Step [1400/4367], Loss: 0.2632
2025-02-14 12:50:40,969 - Epoch [13/1000], Step [1500/4367], Loss: 0.2026
2025-02-14 12:51:15,885 - Epoch [13/1000], Step [1600/4367], Loss: 0.2402
2025-02-14 12:51:50,670 - Epoch [13/1000], Step [1700/4367], Loss: 0.0437
2025-02-14 12:52:25,084 - Epoch [13/1000], Step [1800/4367], Loss: 0.2076
2025-02-14 12:52:59,703 - Epoch [13/1000], Step [1900/4367], Loss: 0.2942
2025-02-14 12:53:34,335 - Epoch [13/1000], Step [2000/4367], Loss: 0.2658
2025-02-14 12:54:08,733 - Epoch [13/1000], Step [2100/4367], Loss: 0.2073
2025-02-14 12:54:43,468 - Epoch [13/1000], Step [2200/4367], Loss: 0.1060
2025-02-14 12:55:18,089 - Epoch [13/1000], Step [2300/4367], Loss: 0.1489
2025-02-14 12:55:53,231 - Epoch [13/1000], Step [2400/4367], Loss: 0.1587
2025-02-14 12:56:28,197 - Epoch [13/1000], Step [2500/4367], Loss: 0.1153
2025-02-14 12:57:02,893 - Epoch [13/1000], Step [2600/4367], Loss: 0.1015
2025-02-14 12:57:37,201 - Epoch [13/1000], Step [2700/4367], Loss: 0.2366
2025-02-14 12:58:12,067 - Epoch [13/1000], Step [2800/4367], Loss: 0.4131
2025-02-14 12:58:46,372 - Epoch [13/1000], Step [2900/4367], Loss: 0.2815
2025-02-14 12:59:20,916 - Epoch [13/1000], Step [3000/4367], Loss: 0.2116
2025-02-14 12:59:55,478 - Epoch [13/1000], Step [3100/4367], Loss: 0.3123
2025-02-14 13:00:30,078 - Epoch [13/1000], Step [3200/4367], Loss: 0.0821
2025-02-14 13:01:04,588 - Epoch [13/1000], Step [3300/4367], Loss: 0.2306
2025-02-14 13:01:39,181 - Epoch [13/1000], Step [3400/4367], Loss: 0.2694
2025-02-14 13:02:13,566 - Epoch [13/1000], Step [3500/4367], Loss: 0.2961
2025-02-14 13:02:48,290 - Epoch [13/1000], Step [3600/4367], Loss: 0.1683
2025-02-14 13:03:23,092 - Epoch [13/1000], Step [3700/4367], Loss: 0.3317
2025-02-14 13:03:57,700 - Epoch [13/1000], Step [3800/4367], Loss: 0.2380
2025-02-14 13:04:32,099 - Epoch [13/1000], Step [3900/4367], Loss: 0.0960
2025-02-14 13:05:06,791 - Epoch [13/1000], Step [4000/4367], Loss: 0.2074
2025-02-14 13:05:41,495 - Epoch [13/1000], Step [4100/4367], Loss: 0.2615
2025-02-14 13:06:16,201 - Epoch [13/1000], Step [4200/4367], Loss: 0.1172
2025-02-14 13:06:51,075 - Epoch [13/1000], Step [4300/4367], Loss: 0.0961
2025-02-14 13:07:23,926 - Epoch [13/1000], Validation Step [100/1090], Val Loss: 0.0143
2025-02-14 13:07:33,104 - Epoch [13/1000], Validation Step [200/1090], Val Loss: 0.0021
2025-02-14 13:07:42,422 - Epoch [13/1000], Validation Step [300/1090], Val Loss: 0.2916
2025-02-14 13:07:52,061 - Epoch [13/1000], Validation Step [400/1090], Val Loss: 0.1558
2025-02-14 13:08:01,155 - Epoch [13/1000], Validation Step [500/1090], Val Loss: 0.9550
2025-02-14 13:08:10,669 - Epoch [13/1000], Validation Step [600/1090], Val Loss: 0.1448
2025-02-14 13:08:20,220 - Epoch [13/1000], Validation Step [700/1090], Val Loss: 0.0970
2025-02-14 13:08:28,988 - Epoch [13/1000], Validation Step [800/1090], Val Loss: 0.0048
2025-02-14 13:08:37,527 - Epoch [13/1000], Validation Step [900/1090], Val Loss: 0.0142
2025-02-14 13:08:46,771 - Epoch [13/1000], Validation Step [1000/1090], Val Loss: 0.0001
2025-02-14 13:08:55,394 - Epoch 13/1000, Train Loss: 0.1959, Val Loss: 0.2001, Accuracy: 92.43%
2025-02-14 13:09:30,748 - Epoch [14/1000], Step [100/4367], Loss: 0.4313
2025-02-14 13:10:05,309 - Epoch [14/1000], Step [200/4367], Loss: 0.4198
2025-02-14 13:10:39,841 - Epoch [14/1000], Step [300/4367], Loss: 0.3341
2025-02-14 13:11:14,661 - Epoch [14/1000], Step [400/4367], Loss: 0.1842
2025-02-14 13:11:49,498 - Epoch [14/1000], Step [500/4367], Loss: 0.0514
2025-02-14 13:12:23,977 - Epoch [14/1000], Step [600/4367], Loss: 0.2832
2025-02-14 13:12:58,718 - Epoch [14/1000], Step [700/4367], Loss: 0.3792
2025-02-14 13:13:33,352 - Epoch [14/1000], Step [800/4367], Loss: 0.1692
2025-02-14 13:14:08,542 - Epoch [14/1000], Step [900/4367], Loss: 0.0655
2025-02-14 13:14:42,871 - Epoch [14/1000], Step [1000/4367], Loss: 0.1840
2025-02-14 13:15:17,271 - Epoch [14/1000], Step [1100/4367], Loss: 0.0969
2025-02-14 13:15:51,311 - Epoch [14/1000], Step [1200/4367], Loss: 0.0700
2025-02-14 13:16:25,976 - Epoch [14/1000], Step [1300/4367], Loss: 0.2971
2025-02-14 13:17:00,486 - Epoch [14/1000], Step [1400/4367], Loss: 0.1115
2025-02-14 13:17:34,931 - Epoch [14/1000], Step [1500/4367], Loss: 0.3120
2025-02-14 13:18:09,550 - Epoch [14/1000], Step [1600/4367], Loss: 0.3075
2025-02-14 13:18:44,026 - Epoch [14/1000], Step [1700/4367], Loss: 0.2326
2025-02-14 13:19:18,610 - Epoch [14/1000], Step [1800/4367], Loss: 0.1339
2025-02-14 13:19:53,725 - Epoch [14/1000], Step [1900/4367], Loss: 0.3397
2025-02-14 13:20:28,234 - Epoch [14/1000], Step [2000/4367], Loss: 0.1473
2025-02-14 13:21:02,353 - Epoch [14/1000], Step [2100/4367], Loss: 0.1130
2025-02-14 13:21:36,964 - Epoch [14/1000], Step [2200/4367], Loss: 0.1250
2025-02-14 13:22:11,705 - Epoch [14/1000], Step [2300/4367], Loss: 0.2882
2025-02-14 13:22:46,409 - Epoch [14/1000], Step [2400/4367], Loss: 0.1441
2025-02-14 13:23:20,886 - Epoch [14/1000], Step [2500/4367], Loss: 0.0654
2025-02-14 13:23:55,783 - Epoch [14/1000], Step [2600/4367], Loss: 0.2205
2025-02-14 13:24:30,540 - Epoch [14/1000], Step [2700/4367], Loss: 0.3194
2025-02-14 13:25:05,332 - Epoch [14/1000], Step [2800/4367], Loss: 0.2409
2025-02-14 13:25:39,972 - Epoch [14/1000], Step [2900/4367], Loss: 0.3307
2025-02-14 13:26:14,384 - Epoch [14/1000], Step [3000/4367], Loss: 0.0678
2025-02-14 13:26:48,797 - Epoch [14/1000], Step [3100/4367], Loss: 0.2199
2025-02-14 13:27:23,900 - Epoch [14/1000], Step [3200/4367], Loss: 0.0330
2025-02-14 13:27:58,723 - Epoch [14/1000], Step [3300/4367], Loss: 0.0752
2025-02-14 13:28:33,372 - Epoch [14/1000], Step [3400/4367], Loss: 0.1058
2025-02-14 13:29:07,802 - Epoch [14/1000], Step [3500/4367], Loss: 0.1238
2025-02-14 13:29:42,541 - Epoch [14/1000], Step [3600/4367], Loss: 0.0979
2025-02-14 13:30:17,491 - Epoch [14/1000], Step [3700/4367], Loss: 0.3621
2025-02-14 13:30:52,273 - Epoch [14/1000], Step [3800/4367], Loss: 0.2098
2025-02-14 13:31:27,182 - Epoch [14/1000], Step [3900/4367], Loss: 0.1107
2025-02-14 13:32:01,570 - Epoch [14/1000], Step [4000/4367], Loss: 0.1961
2025-02-14 13:32:35,951 - Epoch [14/1000], Step [4100/4367], Loss: 0.2228
2025-02-14 13:33:10,418 - Epoch [14/1000], Step [4200/4367], Loss: 0.1551
2025-02-14 13:33:45,178 - Epoch [14/1000], Step [4300/4367], Loss: 0.3419
2025-02-14 13:34:18,148 - Epoch [14/1000], Validation Step [100/1090], Val Loss: 0.2050
2025-02-14 13:34:27,327 - Epoch [14/1000], Validation Step [200/1090], Val Loss: 0.0216
2025-02-14 13:34:36,665 - Epoch [14/1000], Validation Step [300/1090], Val Loss: 1.3351
2025-02-14 13:34:46,322 - Epoch [14/1000], Validation Step [400/1090], Val Loss: 0.1250
2025-02-14 13:34:55,411 - Epoch [14/1000], Validation Step [500/1090], Val Loss: 0.8037
2025-02-14 13:35:04,925 - Epoch [14/1000], Validation Step [600/1090], Val Loss: 0.8515
2025-02-14 13:35:14,468 - Epoch [14/1000], Validation Step [700/1090], Val Loss: 0.6894
2025-02-14 13:35:23,241 - Epoch [14/1000], Validation Step [800/1090], Val Loss: 0.0580
2025-02-14 13:35:31,789 - Epoch [14/1000], Validation Step [900/1090], Val Loss: 0.1988
2025-02-14 13:35:41,029 - Epoch [14/1000], Validation Step [1000/1090], Val Loss: 0.2955
2025-02-14 13:35:49,648 - Epoch 14/1000, Train Loss: 0.1881, Val Loss: 0.5558, Accuracy: 80.62%
2025-02-14 13:35:50,145 - Model saved at /data2/personal/sungjin/korean_dialects/checkpoint_real/checkpoint_epoch_14.pth
2025-02-14 13:36:26,115 - Epoch [15/1000], Step [100/4367], Loss: 0.3032
2025-02-14 13:37:00,724 - Epoch [15/1000], Step [200/4367], Loss: 0.2350
2025-02-14 13:37:35,367 - Epoch [15/1000], Step [300/4367], Loss: 0.2861
2025-02-14 13:38:09,797 - Epoch [15/1000], Step [400/4367], Loss: 0.2143
2025-02-14 13:38:44,493 - Epoch [15/1000], Step [500/4367], Loss: 0.1723
2025-02-14 13:39:19,383 - Epoch [15/1000], Step [600/4367], Loss: 0.1787
2025-02-14 13:39:54,088 - Epoch [15/1000], Step [700/4367], Loss: 0.1003
2025-02-14 13:40:28,715 - Epoch [15/1000], Step [800/4367], Loss: 0.4967
2025-02-14 13:41:03,774 - Epoch [15/1000], Step [900/4367], Loss: 0.2129
2025-02-14 13:41:38,283 - Epoch [15/1000], Step [1000/4367], Loss: 0.2002
2025-02-14 13:42:13,110 - Epoch [15/1000], Step [1100/4367], Loss: 0.1973
2025-02-14 13:42:47,828 - Epoch [15/1000], Step [1200/4367], Loss: 0.0803
2025-02-14 13:43:22,527 - Epoch [15/1000], Step [1300/4367], Loss: 0.1824
2025-02-14 13:43:57,403 - Epoch [15/1000], Step [1400/4367], Loss: 0.1020
2025-02-14 13:44:31,884 - Epoch [15/1000], Step [1500/4367], Loss: 0.2004
2025-02-14 13:45:06,674 - Epoch [15/1000], Step [1600/4367], Loss: 0.1322
2025-02-14 13:45:41,287 - Epoch [15/1000], Step [1700/4367], Loss: 0.2468
2025-02-14 13:46:16,034 - Epoch [15/1000], Step [1800/4367], Loss: 0.2779
2025-02-14 13:46:50,724 - Epoch [15/1000], Step [1900/4367], Loss: 0.1921
2025-02-14 13:47:25,463 - Epoch [15/1000], Step [2000/4367], Loss: 0.2299
2025-02-14 13:48:00,276 - Epoch [15/1000], Step [2100/4367], Loss: 0.1072
2025-02-14 13:48:34,483 - Epoch [15/1000], Step [2200/4367], Loss: 0.3284
2025-02-14 13:49:09,068 - Epoch [15/1000], Step [2300/4367], Loss: 0.2083
2025-02-14 13:49:43,867 - Epoch [15/1000], Step [2400/4367], Loss: 0.2253
2025-02-14 13:50:18,898 - Epoch [15/1000], Step [2500/4367], Loss: 0.1632
2025-02-14 13:50:53,202 - Epoch [15/1000], Step [2600/4367], Loss: 0.2129
2025-02-14 13:51:28,291 - Epoch [15/1000], Step [2700/4367], Loss: 0.0750
2025-02-14 13:52:02,803 - Epoch [15/1000], Step [2800/4367], Loss: 0.0618
2025-02-14 13:52:37,636 - Epoch [15/1000], Step [2900/4367], Loss: 0.1783
2025-02-14 13:53:12,343 - Epoch [15/1000], Step [3000/4367], Loss: 0.2059
2025-02-14 13:53:46,780 - Epoch [15/1000], Step [3100/4367], Loss: 0.1335
2025-02-14 13:54:21,141 - Epoch [15/1000], Step [3200/4367], Loss: 0.3192
2025-02-14 13:54:55,977 - Epoch [15/1000], Step [3300/4367], Loss: 0.1086
2025-02-14 13:55:30,949 - Epoch [15/1000], Step [3400/4367], Loss: 0.1192
2025-02-14 13:56:05,658 - Epoch [15/1000], Step [3500/4367], Loss: 0.0627
2025-02-14 13:56:40,517 - Epoch [15/1000], Step [3600/4367], Loss: 0.1118
2025-02-14 13:57:14,814 - Epoch [15/1000], Step [3700/4367], Loss: 0.1980
2025-02-14 13:57:49,447 - Epoch [15/1000], Step [3800/4367], Loss: 0.2340
2025-02-14 13:58:24,188 - Epoch [15/1000], Step [3900/4367], Loss: 0.1378
2025-02-14 13:58:59,075 - Epoch [15/1000], Step [4000/4367], Loss: 0.2997
2025-02-14 13:59:33,668 - Epoch [15/1000], Step [4100/4367], Loss: 0.2020
2025-02-14 14:00:08,016 - Epoch [15/1000], Step [4200/4367], Loss: 0.0596
2025-02-14 14:00:42,880 - Epoch [15/1000], Step [4300/4367], Loss: 0.2249
2025-02-14 14:01:15,817 - Epoch [15/1000], Validation Step [100/1090], Val Loss: 0.0000
2025-02-14 14:01:25,005 - Epoch [15/1000], Validation Step [200/1090], Val Loss: 0.0000
2025-02-14 14:01:34,334 - Epoch [15/1000], Validation Step [300/1090], Val Loss: 0.7387
2025-02-14 14:01:43,986 - Epoch [15/1000], Validation Step [400/1090], Val Loss: 0.0406
2025-02-14 14:01:53,090 - Epoch [15/1000], Validation Step [500/1090], Val Loss: 0.2514
2025-02-14 14:02:02,615 - Epoch [15/1000], Validation Step [600/1090], Val Loss: 0.2964
2025-02-14 14:02:12,175 - Epoch [15/1000], Validation Step [700/1090], Val Loss: 0.1635
2025-02-14 14:02:20,943 - Epoch [15/1000], Validation Step [800/1090], Val Loss: 0.0784
2025-02-14 14:02:29,486 - Epoch [15/1000], Validation Step [900/1090], Val Loss: 0.0796
2025-02-14 14:02:38,732 - Epoch [15/1000], Validation Step [1000/1090], Val Loss: 0.0779
2025-02-14 14:02:47,354 - Epoch 15/1000, Train Loss: 0.2100, Val Loss: 0.2803, Accuracy: 89.81%
2025-02-14 14:03:22,929 - Epoch [16/1000], Step [100/4367], Loss: 0.1453
2025-02-14 14:03:57,643 - Epoch [16/1000], Step [200/4367], Loss: 0.2875
2025-02-14 14:04:32,389 - Epoch [16/1000], Step [300/4367], Loss: 0.3222
2025-02-14 14:05:06,902 - Epoch [16/1000], Step [400/4367], Loss: 0.1755
2025-02-14 14:05:41,444 - Epoch [16/1000], Step [500/4367], Loss: 0.2166
2025-02-14 14:06:16,204 - Epoch [16/1000], Step [600/4367], Loss: 0.1008
2025-02-14 14:06:50,857 - Epoch [16/1000], Step [700/4367], Loss: 0.2169
2025-02-14 14:07:25,383 - Epoch [16/1000], Step [800/4367], Loss: 0.1781
2025-02-14 14:08:00,286 - Epoch [16/1000], Step [900/4367], Loss: 0.1617
2025-02-14 14:08:35,300 - Epoch [16/1000], Step [1000/4367], Loss: 0.2915
2025-02-14 14:09:10,018 - Epoch [16/1000], Step [1100/4367], Loss: 0.2013
2025-02-14 14:09:44,542 - Epoch [16/1000], Step [1200/4367], Loss: 0.7997
2025-02-14 14:10:19,017 - Epoch [16/1000], Step [1300/4367], Loss: 0.3384
2025-02-14 14:10:53,821 - Epoch [16/1000], Step [1400/4367], Loss: 0.3791
2025-02-14 14:11:28,701 - Epoch [16/1000], Step [1500/4367], Loss: 0.4482
2025-02-14 14:12:03,468 - Epoch [16/1000], Step [1600/4367], Loss: 0.5171
2025-02-14 14:12:38,517 - Epoch [16/1000], Step [1700/4367], Loss: 0.1965
2025-02-14 14:13:13,388 - Epoch [16/1000], Step [1800/4367], Loss: 0.3806
2025-02-14 14:13:47,986 - Epoch [16/1000], Step [1900/4367], Loss: 0.2131
2025-02-14 14:14:22,614 - Epoch [16/1000], Step [2000/4367], Loss: 0.1927
2025-02-14 14:14:57,414 - Epoch [16/1000], Step [2100/4367], Loss: 0.2254
2025-02-14 14:15:31,865 - Epoch [16/1000], Step [2200/4367], Loss: 0.3647
2025-02-14 14:16:06,531 - Epoch [16/1000], Step [2300/4367], Loss: 0.4515
2025-02-14 14:16:41,400 - Epoch [16/1000], Step [2400/4367], Loss: 0.2285
2025-02-14 14:17:15,712 - Epoch [16/1000], Step [2500/4367], Loss: 0.2499
2025-02-14 14:17:50,285 - Epoch [16/1000], Step [2600/4367], Loss: 0.2387
2025-02-14 14:18:25,225 - Epoch [16/1000], Step [2700/4367], Loss: 0.2338
2025-02-14 14:18:59,415 - Epoch [16/1000], Step [2800/4367], Loss: 0.1749
2025-02-14 14:19:34,088 - Epoch [16/1000], Step [2900/4367], Loss: 0.1936
2025-02-14 14:20:09,022 - Epoch [16/1000], Step [3000/4367], Loss: 0.2497
2025-02-14 14:20:43,422 - Epoch [16/1000], Step [3100/4367], Loss: 0.2746
2025-02-14 14:21:18,481 - Epoch [16/1000], Step [3200/4367], Loss: 0.2793
2025-02-14 14:21:52,870 - Epoch [16/1000], Step [3300/4367], Loss: 0.2947
2025-02-14 14:22:27,440 - Epoch [16/1000], Step [3400/4367], Loss: 0.3146
2025-02-14 14:23:02,068 - Epoch [16/1000], Step [3500/4367], Loss: 0.1928
2025-02-14 14:23:36,721 - Epoch [16/1000], Step [3600/4367], Loss: 0.1582
2025-02-14 14:24:11,107 - Epoch [16/1000], Step [3700/4367], Loss: 0.1686
2025-02-14 14:24:45,925 - Epoch [16/1000], Step [3800/4367], Loss: 0.0809
2025-02-14 14:25:20,790 - Epoch [16/1000], Step [3900/4367], Loss: 0.0792
2025-02-14 14:25:55,147 - Epoch [16/1000], Step [4000/4367], Loss: 0.1159
2025-02-14 14:26:29,742 - Epoch [16/1000], Step [4100/4367], Loss: 0.1738
2025-02-14 14:27:04,543 - Epoch [16/1000], Step [4200/4367], Loss: 0.2118
2025-02-14 14:27:39,602 - Epoch [16/1000], Step [4300/4367], Loss: 0.1747
2025-02-14 14:28:12,234 - Epoch [16/1000], Validation Step [100/1090], Val Loss: 0.1985
2025-02-14 14:28:21,434 - Epoch [16/1000], Validation Step [200/1090], Val Loss: 0.4562
2025-02-14 14:28:30,765 - Epoch [16/1000], Validation Step [300/1090], Val Loss: 0.4794
2025-02-14 14:28:40,411 - Epoch [16/1000], Validation Step [400/1090], Val Loss: 0.2963
2025-02-14 14:28:49,505 - Epoch [16/1000], Validation Step [500/1090], Val Loss: 0.7674
2025-02-14 14:28:59,023 - Epoch [16/1000], Validation Step [600/1090], Val Loss: 0.3507
2025-02-14 14:29:08,578 - Epoch [16/1000], Validation Step [700/1090], Val Loss: 0.2590
2025-02-14 14:29:17,353 - Epoch [16/1000], Validation Step [800/1090], Val Loss: 0.0135
2025-02-14 14:29:25,907 - Epoch [16/1000], Validation Step [900/1090], Val Loss: 0.0155
2025-02-14 14:29:35,152 - Epoch [16/1000], Validation Step [1000/1090], Val Loss: 0.1627
2025-02-14 14:29:43,767 - Epoch 16/1000, Train Loss: 0.2335, Val Loss: 0.4250, Accuracy: 84.61%
2025-02-14 14:29:44,203 - Model saved at /data2/personal/sungjin/korean_dialects/checkpoint_real/checkpoint_epoch_16.pth
2025-02-14 14:30:20,316 - Epoch [17/1000], Step [100/4367], Loss: 0.2794
2025-02-14 14:30:55,373 - Epoch [17/1000], Step [200/4367], Loss: 0.2246
2025-02-14 14:31:29,519 - Epoch [17/1000], Step [300/4367], Loss: 0.3459
2025-02-14 14:32:04,155 - Epoch [17/1000], Step [400/4367], Loss: 0.2454
2025-02-14 14:32:38,693 - Epoch [17/1000], Step [500/4367], Loss: 0.1316
2025-02-14 14:33:13,547 - Epoch [17/1000], Step [600/4367], Loss: 0.1747
2025-02-14 14:33:48,728 - Epoch [17/1000], Step [700/4367], Loss: 0.1926
2025-02-14 14:34:23,182 - Epoch [17/1000], Step [800/4367], Loss: 0.2070
2025-02-14 14:34:58,195 - Epoch [17/1000], Step [900/4367], Loss: 0.2209
2025-02-14 14:35:32,664 - Epoch [17/1000], Step [1000/4367], Loss: 0.1480
2025-02-14 14:36:06,999 - Epoch [17/1000], Step [1100/4367], Loss: 0.2712
2025-02-14 14:36:41,683 - Epoch [17/1000], Step [1200/4367], Loss: 0.2623
2025-02-14 14:37:16,357 - Epoch [17/1000], Step [1300/4367], Loss: 0.0857
2025-02-14 14:37:51,211 - Epoch [17/1000], Step [1400/4367], Loss: 0.2256
2025-02-14 14:38:25,995 - Epoch [17/1000], Step [1500/4367], Loss: 0.2559
2025-02-14 14:39:01,031 - Epoch [17/1000], Step [1600/4367], Loss: 0.1685
2025-02-14 14:39:35,537 - Epoch [17/1000], Step [1700/4367], Loss: 0.1899
2025-02-14 14:40:09,948 - Epoch [17/1000], Step [1800/4367], Loss: 0.1097
2025-02-14 14:40:44,242 - Epoch [17/1000], Step [1900/4367], Loss: 0.1044
2025-02-14 14:41:19,325 - Epoch [17/1000], Step [2000/4367], Loss: 0.2851
2025-02-14 14:41:53,712 - Epoch [17/1000], Step [2100/4367], Loss: 0.2150
2025-02-14 14:42:28,196 - Epoch [17/1000], Step [2200/4367], Loss: 0.1065
2025-02-14 14:43:02,978 - Epoch [17/1000], Step [2300/4367], Loss: 0.3644
2025-02-14 14:43:37,653 - Epoch [17/1000], Step [2400/4367], Loss: 0.1085
2025-02-14 14:44:12,027 - Epoch [17/1000], Step [2500/4367], Loss: 0.2201
2025-02-14 14:44:46,878 - Epoch [17/1000], Step [2600/4367], Loss: 0.2711
2025-02-14 14:45:21,110 - Epoch [17/1000], Step [2700/4367], Loss: 0.1930
2025-02-14 14:45:55,759 - Epoch [17/1000], Step [2800/4367], Loss: 0.0954
2025-02-14 14:46:30,629 - Epoch [17/1000], Step [2900/4367], Loss: 0.3682
2025-02-14 14:47:05,209 - Epoch [17/1000], Step [3000/4367], Loss: 0.2836
2025-02-14 14:47:39,156 - Epoch [17/1000], Step [3100/4367], Loss: 0.3653
2025-02-14 14:48:14,108 - Epoch [17/1000], Step [3200/4367], Loss: 0.2218
2025-02-14 14:48:48,951 - Epoch [17/1000], Step [3300/4367], Loss: 0.2029
2025-02-14 14:49:23,387 - Epoch [17/1000], Step [3400/4367], Loss: 0.1511
2025-02-14 14:49:58,214 - Epoch [17/1000], Step [3500/4367], Loss: 0.1167
2025-02-14 14:50:32,774 - Epoch [17/1000], Step [3600/4367], Loss: 0.2456
2025-02-14 14:51:07,504 - Epoch [17/1000], Step [3700/4367], Loss: 0.1462
2025-02-14 14:51:41,853 - Epoch [17/1000], Step [3800/4367], Loss: 0.2385
2025-02-14 14:52:16,594 - Epoch [17/1000], Step [3900/4367], Loss: 0.1885
2025-02-14 14:52:51,248 - Epoch [17/1000], Step [4000/4367], Loss: 0.1965
2025-02-14 14:53:25,789 - Epoch [17/1000], Step [4100/4367], Loss: 0.1847
2025-02-14 14:54:00,358 - Epoch [17/1000], Step [4200/4367], Loss: 0.1819
2025-02-14 14:54:35,259 - Epoch [17/1000], Step [4300/4367], Loss: 0.2622
2025-02-14 14:55:08,633 - Epoch [17/1000], Validation Step [100/1090], Val Loss: 0.0050
2025-02-14 14:55:17,810 - Epoch [17/1000], Validation Step [200/1090], Val Loss: 0.0537
2025-02-14 14:55:27,138 - Epoch [17/1000], Validation Step [300/1090], Val Loss: 0.4178
2025-02-14 14:55:36,782 - Epoch [17/1000], Validation Step [400/1090], Val Loss: 0.3934
2025-02-14 14:55:45,858 - Epoch [17/1000], Validation Step [500/1090], Val Loss: 0.4026
2025-02-14 14:55:55,360 - Epoch [17/1000], Validation Step [600/1090], Val Loss: 0.2955
2025-02-14 14:56:04,899 - Epoch [17/1000], Validation Step [700/1090], Val Loss: 0.2966
2025-02-14 14:56:13,659 - Epoch [17/1000], Validation Step [800/1090], Val Loss: 0.0382
2025-02-14 14:56:22,199 - Epoch [17/1000], Validation Step [900/1090], Val Loss: 0.1907
2025-02-14 14:56:31,433 - Epoch [17/1000], Validation Step [1000/1090], Val Loss: 0.0000
2025-02-14 14:56:40,041 - Epoch 17/1000, Train Loss: 0.2037, Val Loss: 0.2720, Accuracy: 90.14%
2025-02-14 14:57:15,814 - Epoch [18/1000], Step [100/4367], Loss: 0.1399
2025-02-14 14:57:50,570 - Epoch [18/1000], Step [200/4367], Loss: 0.1983
2025-02-14 14:58:25,144 - Epoch [18/1000], Step [300/4367], Loss: 0.2140
2025-02-14 14:58:59,815 - Epoch [18/1000], Step [400/4367], Loss: 0.1222
2025-02-14 14:59:34,561 - Epoch [18/1000], Step [500/4367], Loss: 0.1058
2025-02-14 15:00:09,386 - Epoch [18/1000], Step [600/4367], Loss: 0.1374
2025-02-14 15:00:44,302 - Epoch [18/1000], Step [700/4367], Loss: 0.0437
2025-02-14 15:01:18,940 - Epoch [18/1000], Step [800/4367], Loss: 0.1487
2025-02-14 15:01:53,564 - Epoch [18/1000], Step [900/4367], Loss: 0.0879
2025-02-14 15:02:28,454 - Epoch [18/1000], Step [1000/4367], Loss: 0.3337
2025-02-14 15:03:02,944 - Epoch [18/1000], Step [1100/4367], Loss: 0.2928
2025-02-14 15:03:37,581 - Epoch [18/1000], Step [1200/4367], Loss: 0.1717
2025-02-14 15:04:12,460 - Epoch [18/1000], Step [1300/4367], Loss: 0.2225
2025-02-14 15:04:47,191 - Epoch [18/1000], Step [1400/4367], Loss: 0.2837
2025-02-14 15:05:21,325 - Epoch [18/1000], Step [1500/4367], Loss: 0.0445
2025-02-14 15:05:55,389 - Epoch [18/1000], Step [1600/4367], Loss: 0.1368
2025-02-14 15:06:30,289 - Epoch [18/1000], Step [1700/4367], Loss: 0.1959
2025-02-14 15:07:05,017 - Epoch [18/1000], Step [1800/4367], Loss: 0.1118
2025-02-14 15:07:40,067 - Epoch [18/1000], Step [1900/4367], Loss: 0.3511
2025-02-14 15:08:15,010 - Epoch [18/1000], Step [2000/4367], Loss: 0.1553
2025-02-14 15:08:49,281 - Epoch [18/1000], Step [2100/4367], Loss: 0.2139
2025-02-14 15:09:23,681 - Epoch [18/1000], Step [2200/4367], Loss: 0.2902
2025-02-14 15:09:58,217 - Epoch [18/1000], Step [2300/4367], Loss: 0.2736
2025-02-14 15:10:33,048 - Epoch [18/1000], Step [2400/4367], Loss: 0.1564
2025-02-14 15:11:07,896 - Epoch [18/1000], Step [2500/4367], Loss: 0.3112
2025-02-14 15:11:42,572 - Epoch [18/1000], Step [2600/4367], Loss: 0.1904
2025-02-14 15:12:17,146 - Epoch [18/1000], Step [2700/4367], Loss: 0.1704
2025-02-14 15:12:51,805 - Epoch [18/1000], Step [2800/4367], Loss: 0.1456
2025-02-14 15:13:26,097 - Epoch [18/1000], Step [2900/4367], Loss: 0.1123
2025-02-14 15:14:01,002 - Epoch [18/1000], Step [3000/4367], Loss: 0.0399
2025-02-14 15:14:35,250 - Epoch [18/1000], Step [3100/4367], Loss: 0.1785
2025-02-14 15:15:09,885 - Epoch [18/1000], Step [3200/4367], Loss: 0.1058
2025-02-14 15:15:44,766 - Epoch [18/1000], Step [3300/4367], Loss: 0.2591
2025-02-14 15:16:19,588 - Epoch [18/1000], Step [3400/4367], Loss: 0.0968
2025-02-14 15:16:54,498 - Epoch [18/1000], Step [3500/4367], Loss: 0.2167
2025-02-14 15:17:29,133 - Epoch [18/1000], Step [3600/4367], Loss: 0.2067
2025-02-14 15:18:03,997 - Epoch [18/1000], Step [3700/4367], Loss: 0.1125
2025-02-14 15:18:38,326 - Epoch [18/1000], Step [3800/4367], Loss: 0.0957
2025-02-14 15:19:12,836 - Epoch [18/1000], Step [3900/4367], Loss: 0.1054
2025-02-14 15:19:47,491 - Epoch [18/1000], Step [4000/4367], Loss: 0.0655
2025-02-14 15:20:21,890 - Epoch [18/1000], Step [4100/4367], Loss: 0.3186
2025-02-14 15:20:56,286 - Epoch [18/1000], Step [4200/4367], Loss: 0.1942
2025-02-14 15:21:30,667 - Epoch [18/1000], Step [4300/4367], Loss: 0.1750
2025-02-14 15:22:03,871 - Epoch [18/1000], Validation Step [100/1090], Val Loss: 0.0015
2025-02-14 15:22:13,059 - Epoch [18/1000], Validation Step [200/1090], Val Loss: 0.0015
2025-02-14 15:22:22,388 - Epoch [18/1000], Validation Step [300/1090], Val Loss: 0.2045
2025-02-14 15:22:32,049 - Epoch [18/1000], Validation Step [400/1090], Val Loss: 0.1271
2025-02-14 15:22:41,153 - Epoch [18/1000], Validation Step [500/1090], Val Loss: 0.4407
2025-02-14 15:22:50,672 - Epoch [18/1000], Validation Step [600/1090], Val Loss: 0.6487
2025-02-14 15:23:00,227 - Epoch [18/1000], Validation Step [700/1090], Val Loss: 0.8243
2025-02-14 15:23:09,021 - Epoch [18/1000], Validation Step [800/1090], Val Loss: 0.0212
2025-02-14 15:23:17,590 - Epoch [18/1000], Validation Step [900/1090], Val Loss: 0.0309
2025-02-14 15:23:26,835 - Epoch [18/1000], Validation Step [1000/1090], Val Loss: 0.0004
2025-02-14 15:23:35,452 - Epoch 18/1000, Train Loss: 0.1787, Val Loss: 0.2537, Accuracy: 90.04%
2025-02-14 15:23:35,909 - Model saved at /data2/personal/sungjin/korean_dialects/checkpoint_real/checkpoint_epoch_18.pth
2025-02-14 15:24:11,882 - Epoch [19/1000], Step [100/4367], Loss: 0.1593
2025-02-14 15:24:46,549 - Epoch [19/1000], Step [200/4367], Loss: 0.2469
2025-02-14 15:25:20,819 - Epoch [19/1000], Step [300/4367], Loss: 0.2025
2025-02-14 15:25:55,285 - Epoch [19/1000], Step [400/4367], Loss: 0.2763
2025-02-14 15:26:29,882 - Epoch [19/1000], Step [500/4367], Loss: 0.1544
2025-02-14 15:27:04,559 - Epoch [19/1000], Step [600/4367], Loss: 0.3087
2025-02-14 15:27:39,741 - Epoch [19/1000], Step [700/4367], Loss: 0.1603
2025-02-14 15:28:14,471 - Epoch [19/1000], Step [800/4367], Loss: 0.2417
2025-02-14 15:28:48,847 - Epoch [19/1000], Step [900/4367], Loss: 0.0491
2025-02-14 15:29:23,309 - Epoch [19/1000], Step [1000/4367], Loss: 0.1429
2025-02-14 15:29:58,025 - Epoch [19/1000], Step [1100/4367], Loss: 0.0622
2025-02-14 15:30:32,426 - Epoch [19/1000], Step [1200/4367], Loss: 0.0308
2025-02-14 15:31:07,329 - Epoch [19/1000], Step [1300/4367], Loss: 0.1864
2025-02-14 15:31:41,926 - Epoch [19/1000], Step [1400/4367], Loss: 0.1128
2025-02-14 15:32:16,605 - Epoch [19/1000], Step [1500/4367], Loss: 0.3301
2025-02-14 15:32:51,227 - Epoch [19/1000], Step [1600/4367], Loss: 0.0709
2025-02-14 15:33:25,814 - Epoch [19/1000], Step [1700/4367], Loss: 0.1570
2025-02-14 15:34:00,347 - Epoch [19/1000], Step [1800/4367], Loss: 0.0768
2025-02-14 15:34:34,982 - Epoch [19/1000], Step [1900/4367], Loss: 0.1787
2025-02-14 15:35:09,840 - Epoch [19/1000], Step [2000/4367], Loss: 0.1767
2025-02-14 15:35:44,818 - Epoch [19/1000], Step [2100/4367], Loss: 0.1956
2025-02-14 15:36:18,811 - Epoch [19/1000], Step [2200/4367], Loss: 0.1209
2025-02-14 15:36:53,727 - Epoch [19/1000], Step [2300/4367], Loss: 0.1455
2025-02-14 15:37:28,014 - Epoch [19/1000], Step [2400/4367], Loss: 0.1079
2025-02-14 15:38:02,688 - Epoch [19/1000], Step [2500/4367], Loss: 0.1739
2025-02-14 15:38:37,396 - Epoch [19/1000], Step [2600/4367], Loss: 0.1673
2025-02-14 15:39:12,218 - Epoch [19/1000], Step [2700/4367], Loss: 0.1400
2025-02-14 15:39:46,576 - Epoch [19/1000], Step [2800/4367], Loss: 0.2122
2025-02-14 15:40:21,317 - Epoch [19/1000], Step [2900/4367], Loss: 0.1311
2025-02-14 15:40:55,757 - Epoch [19/1000], Step [3000/4367], Loss: 0.3584
2025-02-14 15:41:30,290 - Epoch [19/1000], Step [3100/4367], Loss: 0.1820
2025-02-14 15:42:05,420 - Epoch [19/1000], Step [3200/4367], Loss: 0.1283
2025-02-14 15:42:40,074 - Epoch [19/1000], Step [3300/4367], Loss: 0.2730
2025-02-14 15:43:14,751 - Epoch [19/1000], Step [3400/4367], Loss: 0.1332
2025-02-14 15:43:49,612 - Epoch [19/1000], Step [3500/4367], Loss: 0.1127
2025-02-14 15:44:24,488 - Epoch [19/1000], Step [3600/4367], Loss: 0.1522
2025-02-14 15:44:59,238 - Epoch [19/1000], Step [3700/4367], Loss: 0.0487
2025-02-14 15:45:33,633 - Epoch [19/1000], Step [3800/4367], Loss: 0.2478
2025-02-14 15:46:08,289 - Epoch [19/1000], Step [3900/4367], Loss: 0.2395
2025-02-14 15:46:43,049 - Epoch [19/1000], Step [4000/4367], Loss: 0.1499
2025-02-14 15:47:17,784 - Epoch [19/1000], Step [4100/4367], Loss: 0.1869
2025-02-14 15:47:51,886 - Epoch [19/1000], Step [4200/4367], Loss: 0.1033
2025-02-14 15:48:26,660 - Epoch [19/1000], Step [4300/4367], Loss: 0.1942
2025-02-14 15:48:59,826 - Epoch [19/1000], Validation Step [100/1090], Val Loss: 0.0002
2025-02-14 15:49:09,035 - Epoch [19/1000], Validation Step [200/1090], Val Loss: 0.0024
2025-02-14 15:49:18,371 - Epoch [19/1000], Validation Step [300/1090], Val Loss: 0.2835
2025-02-14 15:49:28,027 - Epoch [19/1000], Validation Step [400/1090], Val Loss: 0.0708
2025-02-14 15:49:37,124 - Epoch [19/1000], Validation Step [500/1090], Val Loss: 0.4283
2025-02-14 15:49:46,651 - Epoch [19/1000], Validation Step [600/1090], Val Loss: 0.3270
2025-02-14 15:49:56,210 - Epoch [19/1000], Validation Step [700/1090], Val Loss: 0.3234
2025-02-14 15:50:04,989 - Epoch [19/1000], Validation Step [800/1090], Val Loss: 0.0128
2025-02-14 15:50:13,555 - Epoch [19/1000], Validation Step [900/1090], Val Loss: 0.0086
2025-02-14 15:50:22,806 - Epoch [19/1000], Validation Step [1000/1090], Val Loss: 0.0003
2025-02-14 15:50:31,435 - Epoch 19/1000, Train Loss: 0.1703, Val Loss: 0.1803, Accuracy: 93.15%
2025-02-14 15:51:07,241 - Epoch [20/1000], Step [100/4367], Loss: 0.4025
2025-02-14 15:51:41,944 - Epoch [20/1000], Step [200/4367], Loss: 0.4651
2025-02-14 15:52:16,624 - Epoch [20/1000], Step [300/4367], Loss: 0.1076
2025-02-14 15:52:51,660 - Epoch [20/1000], Step [400/4367], Loss: 0.0218
2025-02-14 15:53:26,210 - Epoch [20/1000], Step [500/4367], Loss: 0.3103
2025-02-14 15:54:01,146 - Epoch [20/1000], Step [600/4367], Loss: 0.1271
2025-02-14 15:54:35,834 - Epoch [20/1000], Step [700/4367], Loss: 0.2233
2025-02-14 15:55:10,924 - Epoch [20/1000], Step [800/4367], Loss: 0.0473
2025-02-14 15:55:45,502 - Epoch [20/1000], Step [900/4367], Loss: 0.2593
2025-02-14 15:56:20,141 - Epoch [20/1000], Step [1000/4367], Loss: 0.3455
2025-02-14 15:56:54,631 - Epoch [20/1000], Step [1100/4367], Loss: 0.3242
2025-02-14 15:57:29,154 - Epoch [20/1000], Step [1200/4367], Loss: 0.1653
2025-02-14 15:58:03,776 - Epoch [20/1000], Step [1300/4367], Loss: 0.1206
2025-02-14 15:58:38,194 - Epoch [20/1000], Step [1400/4367], Loss: 0.0902
2025-02-14 15:59:12,920 - Epoch [20/1000], Step [1500/4367], Loss: 0.3471
2025-02-14 15:59:47,305 - Epoch [20/1000], Step [1600/4367], Loss: 0.2318
2025-02-14 16:00:22,037 - Epoch [20/1000], Step [1700/4367], Loss: 0.2475
2025-02-14 16:00:56,562 - Epoch [20/1000], Step [1800/4367], Loss: 0.1428
2025-02-14 16:01:31,416 - Epoch [20/1000], Step [1900/4367], Loss: 0.2084
2025-02-14 16:02:06,355 - Epoch [20/1000], Step [2000/4367], Loss: 0.1981
2025-02-14 16:02:41,349 - Epoch [20/1000], Step [2100/4367], Loss: 0.1485
2025-02-14 16:03:15,691 - Epoch [20/1000], Step [2200/4367], Loss: 0.2856
2025-02-14 16:03:50,343 - Epoch [20/1000], Step [2300/4367], Loss: 0.0775
2025-02-14 16:04:24,598 - Epoch [20/1000], Step [2400/4367], Loss: 0.2488
2025-02-14 16:04:59,293 - Epoch [20/1000], Step [2500/4367], Loss: 0.1510
2025-02-14 16:05:34,183 - Epoch [20/1000], Step [2600/4367], Loss: 0.2366
2025-02-14 16:06:08,780 - Epoch [20/1000], Step [2700/4367], Loss: 0.2734
2025-02-14 16:06:43,710 - Epoch [20/1000], Step [2800/4367], Loss: 0.2176
2025-02-14 16:07:18,496 - Epoch [20/1000], Step [2900/4367], Loss: 0.1995
2025-02-14 16:07:53,071 - Epoch [20/1000], Step [3000/4367], Loss: 0.3148
2025-02-14 16:08:27,377 - Epoch [20/1000], Step [3100/4367], Loss: 0.0792
2025-02-14 16:09:02,243 - Epoch [20/1000], Step [3200/4367], Loss: 0.2057
2025-02-14 16:09:37,159 - Epoch [20/1000], Step [3300/4367], Loss: 0.1831
2025-02-14 16:10:11,663 - Epoch [20/1000], Step [3400/4367], Loss: 0.2011
2025-02-14 16:10:46,146 - Epoch [20/1000], Step [3500/4367], Loss: 0.1614
2025-02-14 16:11:20,518 - Epoch [20/1000], Step [3600/4367], Loss: 0.0297
2025-02-14 16:11:55,334 - Epoch [20/1000], Step [3700/4367], Loss: 0.1815
2025-02-14 16:12:30,083 - Epoch [20/1000], Step [3800/4367], Loss: 0.1252
2025-02-14 16:13:04,224 - Epoch [20/1000], Step [3900/4367], Loss: 0.2138
2025-02-14 16:13:39,446 - Epoch [20/1000], Step [4000/4367], Loss: 0.3349
2025-02-14 16:14:13,854 - Epoch [20/1000], Step [4100/4367], Loss: 0.3530
2025-02-14 16:14:49,041 - Epoch [20/1000], Step [4200/4367], Loss: 0.1316
2025-02-14 16:15:23,380 - Epoch [20/1000], Step [4300/4367], Loss: 0.2069
2025-02-14 16:15:56,704 - Epoch [20/1000], Validation Step [100/1090], Val Loss: 0.0003
2025-02-14 16:16:05,899 - Epoch [20/1000], Validation Step [200/1090], Val Loss: 0.0016
2025-02-14 16:16:15,239 - Epoch [20/1000], Validation Step [300/1090], Val Loss: 0.2867
2025-02-14 16:16:24,866 - Epoch [20/1000], Validation Step [400/1090], Val Loss: 0.2065
2025-02-14 16:16:33,962 - Epoch [20/1000], Validation Step [500/1090], Val Loss: 0.5883
2025-02-14 16:16:43,472 - Epoch [20/1000], Validation Step [600/1090], Val Loss: 0.1772
2025-02-14 16:16:53,022 - Epoch [20/1000], Validation Step [700/1090], Val Loss: 0.1481
2025-02-14 16:17:01,791 - Epoch [20/1000], Validation Step [800/1090], Val Loss: 0.0137
2025-02-14 16:17:10,336 - Epoch [20/1000], Validation Step [900/1090], Val Loss: 0.0072
2025-02-14 16:17:19,586 - Epoch [20/1000], Validation Step [1000/1090], Val Loss: 0.0002
2025-02-14 16:17:28,201 - Epoch 20/1000, Train Loss: 0.1837, Val Loss: 0.1892, Accuracy: 93.09%
2025-02-14 16:17:28,619 - Model saved at /data2/personal/sungjin/korean_dialects/checkpoint_real/checkpoint_epoch_20.pth
2025-02-14 16:18:04,230 - Epoch [21/1000], Step [100/4367], Loss: 0.1318
2025-02-14 16:18:38,691 - Epoch [21/1000], Step [200/4367], Loss: 0.2775
2025-02-14 16:19:13,653 - Epoch [21/1000], Step [300/4367], Loss: 0.0960
2025-02-14 16:19:48,641 - Epoch [21/1000], Step [400/4367], Loss: 0.0456
2025-02-14 16:20:23,400 - Epoch [21/1000], Step [500/4367], Loss: 0.2647
2025-02-14 16:20:58,289 - Epoch [21/1000], Step [600/4367], Loss: 0.3945
2025-02-14 16:21:32,952 - Epoch [21/1000], Step [700/4367], Loss: 0.3535
2025-02-14 16:22:07,694 - Epoch [21/1000], Step [800/4367], Loss: 0.1021
2025-02-14 16:22:42,612 - Epoch [21/1000], Step [900/4367], Loss: 0.2230
2025-02-14 16:23:17,236 - Epoch [21/1000], Step [1000/4367], Loss: 0.0626
2025-02-14 16:23:52,025 - Epoch [21/1000], Step [1100/4367], Loss: 0.4724
2025-02-14 16:24:26,434 - Epoch [21/1000], Step [1200/4367], Loss: 0.1143
2025-02-14 16:25:00,947 - Epoch [21/1000], Step [1300/4367], Loss: 0.1585
2025-02-14 16:25:35,542 - Epoch [21/1000], Step [1400/4367], Loss: 0.2353
2025-02-14 16:26:10,435 - Epoch [21/1000], Step [1500/4367], Loss: 0.1614
2025-02-14 16:26:45,349 - Epoch [21/1000], Step [1600/4367], Loss: 0.2370
2025-02-14 16:27:20,018 - Epoch [21/1000], Step [1700/4367], Loss: 0.0843
2025-02-14 16:27:54,379 - Epoch [21/1000], Step [1800/4367], Loss: 0.1570
2025-02-14 16:28:29,000 - Epoch [21/1000], Step [1900/4367], Loss: 0.0962
2025-02-14 16:29:03,808 - Epoch [21/1000], Step [2000/4367], Loss: 0.1468
2025-02-14 16:29:38,788 - Epoch [21/1000], Step [2100/4367], Loss: 0.2307
2025-02-14 16:30:13,680 - Epoch [21/1000], Step [2200/4367], Loss: 0.1678
2025-02-14 16:30:48,499 - Epoch [21/1000], Step [2300/4367], Loss: 0.1318
2025-02-14 16:31:23,428 - Epoch [21/1000], Step [2400/4367], Loss: 0.1830
2025-02-14 16:31:58,336 - Epoch [21/1000], Step [2500/4367], Loss: 0.1904
2025-02-14 16:32:33,113 - Epoch [21/1000], Step [2600/4367], Loss: 0.2861
2025-02-14 16:33:07,761 - Epoch [21/1000], Step [2700/4367], Loss: 0.1680
2025-02-14 16:33:42,106 - Epoch [21/1000], Step [2800/4367], Loss: 0.2053
2025-02-14 16:34:16,552 - Epoch [21/1000], Step [2900/4367], Loss: 0.2273
2025-02-14 16:34:51,264 - Epoch [21/1000], Step [3000/4367], Loss: 0.0731
2025-02-14 16:35:26,385 - Epoch [21/1000], Step [3100/4367], Loss: 0.2233
2025-02-14 16:36:01,385 - Epoch [21/1000], Step [3200/4367], Loss: 0.4363
2025-02-14 16:36:36,201 - Epoch [21/1000], Step [3300/4367], Loss: 0.1444
2025-02-14 16:37:10,402 - Epoch [21/1000], Step [3400/4367], Loss: 0.2309
2025-02-14 16:37:44,731 - Epoch [21/1000], Step [3500/4367], Loss: 0.2117
2025-02-14 16:38:19,436 - Epoch [21/1000], Step [3600/4367], Loss: 0.2158
2025-02-14 16:38:54,307 - Epoch [21/1000], Step [3700/4367], Loss: 0.2441
2025-02-14 16:39:29,009 - Epoch [21/1000], Step [3800/4367], Loss: 0.1139
2025-02-14 16:40:03,235 - Epoch [21/1000], Step [3900/4367], Loss: 0.3599
2025-02-14 16:40:38,133 - Epoch [21/1000], Step [4000/4367], Loss: 0.1013
2025-02-14 16:41:13,148 - Epoch [21/1000], Step [4100/4367], Loss: 0.2876
2025-02-14 16:41:47,823 - Epoch [21/1000], Step [4200/4367], Loss: 0.1458
2025-02-14 16:42:22,529 - Epoch [21/1000], Step [4300/4367], Loss: 0.0445
2025-02-14 16:42:55,806 - Epoch [21/1000], Validation Step [100/1090], Val Loss: 0.0139
2025-02-14 16:43:05,011 - Epoch [21/1000], Validation Step [200/1090], Val Loss: 0.0054
2025-02-14 16:43:14,354 - Epoch [21/1000], Validation Step [300/1090], Val Loss: 0.3031
2025-02-14 16:43:24,014 - Epoch [21/1000], Validation Step [400/1090], Val Loss: 0.3961
2025-02-14 16:43:33,117 - Epoch [21/1000], Validation Step [500/1090], Val Loss: 0.3401
2025-02-14 16:43:42,636 - Epoch [21/1000], Validation Step [600/1090], Val Loss: 0.3061
2025-02-14 16:43:52,195 - Epoch [21/1000], Validation Step [700/1090], Val Loss: 0.1640
2025-02-14 16:44:00,967 - Epoch [21/1000], Validation Step [800/1090], Val Loss: 0.0413
2025-02-14 16:44:09,513 - Epoch [21/1000], Validation Step [900/1090], Val Loss: 0.0987
2025-02-14 16:44:18,763 - Epoch [21/1000], Validation Step [1000/1090], Val Loss: 0.0035
2025-02-14 16:44:27,376 - Epoch 21/1000, Train Loss: 0.1841, Val Loss: 0.2399, Accuracy: 91.53%
2025-02-14 16:45:02,813 - Epoch [22/1000], Step [100/4367], Loss: 0.1895
2025-02-14 16:45:37,862 - Epoch [22/1000], Step [200/4367], Loss: 0.1609
2025-02-14 16:46:12,304 - Epoch [22/1000], Step [300/4367], Loss: 0.0632
2025-02-14 16:46:47,082 - Epoch [22/1000], Step [400/4367], Loss: 0.2849
2025-02-14 16:47:21,379 - Epoch [22/1000], Step [500/4367], Loss: 0.1022
2025-02-14 16:47:56,054 - Epoch [22/1000], Step [600/4367], Loss: 0.0830
2025-02-14 16:48:30,606 - Epoch [22/1000], Step [700/4367], Loss: 0.0239
2025-02-14 16:49:05,043 - Epoch [22/1000], Step [800/4367], Loss: 0.1047
2025-02-14 16:49:39,901 - Epoch [22/1000], Step [900/4367], Loss: 0.3099
2025-02-14 16:50:14,432 - Epoch [22/1000], Step [1000/4367], Loss: 0.2016
2025-02-14 16:50:49,333 - Epoch [22/1000], Step [1100/4367], Loss: 0.1981
2025-02-14 16:51:24,007 - Epoch [22/1000], Step [1200/4367], Loss: 0.1812
2025-02-14 16:51:58,199 - Epoch [22/1000], Step [1300/4367], Loss: 0.2916
2025-02-14 16:52:33,057 - Epoch [22/1000], Step [1400/4367], Loss: 0.3500
2025-02-14 16:53:07,721 - Epoch [22/1000], Step [1500/4367], Loss: 0.2080
2025-02-14 16:53:42,878 - Epoch [22/1000], Step [1600/4367], Loss: 0.2524
2025-02-14 16:54:17,928 - Epoch [22/1000], Step [1700/4367], Loss: 0.1241
2025-02-14 16:54:52,476 - Epoch [22/1000], Step [1800/4367], Loss: 0.1188
2025-02-14 16:55:27,190 - Epoch [22/1000], Step [1900/4367], Loss: 0.5572
2025-02-14 16:56:02,131 - Epoch [22/1000], Step [2000/4367], Loss: 0.3711
2025-02-14 16:56:37,062 - Epoch [22/1000], Step [2100/4367], Loss: 0.7487
2025-02-14 16:57:11,916 - Epoch [22/1000], Step [2200/4367], Loss: 0.4778
2025-02-14 16:57:46,194 - Epoch [22/1000], Step [2300/4367], Loss: 0.2089
2025-02-14 16:58:21,309 - Epoch [22/1000], Step [2400/4367], Loss: 0.4310
2025-02-14 16:58:56,002 - Epoch [22/1000], Step [2500/4367], Loss: 0.3735
2025-02-14 16:59:30,395 - Epoch [22/1000], Step [2600/4367], Loss: 0.1852
2025-02-14 17:00:05,292 - Epoch [22/1000], Step [2700/4367], Loss: 0.2369
2025-02-14 17:00:40,219 - Epoch [22/1000], Step [2800/4367], Loss: 1.9480
2025-02-14 17:01:15,354 - Epoch [22/1000], Step [2900/4367], Loss: 0.5869
2025-02-14 17:01:50,498 - Epoch [22/1000], Step [3000/4367], Loss: 0.6321
2025-02-14 17:02:25,314 - Epoch [22/1000], Step [3100/4367], Loss: 0.7997
2025-02-14 17:02:59,878 - Epoch [22/1000], Step [3200/4367], Loss: 0.3679
2025-02-14 17:03:34,417 - Epoch [22/1000], Step [3300/4367], Loss: 0.4329
2025-02-14 17:04:09,153 - Epoch [22/1000], Step [3400/4367], Loss: 0.7016
2025-02-14 17:04:43,798 - Epoch [22/1000], Step [3500/4367], Loss: 0.9484
2025-02-14 17:05:18,719 - Epoch [22/1000], Step [3600/4367], Loss: 0.8486
2025-02-14 17:05:53,465 - Epoch [22/1000], Step [3700/4367], Loss: 0.5426
2025-02-14 17:06:28,073 - Epoch [22/1000], Step [3800/4367], Loss: 0.4239
2025-02-14 17:07:03,010 - Epoch [22/1000], Step [3900/4367], Loss: 0.5491
2025-02-14 17:07:37,829 - Epoch [22/1000], Step [4000/4367], Loss: 0.4976
2025-02-14 17:08:12,590 - Epoch [22/1000], Step [4100/4367], Loss: 0.2799
2025-02-14 17:08:47,116 - Epoch [22/1000], Step [4200/4367], Loss: 0.5402
2025-02-14 17:09:22,065 - Epoch [22/1000], Step [4300/4367], Loss: 0.3570
2025-02-14 17:09:54,732 - Epoch [22/1000], Validation Step [100/1090], Val Loss: 0.0001
2025-02-14 17:10:03,930 - Epoch [22/1000], Validation Step [200/1090], Val Loss: 0.0000
2025-02-14 17:10:13,266 - Epoch [22/1000], Validation Step [300/1090], Val Loss: 0.4915
2025-02-14 17:10:22,927 - Epoch [22/1000], Validation Step [400/1090], Val Loss: 0.4833
2025-02-14 17:10:32,043 - Epoch [22/1000], Validation Step [500/1090], Val Loss: 0.8124
2025-02-14 17:10:41,602 - Epoch [22/1000], Validation Step [600/1090], Val Loss: 0.5555
2025-02-14 17:10:51,169 - Epoch [22/1000], Validation Step [700/1090], Val Loss: 0.1981
2025-02-14 17:10:59,938 - Epoch [22/1000], Validation Step [800/1090], Val Loss: 0.2776
2025-02-14 17:11:08,489 - Epoch [22/1000], Validation Step [900/1090], Val Loss: 0.4588
2025-02-14 17:11:17,742 - Epoch [22/1000], Validation Step [1000/1090], Val Loss: 0.0020
2025-02-14 17:11:26,384 - Epoch 22/1000, Train Loss: 0.4146, Val Loss: 0.4354, Accuracy: 83.59%
2025-02-14 17:11:26,831 - Model saved at /data2/personal/sungjin/korean_dialects/checkpoint_real/checkpoint_epoch_22.pth
2025-02-14 17:12:02,751 - Epoch [23/1000], Step [100/4367], Loss: 0.3486
2025-02-14 17:12:37,426 - Epoch [23/1000], Step [200/4367], Loss: 0.3990
2025-02-14 17:13:12,789 - Epoch [23/1000], Step [300/4367], Loss: 0.4001
2025-02-14 17:13:47,347 - Epoch [23/1000], Step [400/4367], Loss: 0.3190
2025-02-14 17:14:22,248 - Epoch [23/1000], Step [500/4367], Loss: 0.4076
2025-02-14 17:14:57,012 - Epoch [23/1000], Step [600/4367], Loss: 0.2100
2025-02-14 17:15:31,607 - Epoch [23/1000], Step [700/4367], Loss: 0.3358
2025-02-14 17:16:05,890 - Epoch [23/1000], Step [800/4367], Loss: 0.4418
2025-02-14 17:16:40,702 - Epoch [23/1000], Step [900/4367], Loss: 0.4555
2025-02-14 17:17:15,834 - Epoch [23/1000], Step [1000/4367], Loss: 0.3580
2025-02-14 17:17:50,423 - Epoch [23/1000], Step [1100/4367], Loss: 0.2907
2025-02-14 17:18:24,681 - Epoch [23/1000], Step [1200/4367], Loss: 0.2491
2025-02-14 17:18:59,634 - Epoch [23/1000], Step [1300/4367], Loss: 0.2919
2025-02-14 17:19:34,415 - Epoch [23/1000], Step [1400/4367], Loss: 0.4836
2025-02-14 17:20:09,192 - Epoch [23/1000], Step [1500/4367], Loss: 0.3147
2025-02-14 17:20:43,977 - Epoch [23/1000], Step [1600/4367], Loss: 0.1884
2025-02-14 17:21:18,608 - Epoch [23/1000], Step [1700/4367], Loss: 0.2011
2025-02-14 17:21:53,335 - Epoch [23/1000], Step [1800/4367], Loss: 0.2387
2025-02-14 17:22:28,044 - Epoch [23/1000], Step [1900/4367], Loss: 0.2005
2025-02-14 17:23:02,636 - Epoch [23/1000], Step [2000/4367], Loss: 0.1730
2025-02-14 17:23:37,304 - Epoch [23/1000], Step [2100/4367], Loss: 0.2698
2025-02-14 17:24:12,085 - Epoch [23/1000], Step [2200/4367], Loss: 0.2911
2025-02-14 17:24:46,650 - Epoch [23/1000], Step [2300/4367], Loss: 0.2028
2025-02-14 17:25:21,405 - Epoch [23/1000], Step [2400/4367], Loss: 0.1290
2025-02-14 17:25:55,669 - Epoch [23/1000], Step [2500/4367], Loss: 0.3690
2025-02-14 17:26:30,387 - Epoch [23/1000], Step [2600/4367], Loss: 0.3216
2025-02-14 17:27:04,601 - Epoch [23/1000], Step [2700/4367], Loss: 0.1249
2025-02-14 17:27:39,461 - Epoch [23/1000], Step [2800/4367], Loss: 0.1499
2025-02-14 17:28:14,139 - Epoch [23/1000], Step [2900/4367], Loss: 0.3646
2025-02-14 17:28:48,651 - Epoch [23/1000], Step [3000/4367], Loss: 0.1877
2025-02-14 17:29:22,877 - Epoch [23/1000], Step [3100/4367], Loss: 0.2143
2025-02-14 17:29:58,252 - Epoch [23/1000], Step [3200/4367], Loss: 0.1997
2025-02-14 17:30:32,808 - Epoch [23/1000], Step [3300/4367], Loss: 0.2185
2025-02-14 17:31:07,493 - Epoch [23/1000], Step [3400/4367], Loss: 0.3063
2025-02-14 17:31:42,260 - Epoch [23/1000], Step [3500/4367], Loss: 0.1526
2025-02-14 17:32:16,845 - Epoch [23/1000], Step [3600/4367], Loss: 0.1852
2025-02-14 17:32:51,871 - Epoch [23/1000], Step [3700/4367], Loss: 0.2781
2025-02-14 17:33:26,722 - Epoch [23/1000], Step [3800/4367], Loss: 0.1835
2025-02-14 17:34:01,728 - Epoch [23/1000], Step [3900/4367], Loss: 0.4895
2025-02-14 17:34:36,613 - Epoch [23/1000], Step [4000/4367], Loss: 0.2124
2025-02-14 17:35:11,117 - Epoch [23/1000], Step [4100/4367], Loss: 0.1405
2025-02-14 17:35:45,495 - Epoch [23/1000], Step [4200/4367], Loss: 0.2488
2025-02-14 17:36:20,027 - Epoch [23/1000], Step [4300/4367], Loss: 0.3004
2025-02-14 17:36:53,264 - Epoch [23/1000], Validation Step [100/1090], Val Loss: 0.0000
2025-02-14 17:37:02,427 - Epoch [23/1000], Validation Step [200/1090], Val Loss: 0.0000
2025-02-14 17:37:11,723 - Epoch [23/1000], Validation Step [300/1090], Val Loss: 0.3128
2025-02-14 17:37:21,326 - Epoch [23/1000], Validation Step [400/1090], Val Loss: 0.1191
2025-02-14 17:37:30,396 - Epoch [23/1000], Validation Step [500/1090], Val Loss: 0.4493
2025-02-14 17:37:39,884 - Epoch [23/1000], Validation Step [600/1090], Val Loss: 0.1045
2025-02-14 17:37:49,399 - Epoch [23/1000], Validation Step [700/1090], Val Loss: 0.1596
2025-02-14 17:37:58,127 - Epoch [23/1000], Validation Step [800/1090], Val Loss: 0.0247
2025-02-14 17:38:06,623 - Epoch [23/1000], Validation Step [900/1090], Val Loss: 0.0251
2025-02-14 17:38:15,810 - Epoch [23/1000], Validation Step [1000/1090], Val Loss: 0.0002
2025-02-14 17:38:24,398 - Epoch 23/1000, Train Loss: 0.2734, Val Loss: 0.1860, Accuracy: 93.10%
2025-02-14 17:39:00,510 - Epoch [24/1000], Step [100/4367], Loss: 0.1790
2025-02-14 17:39:34,669 - Epoch [24/1000], Step [200/4367], Loss: 0.1935
2025-02-14 17:40:09,008 - Epoch [24/1000], Step [300/4367], Loss: 0.1655
2025-02-14 17:40:43,801 - Epoch [24/1000], Step [400/4367], Loss: 0.3833
2025-02-14 17:41:18,657 - Epoch [24/1000], Step [500/4367], Loss: 0.1712
2025-02-14 17:41:53,321 - Epoch [24/1000], Step [600/4367], Loss: 0.1754
2025-02-14 17:42:27,602 - Epoch [24/1000], Step [700/4367], Loss: 0.1253
2025-02-14 17:43:02,508 - Epoch [24/1000], Step [800/4367], Loss: 0.2693
2025-02-14 17:43:37,296 - Epoch [24/1000], Step [900/4367], Loss: 0.1559
2025-02-14 17:44:11,636 - Epoch [24/1000], Step [1000/4367], Loss: 0.2342
2025-02-14 17:44:46,361 - Epoch [24/1000], Step [1100/4367], Loss: 0.1163
2025-02-14 17:45:20,795 - Epoch [24/1000], Step [1200/4367], Loss: 0.4447
2025-02-14 17:45:55,429 - Epoch [24/1000], Step [1300/4367], Loss: 0.0319
2025-02-14 17:46:30,456 - Epoch [24/1000], Step [1400/4367], Loss: 0.1279
2025-02-14 17:47:04,888 - Epoch [24/1000], Step [1500/4367], Loss: 0.1713
2025-02-14 17:47:39,473 - Epoch [24/1000], Step [1600/4367], Loss: 0.1170
2025-02-14 17:48:14,242 - Epoch [24/1000], Step [1700/4367], Loss: 0.2052
2025-02-14 17:48:48,300 - Epoch [24/1000], Step [1800/4367], Loss: 0.3882
2025-02-14 17:49:23,334 - Epoch [24/1000], Step [1900/4367], Loss: 0.1227
2025-02-14 17:49:58,286 - Epoch [24/1000], Step [2000/4367], Loss: 0.2017
2025-02-14 17:50:32,998 - Epoch [24/1000], Step [2100/4367], Loss: 0.1078
2025-02-14 17:51:07,609 - Epoch [24/1000], Step [2200/4367], Loss: 0.3149
2025-02-14 17:51:42,334 - Epoch [24/1000], Step [2300/4367], Loss: 0.3452
2025-02-14 17:52:16,998 - Epoch [24/1000], Step [2400/4367], Loss: 0.0618
2025-02-14 17:52:51,474 - Epoch [24/1000], Step [2500/4367], Loss: 0.2873
2025-02-14 17:53:26,291 - Epoch [24/1000], Step [2600/4367], Loss: 0.1091
2025-02-14 17:54:01,170 - Epoch [24/1000], Step [2700/4367], Loss: 0.5348
2025-02-14 17:54:36,330 - Epoch [24/1000], Step [2800/4367], Loss: 0.0503
2025-02-14 17:55:11,182 - Epoch [24/1000], Step [2900/4367], Loss: 0.1584
2025-02-14 17:55:46,441 - Epoch [24/1000], Step [3000/4367], Loss: 0.3210
2025-02-14 17:56:21,351 - Epoch [24/1000], Step [3100/4367], Loss: 0.1325
2025-02-14 17:56:56,030 - Epoch [24/1000], Step [3200/4367], Loss: 0.1641
2025-02-14 17:57:31,096 - Epoch [24/1000], Step [3300/4367], Loss: 0.1374
2025-02-14 17:58:05,978 - Epoch [24/1000], Step [3400/4367], Loss: 0.1450
2025-02-14 17:58:40,792 - Epoch [24/1000], Step [3500/4367], Loss: 0.1249
2025-02-14 17:59:15,548 - Epoch [24/1000], Step [3600/4367], Loss: 0.0604
2025-02-14 17:59:50,128 - Epoch [24/1000], Step [3700/4367], Loss: 0.3032
2025-02-14 18:00:24,737 - Epoch [24/1000], Step [3800/4367], Loss: 0.0967
2025-02-14 18:00:58,801 - Epoch [24/1000], Step [3900/4367], Loss: 0.0574
2025-02-14 18:01:33,393 - Epoch [24/1000], Step [4000/4367], Loss: 0.2163
2025-02-14 18:02:07,820 - Epoch [24/1000], Step [4100/4367], Loss: 0.1511
2025-02-14 18:02:42,241 - Epoch [24/1000], Step [4200/4367], Loss: 0.1096
2025-02-14 18:03:17,080 - Epoch [24/1000], Step [4300/4367], Loss: 0.3453
2025-02-14 18:03:49,874 - Epoch [24/1000], Validation Step [100/1090], Val Loss: 0.0000
2025-02-14 18:03:59,072 - Epoch [24/1000], Validation Step [200/1090], Val Loss: 0.0001
2025-02-14 18:04:08,408 - Epoch [24/1000], Validation Step [300/1090], Val Loss: 0.2517
2025-02-14 18:04:18,061 - Epoch [24/1000], Validation Step [400/1090], Val Loss: 0.1876
2025-02-14 18:04:27,162 - Epoch [24/1000], Validation Step [500/1090], Val Loss: 0.6749
2025-02-14 18:04:36,681 - Epoch [24/1000], Validation Step [600/1090], Val Loss: 0.1546
2025-02-14 18:04:46,242 - Epoch [24/1000], Validation Step [700/1090], Val Loss: 0.2055
2025-02-14 18:04:55,025 - Epoch [24/1000], Validation Step [800/1090], Val Loss: 0.0050
2025-02-14 18:05:03,574 - Epoch [24/1000], Validation Step [900/1090], Val Loss: 0.0483
2025-02-14 18:05:12,821 - Epoch [24/1000], Validation Step [1000/1090], Val Loss: 0.0001
2025-02-14 18:05:21,439 - Epoch 24/1000, Train Loss: 0.1779, Val Loss: 0.1865, Accuracy: 93.21%
2025-02-14 18:05:21,873 - Model saved at /data2/personal/sungjin/korean_dialects/checkpoint_real/checkpoint_epoch_24.pth
2025-02-14 18:05:57,633 - Epoch [25/1000], Step [100/4367], Loss: 0.1468
2025-02-14 18:06:32,163 - Epoch [25/1000], Step [200/4367], Loss: 0.1027
2025-02-14 18:07:06,577 - Epoch [25/1000], Step [300/4367], Loss: 0.3062
2025-02-14 18:07:41,445 - Epoch [25/1000], Step [400/4367], Loss: 0.1776
2025-02-14 18:08:16,301 - Epoch [25/1000], Step [500/4367], Loss: 0.0481
2025-02-14 18:08:50,955 - Epoch [25/1000], Step [600/4367], Loss: 0.1591
2025-02-14 18:09:26,010 - Epoch [25/1000], Step [700/4367], Loss: 0.0616
2025-02-14 18:10:00,905 - Epoch [25/1000], Step [800/4367], Loss: 0.4426
2025-02-14 18:10:35,715 - Epoch [25/1000], Step [900/4367], Loss: 0.2462
2025-02-14 18:11:10,507 - Epoch [25/1000], Step [1000/4367], Loss: 0.0942
2025-02-14 18:11:45,394 - Epoch [25/1000], Step [1100/4367], Loss: 0.1040
2025-02-14 18:12:19,962 - Epoch [25/1000], Step [1200/4367], Loss: 0.0501
2025-02-14 18:12:54,592 - Epoch [25/1000], Step [1300/4367], Loss: 0.0627
2025-02-14 18:13:29,574 - Epoch [25/1000], Step [1400/4367], Loss: 0.2128
2025-02-14 18:14:04,731 - Epoch [25/1000], Step [1500/4367], Loss: 0.1402
2025-02-14 18:14:39,112 - Epoch [25/1000], Step [1600/4367], Loss: 0.1110
2025-02-14 18:15:13,425 - Epoch [25/1000], Step [1700/4367], Loss: 0.1394
2025-02-14 18:15:47,632 - Epoch [25/1000], Step [1800/4367], Loss: 0.0541
2025-02-14 18:16:22,759 - Epoch [25/1000], Step [1900/4367], Loss: 0.1229
2025-02-14 18:16:57,359 - Epoch [25/1000], Step [2000/4367], Loss: 0.2425
2025-02-14 18:17:32,191 - Epoch [25/1000], Step [2100/4367], Loss: 0.3689
2025-02-14 18:18:06,532 - Epoch [25/1000], Step [2200/4367], Loss: 0.0953
2025-02-14 18:18:41,420 - Epoch [25/1000], Step [2300/4367], Loss: 0.1372
2025-02-14 18:19:16,684 - Epoch [25/1000], Step [2400/4367], Loss: 0.1766
2025-02-14 18:19:51,339 - Epoch [25/1000], Step [2500/4367], Loss: 0.0667
2025-02-14 18:20:25,889 - Epoch [25/1000], Step [2600/4367], Loss: 0.0230
2025-02-14 18:21:00,337 - Epoch [25/1000], Step [2700/4367], Loss: 0.4381
2025-02-14 18:21:35,039 - Epoch [25/1000], Step [2800/4367], Loss: 0.3549
2025-02-14 18:22:09,778 - Epoch [25/1000], Step [2900/4367], Loss: 0.1020
2025-02-14 18:22:44,533 - Epoch [25/1000], Step [3000/4367], Loss: 0.2889
2025-02-14 18:23:18,965 - Epoch [25/1000], Step [3100/4367], Loss: 0.3291
2025-02-14 18:23:53,908 - Epoch [25/1000], Step [3200/4367], Loss: 0.0677
2025-02-14 18:24:28,705 - Epoch [25/1000], Step [3300/4367], Loss: 0.0882
2025-02-14 18:25:03,506 - Epoch [25/1000], Step [3400/4367], Loss: 0.3400
2025-02-14 18:25:38,178 - Epoch [25/1000], Step [3500/4367], Loss: 0.1330
2025-02-14 18:26:12,393 - Epoch [25/1000], Step [3600/4367], Loss: 0.0717
2025-02-14 18:26:46,653 - Epoch [25/1000], Step [3700/4367], Loss: 0.1589
2025-02-14 18:27:21,397 - Epoch [25/1000], Step [3800/4367], Loss: 0.0889
2025-02-14 18:27:55,731 - Epoch [25/1000], Step [3900/4367], Loss: 0.1273
2025-02-14 18:28:30,548 - Epoch [25/1000], Step [4000/4367], Loss: 0.1720
2025-02-14 18:29:05,095 - Epoch [25/1000], Step [4100/4367], Loss: 0.0444
2025-02-14 18:29:39,759 - Epoch [25/1000], Step [4200/4367], Loss: 0.0542
2025-02-14 18:30:14,815 - Epoch [25/1000], Step [4300/4367], Loss: 0.0322
2025-02-14 18:30:47,597 - Epoch [25/1000], Validation Step [100/1090], Val Loss: 0.0001
2025-02-14 18:30:56,788 - Epoch [25/1000], Validation Step [200/1090], Val Loss: 0.0010
2025-02-14 18:31:06,121 - Epoch [25/1000], Validation Step [300/1090], Val Loss: 0.2620
2025-02-14 18:31:15,775 - Epoch [25/1000], Validation Step [400/1090], Val Loss: 0.0784
2025-02-14 18:31:24,872 - Epoch [25/1000], Validation Step [500/1090], Val Loss: 0.4888
2025-02-14 18:31:34,388 - Epoch [25/1000], Validation Step [600/1090], Val Loss: 0.0645
2025-02-14 18:31:43,943 - Epoch [25/1000], Validation Step [700/1090], Val Loss: 0.1007
2025-02-14 18:31:52,717 - Epoch [25/1000], Validation Step [800/1090], Val Loss: 0.0158
2025-02-14 18:32:01,267 - Epoch [25/1000], Validation Step [900/1090], Val Loss: 0.0223
2025-02-14 18:32:10,522 - Epoch [25/1000], Validation Step [1000/1090], Val Loss: 0.0008
2025-02-14 18:32:19,144 - Epoch 25/1000, Train Loss: 0.1706, Val Loss: 0.1750, Accuracy: 93.54%
2025-02-14 18:32:55,372 - Epoch [26/1000], Step [100/4367], Loss: 0.1201
2025-02-14 18:33:30,154 - Epoch [26/1000], Step [200/4367], Loss: 0.2151
2025-02-14 18:34:04,585 - Epoch [26/1000], Step [300/4367], Loss: 0.1599
2025-02-14 18:34:39,162 - Epoch [26/1000], Step [400/4367], Loss: 0.2088
2025-02-14 18:35:13,742 - Epoch [26/1000], Step [500/4367], Loss: 0.1320
2025-02-14 18:35:48,116 - Epoch [26/1000], Step [600/4367], Loss: 0.2222
2025-02-14 18:36:22,497 - Epoch [26/1000], Step [700/4367], Loss: 0.0802
2025-02-14 18:36:56,918 - Epoch [26/1000], Step [800/4367], Loss: 0.1062
2025-02-14 18:37:31,461 - Epoch [26/1000], Step [900/4367], Loss: 0.1906
2025-02-14 18:38:05,942 - Epoch [26/1000], Step [1000/4367], Loss: 0.0470
2025-02-14 18:38:40,652 - Epoch [26/1000], Step [1100/4367], Loss: 0.0900
2025-02-14 18:39:15,490 - Epoch [26/1000], Step [1200/4367], Loss: 0.0956
2025-02-14 18:39:50,381 - Epoch [26/1000], Step [1300/4367], Loss: 0.2314
2025-02-14 18:40:24,820 - Epoch [26/1000], Step [1400/4367], Loss: 0.2720
2025-02-14 18:40:59,345 - Epoch [26/1000], Step [1500/4367], Loss: 0.1064
2025-02-14 18:41:33,744 - Epoch [26/1000], Step [1600/4367], Loss: 0.0585
2025-02-14 18:42:08,470 - Epoch [26/1000], Step [1700/4367], Loss: 0.1118
2025-02-14 18:42:43,059 - Epoch [26/1000], Step [1800/4367], Loss: 0.0618
2025-02-14 18:43:18,062 - Epoch [26/1000], Step [1900/4367], Loss: 0.0719
2025-02-14 18:43:52,137 - Epoch [26/1000], Step [2000/4367], Loss: 0.1297
2025-02-14 18:44:26,912 - Epoch [26/1000], Step [2100/4367], Loss: 0.1324
2025-02-14 18:45:01,327 - Epoch [26/1000], Step [2200/4367], Loss: 0.0726
2025-02-14 18:45:35,799 - Epoch [26/1000], Step [2300/4367], Loss: 0.0446
2025-02-14 18:46:10,620 - Epoch [26/1000], Step [2400/4367], Loss: 0.1201
2025-02-14 18:46:45,602 - Epoch [26/1000], Step [2500/4367], Loss: 0.1872
2025-02-14 18:47:20,702 - Epoch [26/1000], Step [2600/4367], Loss: 0.2432
2025-02-14 18:47:55,139 - Epoch [26/1000], Step [2700/4367], Loss: 0.2250
2025-02-14 18:48:29,948 - Epoch [26/1000], Step [2800/4367], Loss: 0.1324
2025-02-14 18:49:04,618 - Epoch [26/1000], Step [2900/4367], Loss: 0.1475
2025-02-14 18:49:39,658 - Epoch [26/1000], Step [3000/4367], Loss: 0.3567
2025-02-14 18:50:14,442 - Epoch [26/1000], Step [3100/4367], Loss: 0.0492
2025-02-14 18:50:49,368 - Epoch [26/1000], Step [3200/4367], Loss: 0.3159
2025-02-14 18:51:24,367 - Epoch [26/1000], Step [3300/4367], Loss: 0.3280
2025-02-14 18:51:59,366 - Epoch [26/1000], Step [3400/4367], Loss: 0.1262
2025-02-14 18:52:33,965 - Epoch [26/1000], Step [3500/4367], Loss: 0.1742
2025-02-14 18:53:08,426 - Epoch [26/1000], Step [3600/4367], Loss: 0.1976
2025-02-14 18:53:43,324 - Epoch [26/1000], Step [3700/4367], Loss: 0.0683
2025-02-14 18:54:17,875 - Epoch [26/1000], Step [3800/4367], Loss: 0.2607
2025-02-14 18:54:52,636 - Epoch [26/1000], Step [3900/4367], Loss: 0.1040
2025-02-14 18:55:26,886 - Epoch [26/1000], Step [4000/4367], Loss: 0.3057
2025-02-14 18:56:01,462 - Epoch [26/1000], Step [4100/4367], Loss: 0.2465
2025-02-14 18:56:36,086 - Epoch [26/1000], Step [4200/4367], Loss: 0.0669
2025-02-14 18:57:10,863 - Epoch [26/1000], Step [4300/4367], Loss: 0.3198
2025-02-14 18:57:43,804 - Epoch [26/1000], Validation Step [100/1090], Val Loss: 0.0001
2025-02-14 18:57:53,004 - Epoch [26/1000], Validation Step [200/1090], Val Loss: 0.0003
2025-02-14 18:58:02,342 - Epoch [26/1000], Validation Step [300/1090], Val Loss: 0.2145
2025-02-14 18:58:11,995 - Epoch [26/1000], Validation Step [400/1090], Val Loss: 0.0365
2025-02-14 18:58:21,094 - Epoch [26/1000], Validation Step [500/1090], Val Loss: 0.3603
2025-02-14 18:58:30,625 - Epoch [26/1000], Validation Step [600/1090], Val Loss: 0.2055
2025-02-14 18:58:40,188 - Epoch [26/1000], Validation Step [700/1090], Val Loss: 0.2032
2025-02-14 18:58:48,963 - Epoch [26/1000], Validation Step [800/1090], Val Loss: 0.0114
2025-02-14 18:58:57,520 - Epoch [26/1000], Validation Step [900/1090], Val Loss: 0.0094
2025-02-14 18:59:06,775 - Epoch [26/1000], Validation Step [1000/1090], Val Loss: 0.0000
2025-02-14 18:59:15,395 - Epoch 26/1000, Train Loss: 0.1661, Val Loss: 0.1648, Accuracy: 93.81%
2025-02-14 18:59:15,783 - Model saved at /data2/personal/sungjin/korean_dialects/checkpoint_real/checkpoint_epoch_26.pth
2025-02-14 18:59:51,512 - Epoch [27/1000], Step [100/4367], Loss: 0.3215
2025-02-14 19:00:26,281 - Epoch [27/1000], Step [200/4367], Loss: 0.2066
2025-02-14 19:01:01,187 - Epoch [27/1000], Step [300/4367], Loss: 0.0864
2025-02-14 19:01:36,032 - Epoch [27/1000], Step [400/4367], Loss: 0.1666
2025-02-14 19:02:10,576 - Epoch [27/1000], Step [500/4367], Loss: 0.2998
2025-02-14 19:02:45,200 - Epoch [27/1000], Step [600/4367], Loss: 0.0749
2025-02-14 19:03:19,678 - Epoch [27/1000], Step [700/4367], Loss: 0.2424
2025-02-14 19:03:54,485 - Epoch [27/1000], Step [800/4367], Loss: 0.2972
2025-02-14 19:04:29,241 - Epoch [27/1000], Step [900/4367], Loss: 0.1310
2025-02-14 19:05:04,011 - Epoch [27/1000], Step [1000/4367], Loss: 0.2722
2025-02-14 19:05:38,388 - Epoch [27/1000], Step [1100/4367], Loss: 0.1027
2025-02-14 19:06:12,833 - Epoch [27/1000], Step [1200/4367], Loss: 0.0423
2025-02-14 19:06:47,106 - Epoch [27/1000], Step [1300/4367], Loss: 0.2851
2025-02-14 19:07:21,470 - Epoch [27/1000], Step [1400/4367], Loss: 0.6086
2025-02-14 19:07:56,063 - Epoch [27/1000], Step [1500/4367], Loss: 0.1536
2025-02-14 19:08:30,763 - Epoch [27/1000], Step [1600/4367], Loss: 0.1299
2025-02-14 19:09:05,227 - Epoch [27/1000], Step [1700/4367], Loss: 0.0785
2025-02-14 19:09:39,990 - Epoch [27/1000], Step [1800/4367], Loss: 0.1556
2025-02-14 19:10:14,674 - Epoch [27/1000], Step [1900/4367], Loss: 0.2331
2025-02-14 19:10:48,929 - Epoch [27/1000], Step [2000/4367], Loss: 0.1133
2025-02-14 19:11:23,667 - Epoch [27/1000], Step [2100/4367], Loss: 0.1427
2025-02-14 19:11:58,777 - Epoch [27/1000], Step [2200/4367], Loss: 0.3403
2025-02-14 19:12:33,338 - Epoch [27/1000], Step [2300/4367], Loss: 0.1026
2025-02-14 19:13:07,920 - Epoch [27/1000], Step [2400/4367], Loss: 0.1029
2025-02-14 19:13:42,611 - Epoch [27/1000], Step [2500/4367], Loss: 0.2702
2025-02-14 19:14:17,107 - Epoch [27/1000], Step [2600/4367], Loss: 0.1144
2025-02-14 19:14:51,624 - Epoch [27/1000], Step [2700/4367], Loss: 0.0335
2025-02-14 19:15:26,408 - Epoch [27/1000], Step [2800/4367], Loss: 0.0396
2025-02-14 19:16:01,046 - Epoch [27/1000], Step [2900/4367], Loss: 0.2773
2025-02-14 19:16:35,505 - Epoch [27/1000], Step [3000/4367], Loss: 0.0511
2025-02-14 19:17:09,944 - Epoch [27/1000], Step [3100/4367], Loss: 0.3493
2025-02-14 19:17:44,560 - Epoch [27/1000], Step [3200/4367], Loss: 0.1917
2025-02-14 19:18:19,171 - Epoch [27/1000], Step [3300/4367], Loss: 0.0663
2025-02-14 19:18:54,019 - Epoch [27/1000], Step [3400/4367], Loss: 0.0674
2025-02-14 19:19:28,680 - Epoch [27/1000], Step [3500/4367], Loss: 0.0592
2025-02-14 19:20:03,256 - Epoch [27/1000], Step [3600/4367], Loss: 0.2010
2025-02-14 19:20:37,530 - Epoch [27/1000], Step [3700/4367], Loss: 0.0806
2025-02-14 19:21:12,035 - Epoch [27/1000], Step [3800/4367], Loss: 0.0542
2025-02-14 19:21:46,449 - Epoch [27/1000], Step [3900/4367], Loss: 0.2895
2025-02-14 19:22:21,065 - Epoch [27/1000], Step [4000/4367], Loss: 0.0542
2025-02-14 19:22:56,182 - Epoch [27/1000], Step [4100/4367], Loss: 0.0358
2025-02-14 19:23:30,941 - Epoch [27/1000], Step [4200/4367], Loss: 0.1193
2025-02-14 19:24:05,747 - Epoch [27/1000], Step [4300/4367], Loss: 0.1704
2025-02-14 19:24:39,016 - Epoch [27/1000], Validation Step [100/1090], Val Loss: 0.0000
2025-02-14 19:24:48,216 - Epoch [27/1000], Validation Step [200/1090], Val Loss: 0.0002
2025-02-14 19:24:57,558 - Epoch [27/1000], Validation Step [300/1090], Val Loss: 0.2817
2025-02-14 19:25:07,214 - Epoch [27/1000], Validation Step [400/1090], Val Loss: 0.0506
2025-02-14 19:25:16,314 - Epoch [27/1000], Validation Step [500/1090], Val Loss: 0.2520
2025-02-14 19:25:25,842 - Epoch [27/1000], Validation Step [600/1090], Val Loss: 0.0951
2025-02-14 19:25:35,396 - Epoch [27/1000], Validation Step [700/1090], Val Loss: 0.1471
2025-02-14 19:25:44,177 - Epoch [27/1000], Validation Step [800/1090], Val Loss: 0.0282
2025-02-14 19:25:52,724 - Epoch [27/1000], Validation Step [900/1090], Val Loss: 0.0117
2025-02-14 19:26:01,980 - Epoch [27/1000], Validation Step [1000/1090], Val Loss: 0.0001
2025-02-14 19:26:10,602 - Epoch 27/1000, Train Loss: 0.1656, Val Loss: 0.1663, Accuracy: 93.74%
2025-02-14 19:26:46,325 - Epoch [28/1000], Step [100/4367], Loss: 0.3097
2025-02-14 19:27:21,098 - Epoch [28/1000], Step [200/4367], Loss: 0.1336
2025-02-14 19:27:55,741 - Epoch [28/1000], Step [300/4367], Loss: 0.0643
2025-02-14 19:28:30,489 - Epoch [28/1000], Step [400/4367], Loss: 0.0937
2025-02-14 19:29:05,552 - Epoch [28/1000], Step [500/4367], Loss: 0.5091
2025-02-14 19:29:39,750 - Epoch [28/1000], Step [600/4367], Loss: 0.0756
2025-02-14 19:30:14,509 - Epoch [28/1000], Step [700/4367], Loss: 0.2547
2025-02-14 19:30:48,954 - Epoch [28/1000], Step [800/4367], Loss: 0.0656
2025-02-14 19:31:23,118 - Epoch [28/1000], Step [900/4367], Loss: 0.0906
2025-02-14 19:31:57,829 - Epoch [28/1000], Step [1000/4367], Loss: 0.1197
2025-02-14 19:32:32,252 - Epoch [28/1000], Step [1100/4367], Loss: 0.1558
2025-02-14 19:33:07,146 - Epoch [28/1000], Step [1200/4367], Loss: 0.1466
2025-02-14 19:33:41,627 - Epoch [28/1000], Step [1300/4367], Loss: 0.1338
2025-02-14 19:34:16,445 - Epoch [28/1000], Step [1400/4367], Loss: 0.3006
2025-02-14 19:34:50,881 - Epoch [28/1000], Step [1500/4367], Loss: 0.2033
2025-02-14 19:35:25,773 - Epoch [28/1000], Step [1600/4367], Loss: 0.2523
2025-02-14 19:36:00,393 - Epoch [28/1000], Step [1700/4367], Loss: 0.1072
2025-02-14 19:36:35,345 - Epoch [28/1000], Step [1800/4367], Loss: 0.3342
2025-02-14 19:37:09,645 - Epoch [28/1000], Step [1900/4367], Loss: 0.0769
2025-02-14 19:37:44,256 - Epoch [28/1000], Step [2000/4367], Loss: 0.0994
2025-02-14 19:38:18,968 - Epoch [28/1000], Step [2100/4367], Loss: 0.1552
2025-02-14 19:38:53,124 - Epoch [28/1000], Step [2200/4367], Loss: 0.2259
2025-02-14 19:39:27,192 - Epoch [28/1000], Step [2300/4367], Loss: 0.1025
2025-02-14 19:40:01,755 - Epoch [28/1000], Step [2400/4367], Loss: 0.1636
2025-02-14 19:40:36,475 - Epoch [28/1000], Step [2500/4367], Loss: 0.0635
2025-02-14 19:41:10,914 - Epoch [28/1000], Step [2600/4367], Loss: 0.1200
2025-02-14 19:41:45,714 - Epoch [28/1000], Step [2700/4367], Loss: 0.0577
2025-02-14 19:42:20,154 - Epoch [28/1000], Step [2800/4367], Loss: 0.1862
2025-02-14 19:42:55,173 - Epoch [28/1000], Step [2900/4367], Loss: 0.2874
2025-02-14 19:43:29,813 - Epoch [28/1000], Step [3000/4367], Loss: 0.0990
2025-02-14 19:44:04,512 - Epoch [28/1000], Step [3100/4367], Loss: 0.1004
2025-02-14 19:44:39,041 - Epoch [28/1000], Step [3200/4367], Loss: 0.2986
2025-02-14 19:45:13,613 - Epoch [28/1000], Step [3300/4367], Loss: 0.2177
2025-02-14 19:45:48,226 - Epoch [28/1000], Step [3400/4367], Loss: 0.2700
2025-02-14 19:46:22,870 - Epoch [28/1000], Step [3500/4367], Loss: 0.1418
2025-02-14 19:46:57,368 - Epoch [28/1000], Step [3600/4367], Loss: 0.0246
2025-02-14 19:47:32,184 - Epoch [28/1000], Step [3700/4367], Loss: 0.1627
2025-02-14 19:48:07,142 - Epoch [28/1000], Step [3800/4367], Loss: 0.1458
2025-02-14 19:48:41,652 - Epoch [28/1000], Step [3900/4367], Loss: 0.0266
2025-02-14 19:49:16,329 - Epoch [28/1000], Step [4000/4367], Loss: 0.2739
2025-02-14 19:49:50,578 - Epoch [28/1000], Step [4100/4367], Loss: 0.2258
2025-02-14 19:50:25,757 - Epoch [28/1000], Step [4200/4367], Loss: 0.0770
2025-02-14 19:51:00,984 - Epoch [28/1000], Step [4300/4367], Loss: 0.0863
2025-02-14 19:51:33,868 - Epoch [28/1000], Validation Step [100/1090], Val Loss: 0.0002
2025-02-14 19:51:43,043 - Epoch [28/1000], Validation Step [200/1090], Val Loss: 0.0008
2025-02-14 19:51:52,379 - Epoch [28/1000], Validation Step [300/1090], Val Loss: 0.2002
2025-02-14 19:52:02,035 - Epoch [28/1000], Validation Step [400/1090], Val Loss: 0.0993
2025-02-14 19:52:11,131 - Epoch [28/1000], Validation Step [500/1090], Val Loss: 0.3571
2025-02-14 19:52:20,649 - Epoch [28/1000], Validation Step [600/1090], Val Loss: 0.2063
2025-02-14 19:52:30,201 - Epoch [28/1000], Validation Step [700/1090], Val Loss: 0.1137
2025-02-14 19:52:38,984 - Epoch [28/1000], Validation Step [800/1090], Val Loss: 0.0232
2025-02-14 19:52:47,539 - Epoch [28/1000], Validation Step [900/1090], Val Loss: 0.0168
2025-02-14 19:52:56,777 - Epoch [28/1000], Validation Step [1000/1090], Val Loss: 0.0111
2025-02-14 19:53:05,403 - Epoch 28/1000, Train Loss: 0.1636, Val Loss: 0.1675, Accuracy: 93.74%
2025-02-14 19:53:05,830 - Model saved at /data2/personal/sungjin/korean_dialects/checkpoint_real/checkpoint_epoch_28.pth
2025-02-14 19:53:41,439 - Epoch [29/1000], Step [100/4367], Loss: 0.2450
2025-02-14 19:54:16,363 - Epoch [29/1000], Step [200/4367], Loss: 0.2604
2025-02-14 19:54:51,288 - Epoch [29/1000], Step [300/4367], Loss: 0.1514
2025-02-14 19:55:25,898 - Epoch [29/1000], Step [400/4367], Loss: 0.0612
2025-02-14 19:56:00,346 - Epoch [29/1000], Step [500/4367], Loss: 0.1846
2025-02-14 19:56:35,215 - Epoch [29/1000], Step [600/4367], Loss: 0.2299
2025-02-14 19:57:09,967 - Epoch [29/1000], Step [700/4367], Loss: 0.1933
2025-02-14 19:57:44,754 - Epoch [29/1000], Step [800/4367], Loss: 0.1508
2025-02-14 19:58:19,797 - Epoch [29/1000], Step [900/4367], Loss: 0.1872
2025-02-14 19:58:54,925 - Epoch [29/1000], Step [1000/4367], Loss: 0.1400
2025-02-14 19:59:29,638 - Epoch [29/1000], Step [1100/4367], Loss: 0.1055
2025-02-14 20:00:04,192 - Epoch [29/1000], Step [1200/4367], Loss: 0.2937
2025-02-14 20:00:38,817 - Epoch [29/1000], Step [1300/4367], Loss: 0.1137
2025-02-14 20:01:13,137 - Epoch [29/1000], Step [1400/4367], Loss: 0.4643
2025-02-14 20:01:48,042 - Epoch [29/1000], Step [1500/4367], Loss: 0.1930
2025-02-14 20:02:22,904 - Epoch [29/1000], Step [1600/4367], Loss: 0.1264
2025-02-14 20:02:58,230 - Epoch [29/1000], Step [1700/4367], Loss: 0.1322
2025-02-14 20:03:32,980 - Epoch [29/1000], Step [1800/4367], Loss: 0.2123
2025-02-14 20:04:07,305 - Epoch [29/1000], Step [1900/4367], Loss: 0.4131
2025-02-14 20:04:42,009 - Epoch [29/1000], Step [2000/4367], Loss: 0.0455
2025-02-14 20:05:17,063 - Epoch [29/1000], Step [2100/4367], Loss: 0.0905
2025-02-14 20:05:51,970 - Epoch [29/1000], Step [2200/4367], Loss: 0.2397
2025-02-14 20:06:26,769 - Epoch [29/1000], Step [2300/4367], Loss: 0.2255
2025-02-14 20:07:00,917 - Epoch [29/1000], Step [2400/4367], Loss: 0.0883
2025-02-14 20:07:35,243 - Epoch [29/1000], Step [2500/4367], Loss: 0.0703
2025-02-14 20:08:09,263 - Epoch [29/1000], Step [2600/4367], Loss: 0.0559
2025-02-14 20:08:44,230 - Epoch [29/1000], Step [2700/4367], Loss: 0.1546
2025-02-14 20:09:19,121 - Epoch [29/1000], Step [2800/4367], Loss: 0.0704
2025-02-14 20:09:53,765 - Epoch [29/1000], Step [2900/4367], Loss: 0.2810
2025-02-14 20:10:28,612 - Epoch [29/1000], Step [3000/4367], Loss: 0.1989
2025-02-14 20:11:03,089 - Epoch [29/1000], Step [3100/4367], Loss: 0.0749
2025-02-14 20:11:37,753 - Epoch [29/1000], Step [3200/4367], Loss: 0.1846
2025-02-14 20:12:12,081 - Epoch [29/1000], Step [3300/4367], Loss: 0.1135
2025-02-14 20:12:46,818 - Epoch [29/1000], Step [3400/4367], Loss: 0.1350
2025-02-14 20:13:21,817 - Epoch [29/1000], Step [3500/4367], Loss: 0.0932
2025-02-14 20:13:55,989 - Epoch [29/1000], Step [3600/4367], Loss: 0.2851
2025-02-14 20:14:31,122 - Epoch [29/1000], Step [3700/4367], Loss: 0.0705
2025-02-14 20:15:06,145 - Epoch [29/1000], Step [3800/4367], Loss: 0.1922
2025-02-14 20:15:40,543 - Epoch [29/1000], Step [3900/4367], Loss: 0.3212
2025-02-14 20:16:15,101 - Epoch [29/1000], Step [4000/4367], Loss: 0.0884
2025-02-14 20:16:49,897 - Epoch [29/1000], Step [4100/4367], Loss: 0.0326
2025-02-14 20:17:24,463 - Epoch [29/1000], Step [4200/4367], Loss: 0.2117
2025-02-14 20:17:58,939 - Epoch [29/1000], Step [4300/4367], Loss: 0.1035
2025-02-14 20:18:32,274 - Epoch [29/1000], Validation Step [100/1090], Val Loss: 0.0002
2025-02-14 20:18:41,444 - Epoch [29/1000], Validation Step [200/1090], Val Loss: 0.0014
2025-02-14 20:18:50,760 - Epoch [29/1000], Validation Step [300/1090], Val Loss: 0.2549
2025-02-14 20:19:00,399 - Epoch [29/1000], Validation Step [400/1090], Val Loss: 0.0606
2025-02-14 20:19:09,493 - Epoch [29/1000], Validation Step [500/1090], Val Loss: 0.2501
2025-02-14 20:19:19,008 - Epoch [29/1000], Validation Step [600/1090], Val Loss: 0.0951
2025-02-14 20:19:28,556 - Epoch [29/1000], Validation Step [700/1090], Val Loss: 0.0781
2025-02-14 20:19:37,324 - Epoch [29/1000], Validation Step [800/1090], Val Loss: 0.0440
2025-02-14 20:19:45,872 - Epoch [29/1000], Validation Step [900/1090], Val Loss: 0.0169
2025-02-14 20:19:55,107 - Epoch [29/1000], Validation Step [1000/1090], Val Loss: 0.0682
2025-02-14 20:20:03,720 - Epoch 29/1000, Train Loss: 0.1644, Val Loss: 0.1742, Accuracy: 93.49%
2025-02-14 20:20:39,591 - Epoch [30/1000], Step [100/4367], Loss: 0.1702
2025-02-14 20:21:14,276 - Epoch [30/1000], Step [200/4367], Loss: 0.3318
2025-02-14 20:21:49,076 - Epoch [30/1000], Step [300/4367], Loss: 0.2446
2025-02-14 20:22:24,398 - Epoch [30/1000], Step [400/4367], Loss: 0.1077
2025-02-14 20:22:58,925 - Epoch [30/1000], Step [500/4367], Loss: 0.2232
2025-02-14 20:23:33,136 - Epoch [30/1000], Step [600/4367], Loss: 0.1390
2025-02-14 20:24:07,444 - Epoch [30/1000], Step [700/4367], Loss: 0.0754
2025-02-14 20:24:42,000 - Epoch [30/1000], Step [800/4367], Loss: 0.1878
2025-02-14 20:25:16,483 - Epoch [30/1000], Step [900/4367], Loss: 0.1687
2025-02-14 20:25:51,178 - Epoch [30/1000], Step [1000/4367], Loss: 0.0639
2025-02-14 20:26:25,821 - Epoch [30/1000], Step [1100/4367], Loss: 0.1029
2025-02-14 20:27:00,260 - Epoch [30/1000], Step [1200/4367], Loss: 0.1499
2025-02-14 20:27:34,594 - Epoch [30/1000], Step [1300/4367], Loss: 0.0140
2025-02-14 20:28:09,674 - Epoch [30/1000], Step [1400/4367], Loss: 0.2935
2025-02-14 20:28:44,073 - Epoch [30/1000], Step [1500/4367], Loss: 0.1941
2025-02-14 20:29:18,182 - Epoch [30/1000], Step [1600/4367], Loss: 0.2962
2025-02-14 20:29:52,657 - Epoch [30/1000], Step [1700/4367], Loss: 0.3832
2025-02-14 20:30:27,411 - Epoch [30/1000], Step [1800/4367], Loss: 0.1258
2025-02-14 20:31:02,104 - Epoch [30/1000], Step [1900/4367], Loss: 0.1229
2025-02-14 20:31:36,736 - Epoch [30/1000], Step [2000/4367], Loss: 0.0978
2025-02-14 20:32:11,804 - Epoch [30/1000], Step [2100/4367], Loss: 0.1665
2025-02-14 20:32:46,650 - Epoch [30/1000], Step [2200/4367], Loss: 0.0352
2025-02-14 20:33:21,085 - Epoch [30/1000], Step [2300/4367], Loss: 0.1615
2025-02-14 20:33:55,576 - Epoch [30/1000], Step [2400/4367], Loss: 0.0930
2025-02-14 20:34:30,070 - Epoch [30/1000], Step [2500/4367], Loss: 0.1262
2025-02-14 20:35:04,863 - Epoch [30/1000], Step [2600/4367], Loss: 0.1447
2025-02-14 20:35:39,884 - Epoch [30/1000], Step [2700/4367], Loss: 0.0909
2025-02-14 20:36:14,442 - Epoch [30/1000], Step [2800/4367], Loss: 0.3985
2025-02-14 20:36:48,702 - Epoch [30/1000], Step [2900/4367], Loss: 0.0265
2025-02-14 20:37:23,488 - Epoch [30/1000], Step [3000/4367], Loss: 0.1493
2025-02-14 20:37:57,840 - Epoch [30/1000], Step [3100/4367], Loss: 0.1393
2025-02-14 20:38:33,260 - Epoch [30/1000], Step [3200/4367], Loss: 0.1264
2025-02-14 20:39:07,774 - Epoch [30/1000], Step [3300/4367], Loss: 0.0643
2025-02-14 20:39:42,370 - Epoch [30/1000], Step [3400/4367], Loss: 0.3736
2025-02-14 20:40:16,871 - Epoch [30/1000], Step [3500/4367], Loss: 0.1007
2025-02-14 20:40:51,317 - Epoch [30/1000], Step [3600/4367], Loss: 0.2031
2025-02-14 20:41:25,965 - Epoch [30/1000], Step [3700/4367], Loss: 0.1600
2025-02-14 20:42:00,316 - Epoch [30/1000], Step [3800/4367], Loss: 0.1222
2025-02-14 20:42:35,261 - Epoch [30/1000], Step [3900/4367], Loss: 0.0301
2025-02-14 20:43:10,052 - Epoch [30/1000], Step [4000/4367], Loss: 0.0512
2025-02-14 20:43:44,602 - Epoch [30/1000], Step [4100/4367], Loss: 0.2534
2025-02-14 20:44:19,198 - Epoch [30/1000], Step [4200/4367], Loss: 0.1022
2025-02-14 20:44:53,117 - Epoch [30/1000], Step [4300/4367], Loss: 0.0813
2025-02-14 20:45:25,703 - Epoch [30/1000], Validation Step [100/1090], Val Loss: 0.0084
2025-02-14 20:45:34,889 - Epoch [30/1000], Validation Step [200/1090], Val Loss: 0.0257
2025-02-14 20:45:44,235 - Epoch [30/1000], Validation Step [300/1090], Val Loss: 0.2306
2025-02-14 20:45:53,890 - Epoch [30/1000], Validation Step [400/1090], Val Loss: 0.1350
2025-02-14 20:46:02,991 - Epoch [30/1000], Validation Step [500/1090], Val Loss: 0.3258
2025-02-14 20:46:12,513 - Epoch [30/1000], Validation Step [600/1090], Val Loss: 0.1248
2025-02-14 20:46:22,060 - Epoch [30/1000], Validation Step [700/1090], Val Loss: 0.1313
2025-02-14 20:46:30,830 - Epoch [30/1000], Validation Step [800/1090], Val Loss: 0.0600
2025-02-14 20:46:39,379 - Epoch [30/1000], Validation Step [900/1090], Val Loss: 0.0305
2025-02-14 20:46:48,618 - Epoch [30/1000], Validation Step [1000/1090], Val Loss: 0.0429
2025-02-14 20:46:57,241 - Epoch 30/1000, Train Loss: 0.1616, Val Loss: 0.1854, Accuracy: 93.29%
2025-02-14 20:46:57,671 - Model saved at /data2/personal/sungjin/korean_dialects/checkpoint_real/checkpoint_epoch_30.pth
2025-02-14 20:47:32,668 - Epoch [31/1000], Step [100/4367], Loss: 0.0777
2025-02-14 20:48:07,331 - Epoch [31/1000], Step [200/4367], Loss: 0.3244
2025-02-14 20:48:42,003 - Epoch [31/1000], Step [300/4367], Loss: 0.1717
2025-02-14 20:49:16,798 - Epoch [31/1000], Step [400/4367], Loss: 0.2313
2025-02-14 20:49:51,862 - Epoch [31/1000], Step [500/4367], Loss: 0.2081
2025-02-14 20:50:26,415 - Epoch [31/1000], Step [600/4367], Loss: 0.1422
2025-02-14 20:51:00,917 - Epoch [31/1000], Step [700/4367], Loss: 0.1409
2025-02-14 20:51:35,467 - Epoch [31/1000], Step [800/4367], Loss: 0.1194
2025-02-14 20:52:10,276 - Epoch [31/1000], Step [900/4367], Loss: 0.0594
2025-02-14 20:52:44,874 - Epoch [31/1000], Step [1000/4367], Loss: 0.0338
2025-02-14 20:53:19,589 - Epoch [31/1000], Step [1100/4367], Loss: 0.0479
2025-02-14 20:53:54,261 - Epoch [31/1000], Step [1200/4367], Loss: 0.0973
2025-02-14 20:54:28,267 - Epoch [31/1000], Step [1300/4367], Loss: 0.1812
2025-02-14 20:55:02,775 - Epoch [31/1000], Step [1400/4367], Loss: 0.2570
2025-02-14 20:55:37,094 - Epoch [31/1000], Step [1500/4367], Loss: 0.1530
2025-02-14 20:56:11,867 - Epoch [31/1000], Step [1600/4367], Loss: 0.1451
2025-02-14 20:56:46,580 - Epoch [31/1000], Step [1700/4367], Loss: 0.2561
2025-02-14 20:57:21,309 - Epoch [31/1000], Step [1800/4367], Loss: 0.0342
2025-02-14 20:57:56,346 - Epoch [31/1000], Step [1900/4367], Loss: 0.1051
2025-02-14 20:58:31,045 - Epoch [31/1000], Step [2000/4367], Loss: 0.1776
2025-02-14 20:59:05,652 - Epoch [31/1000], Step [2100/4367], Loss: 0.1144
2025-02-14 20:59:40,120 - Epoch [31/1000], Step [2200/4367], Loss: 0.1818
2025-02-14 21:00:14,491 - Epoch [31/1000], Step [2300/4367], Loss: 0.0502
2025-02-14 21:00:49,154 - Epoch [31/1000], Step [2400/4367], Loss: 0.1172
2025-02-14 21:01:23,716 - Epoch [31/1000], Step [2500/4367], Loss: 0.0969
2025-02-14 21:01:58,458 - Epoch [31/1000], Step [2600/4367], Loss: 0.0193
2025-02-14 21:02:32,735 - Epoch [31/1000], Step [2700/4367], Loss: 0.3456
2025-02-14 21:03:07,311 - Epoch [31/1000], Step [2800/4367], Loss: 0.1955
2025-02-14 21:03:42,076 - Epoch [31/1000], Step [2900/4367], Loss: 0.3429
2025-02-14 21:04:16,596 - Epoch [31/1000], Step [3000/4367], Loss: 0.1680
2025-02-14 21:04:51,180 - Epoch [31/1000], Step [3100/4367], Loss: 0.1058
2025-02-14 21:05:25,720 - Epoch [31/1000], Step [3200/4367], Loss: 0.1314
2025-02-14 21:06:00,859 - Epoch [31/1000], Step [3300/4367], Loss: 0.2404
2025-02-14 21:06:35,218 - Epoch [31/1000], Step [3400/4367], Loss: 0.1970
2025-02-14 21:07:09,304 - Epoch [31/1000], Step [3500/4367], Loss: 0.2067
2025-02-14 21:07:43,759 - Epoch [31/1000], Step [3600/4367], Loss: 0.1659
2025-02-14 21:08:18,570 - Epoch [31/1000], Step [3700/4367], Loss: 0.1286
2025-02-14 21:08:53,245 - Epoch [31/1000], Step [3800/4367], Loss: 0.1700
2025-02-14 21:09:27,514 - Epoch [31/1000], Step [3900/4367], Loss: 0.0774
2025-02-14 21:10:02,394 - Epoch [31/1000], Step [4000/4367], Loss: 0.1893
2025-02-14 21:10:37,165 - Epoch [31/1000], Step [4100/4367], Loss: 0.1144
2025-02-14 21:11:11,915 - Epoch [31/1000], Step [4200/4367], Loss: 0.2629
2025-02-14 21:11:46,179 - Epoch [31/1000], Step [4300/4367], Loss: 0.2553
2025-02-14 21:12:19,330 - Epoch [31/1000], Validation Step [100/1090], Val Loss: 0.0003
2025-02-14 21:12:28,537 - Epoch [31/1000], Validation Step [200/1090], Val Loss: 0.0024
2025-02-14 21:12:37,871 - Epoch [31/1000], Validation Step [300/1090], Val Loss: 0.2958
2025-02-14 21:12:47,519 - Epoch [31/1000], Validation Step [400/1090], Val Loss: 0.0943
2025-02-14 21:12:56,605 - Epoch [31/1000], Validation Step [500/1090], Val Loss: 0.4112
2025-02-14 21:13:06,150 - Epoch [31/1000], Validation Step [600/1090], Val Loss: 0.0777
2025-02-14 21:13:15,715 - Epoch [31/1000], Validation Step [700/1090], Val Loss: 0.0792
2025-02-14 21:13:24,499 - Epoch [31/1000], Validation Step [800/1090], Val Loss: 0.0157
2025-02-14 21:13:33,046 - Epoch [31/1000], Validation Step [900/1090], Val Loss: 0.0082
2025-02-14 21:13:42,279 - Epoch [31/1000], Validation Step [1000/1090], Val Loss: 0.0018
2025-02-14 21:13:50,909 - Epoch 31/1000, Train Loss: 0.1569, Val Loss: 0.1607, Accuracy: 94.00%
2025-02-14 21:14:26,571 - Epoch [32/1000], Step [100/4367], Loss: 0.1147
2025-02-14 21:15:00,805 - Epoch [32/1000], Step [200/4367], Loss: 0.1093
2025-02-14 21:15:35,338 - Epoch [32/1000], Step [300/4367], Loss: 0.1556
2025-02-14 21:16:09,799 - Epoch [32/1000], Step [400/4367], Loss: 0.1936
2025-02-14 21:16:44,921 - Epoch [32/1000], Step [500/4367], Loss: 0.0976
2025-02-14 21:17:19,541 - Epoch [32/1000], Step [600/4367], Loss: 0.1363
2025-02-14 21:17:53,914 - Epoch [32/1000], Step [700/4367], Loss: 0.0620
2025-02-14 21:18:28,493 - Epoch [32/1000], Step [800/4367], Loss: 0.0875
2025-02-14 21:19:03,188 - Epoch [32/1000], Step [900/4367], Loss: 0.2647
2025-02-14 21:19:37,558 - Epoch [32/1000], Step [1000/4367], Loss: 0.1104
2025-02-14 21:20:11,940 - Epoch [32/1000], Step [1100/4367], Loss: 0.1049
2025-02-14 21:20:46,636 - Epoch [32/1000], Step [1200/4367], Loss: 0.2812
2025-02-14 21:21:21,652 - Epoch [32/1000], Step [1300/4367], Loss: 0.1904
2025-02-14 21:21:56,717 - Epoch [32/1000], Step [1400/4367], Loss: 0.0718
2025-02-14 21:22:31,541 - Epoch [32/1000], Step [1500/4367], Loss: 0.1325
2025-02-14 21:23:06,102 - Epoch [32/1000], Step [1600/4367], Loss: 0.1635
2025-02-14 21:23:41,103 - Epoch [32/1000], Step [1700/4367], Loss: 0.0845
2025-02-14 21:24:16,042 - Epoch [32/1000], Step [1800/4367], Loss: 0.1222
2025-02-14 21:24:50,223 - Epoch [32/1000], Step [1900/4367], Loss: 0.2023
2025-02-14 21:25:25,051 - Epoch [32/1000], Step [2000/4367], Loss: 0.0676
2025-02-14 21:25:59,979 - Epoch [32/1000], Step [2100/4367], Loss: 0.1689
2025-02-14 21:26:34,806 - Epoch [32/1000], Step [2200/4367], Loss: 0.0937
2025-02-14 21:27:09,419 - Epoch [32/1000], Step [2300/4367], Loss: 0.0092
2025-02-14 21:27:43,641 - Epoch [32/1000], Step [2400/4367], Loss: 0.0669
2025-02-14 21:28:18,609 - Epoch [32/1000], Step [2500/4367], Loss: 0.0971
2025-02-14 21:28:53,322 - Epoch [32/1000], Step [2600/4367], Loss: 0.0765
2025-02-14 21:29:27,717 - Epoch [32/1000], Step [2700/4367], Loss: 0.3103
2025-02-14 21:30:02,773 - Epoch [32/1000], Step [2800/4367], Loss: 0.2352
2025-02-14 21:30:37,485 - Epoch [32/1000], Step [2900/4367], Loss: 0.1872
2025-02-14 21:31:12,110 - Epoch [32/1000], Step [3000/4367], Loss: 0.1097
2025-02-14 21:31:46,567 - Epoch [32/1000], Step [3100/4367], Loss: 0.0918
2025-02-14 21:32:21,384 - Epoch [32/1000], Step [3200/4367], Loss: 0.1128
2025-02-14 21:32:56,394 - Epoch [32/1000], Step [3300/4367], Loss: 0.1289
2025-02-14 21:33:30,935 - Epoch [32/1000], Step [3400/4367], Loss: 0.1212
2025-02-14 21:34:05,512 - Epoch [32/1000], Step [3500/4367], Loss: 0.2685
2025-02-14 21:34:39,780 - Epoch [32/1000], Step [3600/4367], Loss: 0.3699
2025-02-14 21:35:14,322 - Epoch [32/1000], Step [3700/4367], Loss: 0.0285
2025-02-14 21:35:49,453 - Epoch [32/1000], Step [3800/4367], Loss: 0.1854
2025-02-14 21:36:24,154 - Epoch [32/1000], Step [3900/4367], Loss: 0.1715
2025-02-14 21:36:58,367 - Epoch [32/1000], Step [4000/4367], Loss: 0.2518
2025-02-14 21:37:32,726 - Epoch [32/1000], Step [4100/4367], Loss: 0.1474
2025-02-14 21:38:08,087 - Epoch [32/1000], Step [4200/4367], Loss: 0.1561
2025-02-14 21:38:42,893 - Epoch [32/1000], Step [4300/4367], Loss: 0.1037
2025-02-14 21:39:16,147 - Epoch [32/1000], Validation Step [100/1090], Val Loss: 0.0002
2025-02-14 21:39:25,345 - Epoch [32/1000], Validation Step [200/1090], Val Loss: 0.0003
2025-02-14 21:39:34,678 - Epoch [32/1000], Validation Step [300/1090], Val Loss: 0.2700
2025-02-14 21:39:44,335 - Epoch [32/1000], Validation Step [400/1090], Val Loss: 0.1030
2025-02-14 21:39:53,438 - Epoch [32/1000], Validation Step [500/1090], Val Loss: 0.4528
2025-02-14 21:40:02,959 - Epoch [32/1000], Validation Step [600/1090], Val Loss: 0.1295
2025-02-14 21:40:12,518 - Epoch [32/1000], Validation Step [700/1090], Val Loss: 0.1870
2025-02-14 21:40:21,293 - Epoch [32/1000], Validation Step [800/1090], Val Loss: 0.0070
2025-02-14 21:40:29,857 - Epoch [32/1000], Validation Step [900/1090], Val Loss: 0.0043
2025-02-14 21:40:39,108 - Epoch [32/1000], Validation Step [1000/1090], Val Loss: 0.0027
2025-02-14 21:40:47,750 - Epoch 32/1000, Train Loss: 0.1552, Val Loss: 0.1554, Accuracy: 94.28%
2025-02-14 21:40:48,174 - Model saved at /data2/personal/sungjin/korean_dialects/checkpoint_real/checkpoint_epoch_32.pth
2025-02-14 21:41:23,834 - Epoch [33/1000], Step [100/4367], Loss: 0.0718
2025-02-14 21:41:58,805 - Epoch [33/1000], Step [200/4367], Loss: 0.1521
2025-02-14 21:42:33,566 - Epoch [33/1000], Step [300/4367], Loss: 0.2751
2025-02-14 21:43:07,974 - Epoch [33/1000], Step [400/4367], Loss: 0.1824
2025-02-14 21:43:42,365 - Epoch [33/1000], Step [500/4367], Loss: 0.0889
2025-02-14 21:44:17,030 - Epoch [33/1000], Step [600/4367], Loss: 0.1571
2025-02-14 21:44:51,874 - Epoch [33/1000], Step [700/4367], Loss: 0.1675
2025-02-14 21:45:26,603 - Epoch [33/1000], Step [800/4367], Loss: 0.2466
2025-02-14 21:46:00,553 - Epoch [33/1000], Step [900/4367], Loss: 0.2261
2025-02-14 21:46:35,333 - Epoch [33/1000], Step [1000/4367], Loss: 0.3101
2025-02-14 21:47:09,949 - Epoch [33/1000], Step [1100/4367], Loss: 0.1044
2025-02-14 21:47:44,763 - Epoch [33/1000], Step [1200/4367], Loss: 0.1020
2025-02-14 21:48:19,566 - Epoch [33/1000], Step [1300/4367], Loss: 0.0899
2025-02-14 21:48:54,585 - Epoch [33/1000], Step [1400/4367], Loss: 0.1476
2025-02-14 21:49:29,162 - Epoch [33/1000], Step [1500/4367], Loss: 0.1712
2025-02-14 21:50:03,683 - Epoch [33/1000], Step [1600/4367], Loss: 0.1160
2025-02-14 21:50:38,167 - Epoch [33/1000], Step [1700/4367], Loss: 0.1891
2025-02-14 21:51:13,001 - Epoch [33/1000], Step [1800/4367], Loss: 0.2430
2025-02-14 21:51:47,474 - Epoch [33/1000], Step [1900/4367], Loss: 0.0656
2025-02-14 21:52:22,120 - Epoch [33/1000], Step [2000/4367], Loss: 0.1812
2025-02-14 21:52:56,732 - Epoch [33/1000], Step [2100/4367], Loss: 0.0554
2025-02-14 21:53:31,611 - Epoch [33/1000], Step [2200/4367], Loss: 0.1432
2025-02-14 21:54:06,195 - Epoch [33/1000], Step [2300/4367], Loss: 0.2330
2025-02-14 21:54:41,020 - Epoch [33/1000], Step [2400/4367], Loss: 0.0627
2025-02-14 21:55:15,216 - Epoch [33/1000], Step [2500/4367], Loss: 0.1786
2025-02-14 21:55:49,970 - Epoch [33/1000], Step [2600/4367], Loss: 0.1530
2025-02-14 21:56:24,105 - Epoch [33/1000], Step [2700/4367], Loss: 0.0677
2025-02-14 21:56:58,736 - Epoch [33/1000], Step [2800/4367], Loss: 0.1284
2025-02-14 21:57:33,419 - Epoch [33/1000], Step [2900/4367], Loss: 0.0776
2025-02-14 21:58:07,655 - Epoch [33/1000], Step [3000/4367], Loss: 0.0998
2025-02-14 21:58:42,540 - Epoch [33/1000], Step [3100/4367], Loss: 0.0925
2025-02-14 21:59:16,826 - Epoch [33/1000], Step [3200/4367], Loss: 0.2342
2025-02-14 21:59:51,811 - Epoch [33/1000], Step [3300/4367], Loss: 0.1085
2025-02-14 22:00:25,675 - Epoch [33/1000], Step [3400/4367], Loss: 0.1232
2025-02-14 22:01:00,153 - Epoch [33/1000], Step [3500/4367], Loss: 0.1740
2025-02-14 22:01:35,229 - Epoch [33/1000], Step [3600/4367], Loss: 0.1828
2025-02-14 22:02:10,054 - Epoch [33/1000], Step [3700/4367], Loss: 0.1967
2025-02-14 22:02:44,494 - Epoch [33/1000], Step [3800/4367], Loss: 0.4348
2025-02-14 22:03:19,456 - Epoch [33/1000], Step [3900/4367], Loss: 0.1393
2025-02-14 22:03:54,313 - Epoch [33/1000], Step [4000/4367], Loss: 0.1667
2025-02-14 22:04:29,156 - Epoch [33/1000], Step [4100/4367], Loss: 0.0559
2025-02-14 22:05:03,929 - Epoch [33/1000], Step [4200/4367], Loss: 0.2020
2025-02-14 22:05:38,812 - Epoch [33/1000], Step [4300/4367], Loss: 0.2049
2025-02-14 22:06:11,432 - Epoch [33/1000], Validation Step [100/1090], Val Loss: 0.0001
2025-02-14 22:06:20,614 - Epoch [33/1000], Validation Step [200/1090], Val Loss: 0.0000
2025-02-14 22:06:29,950 - Epoch [33/1000], Validation Step [300/1090], Val Loss: 0.2600
2025-02-14 22:06:39,615 - Epoch [33/1000], Validation Step [400/1090], Val Loss: 0.0751
2025-02-14 22:06:48,710 - Epoch [33/1000], Validation Step [500/1090], Val Loss: 0.4132
2025-02-14 22:06:58,220 - Epoch [33/1000], Validation Step [600/1090], Val Loss: 0.1066
2025-02-14 22:07:07,776 - Epoch [33/1000], Validation Step [700/1090], Val Loss: 0.1534
2025-02-14 22:07:16,540 - Epoch [33/1000], Validation Step [800/1090], Val Loss: 0.0098
2025-02-14 22:07:25,082 - Epoch [33/1000], Validation Step [900/1090], Val Loss: 0.0081
2025-02-14 22:07:34,317 - Epoch [33/1000], Validation Step [1000/1090], Val Loss: 0.0007
2025-02-14 22:07:42,937 - Epoch 33/1000, Train Loss: 0.1553, Val Loss: 0.1561, Accuracy: 94.21%
2025-02-14 22:08:18,590 - Epoch [34/1000], Step [100/4367], Loss: 0.0472
2025-02-14 22:08:53,066 - Epoch [34/1000], Step [200/4367], Loss: 0.0483
2025-02-14 22:09:27,794 - Epoch [34/1000], Step [300/4367], Loss: 0.2473
2025-02-14 22:10:02,502 - Epoch [34/1000], Step [400/4367], Loss: 0.3258
2025-02-14 22:10:37,050 - Epoch [34/1000], Step [500/4367], Loss: 0.1822
2025-02-14 22:11:11,790 - Epoch [34/1000], Step [600/4367], Loss: 0.1519
2025-02-14 22:11:46,593 - Epoch [34/1000], Step [700/4367], Loss: 0.1089
2025-02-14 22:12:20,938 - Epoch [34/1000], Step [800/4367], Loss: 0.4123
2025-02-14 22:12:55,475 - Epoch [34/1000], Step [900/4367], Loss: 0.0586
2025-02-14 22:13:30,082 - Epoch [34/1000], Step [1000/4367], Loss: 0.2037
2025-02-14 22:14:04,959 - Epoch [34/1000], Step [1100/4367], Loss: 0.1695
2025-02-14 22:14:39,543 - Epoch [34/1000], Step [1200/4367], Loss: 0.0582
2025-02-14 22:15:14,271 - Epoch [34/1000], Step [1300/4367], Loss: 0.2350
2025-02-14 22:15:48,624 - Epoch [34/1000], Step [1400/4367], Loss: 0.0700
2025-02-14 22:16:23,009 - Epoch [34/1000], Step [1500/4367], Loss: 0.0834
2025-02-14 22:16:57,364 - Epoch [34/1000], Step [1600/4367], Loss: 0.2014
2025-02-14 22:17:32,305 - Epoch [34/1000], Step [1700/4367], Loss: 0.1066
2025-02-14 22:18:07,096 - Epoch [34/1000], Step [1800/4367], Loss: 0.1955
2025-02-14 22:18:41,700 - Epoch [34/1000], Step [1900/4367], Loss: 0.1288
2025-02-14 22:19:16,544 - Epoch [34/1000], Step [2000/4367], Loss: 0.2032
2025-02-14 22:19:51,173 - Epoch [34/1000], Step [2100/4367], Loss: 0.1123
2025-02-14 22:20:25,601 - Epoch [34/1000], Step [2200/4367], Loss: 0.1794
2025-02-14 22:20:59,892 - Epoch [34/1000], Step [2300/4367], Loss: 0.1692
2025-02-14 22:21:35,083 - Epoch [34/1000], Step [2400/4367], Loss: 0.1450
2025-02-14 22:22:10,081 - Epoch [34/1000], Step [2500/4367], Loss: 0.0554
2025-02-14 22:22:44,599 - Epoch [34/1000], Step [2600/4367], Loss: 0.1886
2025-02-14 22:23:19,006 - Epoch [34/1000], Step [2700/4367], Loss: 0.0589
2025-02-14 22:23:53,865 - Epoch [34/1000], Step [2800/4367], Loss: 0.1483
2025-02-14 22:24:28,612 - Epoch [34/1000], Step [2900/4367], Loss: 0.0908
2025-02-14 22:25:03,263 - Epoch [34/1000], Step [3000/4367], Loss: 0.2314
2025-02-14 22:25:37,312 - Epoch [34/1000], Step [3100/4367], Loss: 0.1278
2025-02-14 22:26:12,118 - Epoch [34/1000], Step [3200/4367], Loss: 0.0569
2025-02-14 22:26:47,122 - Epoch [34/1000], Step [3300/4367], Loss: 0.1616
2025-02-14 22:27:21,703 - Epoch [34/1000], Step [3400/4367], Loss: 0.1159
2025-02-14 22:27:56,314 - Epoch [34/1000], Step [3500/4367], Loss: 0.1479
2025-02-14 22:28:31,404 - Epoch [34/1000], Step [3600/4367], Loss: 0.0303
2025-02-14 22:29:06,211 - Epoch [34/1000], Step [3700/4367], Loss: 0.1102
2025-02-14 22:29:41,074 - Epoch [34/1000], Step [3800/4367], Loss: 0.0943
2025-02-14 22:30:16,016 - Epoch [34/1000], Step [3900/4367], Loss: 0.1711
2025-02-14 22:30:50,612 - Epoch [34/1000], Step [4000/4367], Loss: 0.1948
2025-02-14 22:31:25,094 - Epoch [34/1000], Step [4100/4367], Loss: 0.1393
2025-02-14 22:31:59,300 - Epoch [34/1000], Step [4200/4367], Loss: 0.0678
2025-02-14 22:32:33,775 - Epoch [34/1000], Step [4300/4367], Loss: 0.1837
2025-02-14 22:33:06,526 - Epoch [34/1000], Validation Step [100/1090], Val Loss: 0.0065
2025-02-14 22:33:15,702 - Epoch [34/1000], Validation Step [200/1090], Val Loss: 0.0036
2025-02-14 22:33:25,049 - Epoch [34/1000], Validation Step [300/1090], Val Loss: 0.2980
2025-02-14 22:33:34,701 - Epoch [34/1000], Validation Step [400/1090], Val Loss: 0.0757
2025-02-14 22:33:43,778 - Epoch [34/1000], Validation Step [500/1090], Val Loss: 0.4774
2025-02-14 22:33:53,286 - Epoch [34/1000], Validation Step [600/1090], Val Loss: 0.1056
2025-02-14 22:34:02,837 - Epoch [34/1000], Validation Step [700/1090], Val Loss: 0.1045
2025-02-14 22:34:11,618 - Epoch [34/1000], Validation Step [800/1090], Val Loss: 0.0074
2025-02-14 22:34:20,162 - Epoch [34/1000], Validation Step [900/1090], Val Loss: 0.0059
2025-02-14 22:34:29,405 - Epoch [34/1000], Validation Step [1000/1090], Val Loss: 0.0010
2025-02-14 22:34:38,019 - Epoch 34/1000, Train Loss: 0.1535, Val Loss: 0.1583, Accuracy: 94.15%
2025-02-14 22:34:38,509 - Model saved at /data2/personal/sungjin/korean_dialects/checkpoint_real/checkpoint_epoch_34.pth
2025-02-14 22:35:14,565 - Epoch [35/1000], Step [100/4367], Loss: 0.0723
2025-02-14 22:35:48,965 - Epoch [35/1000], Step [200/4367], Loss: 0.0794
2025-02-14 22:36:23,648 - Epoch [35/1000], Step [300/4367], Loss: 0.1723
2025-02-14 22:36:58,452 - Epoch [35/1000], Step [400/4367], Loss: 0.0268
2025-02-14 22:37:33,411 - Epoch [35/1000], Step [500/4367], Loss: 0.1372
2025-02-14 22:38:08,050 - Epoch [35/1000], Step [600/4367], Loss: 0.1240
2025-02-14 22:38:42,727 - Epoch [35/1000], Step [700/4367], Loss: 0.1400
2025-02-14 22:39:17,361 - Epoch [35/1000], Step [800/4367], Loss: 0.0688
2025-02-14 22:39:51,905 - Epoch [35/1000], Step [900/4367], Loss: 0.0349
2025-02-14 22:40:26,477 - Epoch [35/1000], Step [1000/4367], Loss: 0.0370
2025-02-14 22:41:01,216 - Epoch [35/1000], Step [1100/4367], Loss: 0.2315
2025-02-14 22:41:35,753 - Epoch [35/1000], Step [1200/4367], Loss: 0.1471
2025-02-14 22:42:10,020 - Epoch [35/1000], Step [1300/4367], Loss: 0.2335
2025-02-14 22:42:44,979 - Epoch [35/1000], Step [1400/4367], Loss: 0.2398
2025-02-14 22:43:19,639 - Epoch [35/1000], Step [1500/4367], Loss: 0.1206
2025-02-14 22:43:54,177 - Epoch [35/1000], Step [1600/4367], Loss: 0.0850
2025-02-14 22:44:28,605 - Epoch [35/1000], Step [1700/4367], Loss: 0.2681
2025-02-14 22:45:03,573 - Epoch [35/1000], Step [1800/4367], Loss: 0.0393
2025-02-14 22:45:37,964 - Epoch [35/1000], Step [1900/4367], Loss: 0.1788
2025-02-14 22:46:12,579 - Epoch [35/1000], Step [2000/4367], Loss: 0.1089
2025-02-14 22:46:47,629 - Epoch [35/1000], Step [2100/4367], Loss: 0.3388
2025-02-14 22:47:22,346 - Epoch [35/1000], Step [2200/4367], Loss: 0.0911
2025-02-14 22:47:57,206 - Epoch [35/1000], Step [2300/4367], Loss: 0.1070
2025-02-14 22:48:31,963 - Epoch [35/1000], Step [2400/4367], Loss: 0.2003
2025-02-14 22:49:06,779 - Epoch [35/1000], Step [2500/4367], Loss: 0.1772
2025-02-14 22:49:41,390 - Epoch [35/1000], Step [2600/4367], Loss: 0.0698
2025-02-14 22:50:15,784 - Epoch [35/1000], Step [2700/4367], Loss: 0.2591
2025-02-14 22:50:50,456 - Epoch [35/1000], Step [2800/4367], Loss: 0.3011
2025-02-14 22:51:25,240 - Epoch [35/1000], Step [2900/4367], Loss: 0.0518
2025-02-14 22:52:00,014 - Epoch [35/1000], Step [3000/4367], Loss: 0.0684
2025-02-14 22:52:35,107 - Epoch [35/1000], Step [3100/4367], Loss: 0.0518
2025-02-14 22:53:10,235 - Epoch [35/1000], Step [3200/4367], Loss: 0.2744
2025-02-14 22:53:44,846 - Epoch [35/1000], Step [3300/4367], Loss: 0.2160
2025-02-14 22:54:19,739 - Epoch [35/1000], Step [3400/4367], Loss: 0.1601
2025-02-14 22:54:54,236 - Epoch [35/1000], Step [3500/4367], Loss: 0.1007
2025-02-14 22:55:29,438 - Epoch [35/1000], Step [3600/4367], Loss: 0.2267
2025-02-14 22:56:04,195 - Epoch [35/1000], Step [3700/4367], Loss: 0.1048
2025-02-14 22:56:39,020 - Epoch [35/1000], Step [3800/4367], Loss: 0.2578
2025-02-14 22:57:13,507 - Epoch [35/1000], Step [3900/4367], Loss: 0.1599
2025-02-14 22:57:48,135 - Epoch [35/1000], Step [4000/4367], Loss: 0.3891
2025-02-14 22:58:22,956 - Epoch [35/1000], Step [4100/4367], Loss: 0.1717
2025-02-14 22:58:57,900 - Epoch [35/1000], Step [4200/4367], Loss: 0.0413
2025-02-14 22:59:32,600 - Epoch [35/1000], Step [4300/4367], Loss: 0.2423
2025-02-14 23:00:05,538 - Epoch [35/1000], Validation Step [100/1090], Val Loss: 0.0001
2025-02-14 23:00:14,723 - Epoch [35/1000], Validation Step [200/1090], Val Loss: 0.0006
2025-02-14 23:00:24,067 - Epoch [35/1000], Validation Step [300/1090], Val Loss: 0.3053
2025-02-14 23:00:33,714 - Epoch [35/1000], Validation Step [400/1090], Val Loss: 0.0858
2025-02-14 23:00:42,804 - Epoch [35/1000], Validation Step [500/1090], Val Loss: 0.5049
2025-02-14 23:00:52,321 - Epoch [35/1000], Validation Step [600/1090], Val Loss: 0.1359
2025-02-14 23:01:01,859 - Epoch [35/1000], Validation Step [700/1090], Val Loss: 0.1244
2025-02-14 23:01:10,619 - Epoch [35/1000], Validation Step [800/1090], Val Loss: 0.0055
2025-02-14 23:01:19,156 - Epoch [35/1000], Validation Step [900/1090], Val Loss: 0.0045
2025-02-14 23:01:28,395 - Epoch [35/1000], Validation Step [1000/1090], Val Loss: 0.0056
2025-02-14 23:01:37,005 - Epoch 35/1000, Train Loss: 0.1533, Val Loss: 0.1597, Accuracy: 94.13%
2025-02-14 23:02:12,713 - Epoch [36/1000], Step [100/4367], Loss: 0.1131
2025-02-14 23:02:47,035 - Epoch [36/1000], Step [200/4367], Loss: 0.2191
2025-02-14 23:03:21,874 - Epoch [36/1000], Step [300/4367], Loss: 0.1360
2025-02-14 23:03:56,756 - Epoch [36/1000], Step [400/4367], Loss: 0.4640
2025-02-14 23:04:31,779 - Epoch [36/1000], Step [500/4367], Loss: 0.0581
2025-02-14 23:05:06,483 - Epoch [36/1000], Step [600/4367], Loss: 0.1738
2025-02-14 23:05:41,137 - Epoch [36/1000], Step [700/4367], Loss: 0.0755
2025-02-14 23:06:15,275 - Epoch [36/1000], Step [800/4367], Loss: 0.0827
2025-02-14 23:06:49,646 - Epoch [36/1000], Step [900/4367], Loss: 0.0916
2025-02-14 23:07:24,490 - Epoch [36/1000], Step [1000/4367], Loss: 0.1372
2025-02-14 23:07:59,326 - Epoch [36/1000], Step [1100/4367], Loss: 0.0269
2025-02-14 23:08:34,240 - Epoch [36/1000], Step [1200/4367], Loss: 0.1320
2025-02-14 23:09:08,556 - Epoch [36/1000], Step [1300/4367], Loss: 0.1786
2025-02-14 23:09:43,441 - Epoch [36/1000], Step [1400/4367], Loss: 0.1684
2025-02-14 23:10:18,513 - Epoch [36/1000], Step [1500/4367], Loss: 0.2813
2025-02-14 23:10:53,262 - Epoch [36/1000], Step [1600/4367], Loss: 0.0912
2025-02-14 23:11:28,231 - Epoch [36/1000], Step [1700/4367], Loss: 0.0935
2025-02-14 23:12:02,928 - Epoch [36/1000], Step [1800/4367], Loss: 0.2479
2025-02-14 23:12:37,432 - Epoch [36/1000], Step [1900/4367], Loss: 0.2147
2025-02-14 23:13:12,025 - Epoch [36/1000], Step [2000/4367], Loss: 0.1036
2025-02-14 23:13:46,868 - Epoch [36/1000], Step [2100/4367], Loss: 0.2234
2025-02-14 23:14:21,296 - Epoch [36/1000], Step [2200/4367], Loss: 0.1528
2025-02-14 23:14:56,369 - Epoch [36/1000], Step [2300/4367], Loss: 0.1762
2025-02-14 23:15:30,886 - Epoch [36/1000], Step [2400/4367], Loss: 0.0996
2025-02-14 23:16:05,865 - Epoch [36/1000], Step [2500/4367], Loss: 0.2365
2025-02-14 23:16:40,213 - Epoch [36/1000], Step [2600/4367], Loss: 0.2497
2025-02-14 23:17:15,072 - Epoch [36/1000], Step [2700/4367], Loss: 0.1664
2025-02-14 23:17:49,488 - Epoch [36/1000], Step [2800/4367], Loss: 0.1742
2025-02-14 23:18:24,552 - Epoch [36/1000], Step [2900/4367], Loss: 0.2238
2025-02-14 23:18:59,242 - Epoch [36/1000], Step [3000/4367], Loss: 0.1449
2025-02-14 23:19:33,615 - Epoch [36/1000], Step [3100/4367], Loss: 0.1612
2025-02-14 23:20:08,558 - Epoch [36/1000], Step [3200/4367], Loss: 0.0866
2025-02-14 23:20:43,631 - Epoch [36/1000], Step [3300/4367], Loss: 0.1152
2025-02-14 23:21:18,563 - Epoch [36/1000], Step [3400/4367], Loss: 0.0989
2025-02-14 23:21:53,057 - Epoch [36/1000], Step [3500/4367], Loss: 0.1674
2025-02-14 23:22:27,504 - Epoch [36/1000], Step [3600/4367], Loss: 0.0481
2025-02-14 23:23:02,141 - Epoch [36/1000], Step [3700/4367], Loss: 0.0898
2025-02-14 23:23:36,538 - Epoch [36/1000], Step [3800/4367], Loss: 0.1456
2025-02-14 23:24:10,975 - Epoch [36/1000], Step [3900/4367], Loss: 0.1511
2025-02-14 23:24:45,956 - Epoch [36/1000], Step [4000/4367], Loss: 0.1922
2025-02-14 23:25:20,158 - Epoch [36/1000], Step [4100/4367], Loss: 0.1825
2025-02-14 23:25:54,759 - Epoch [36/1000], Step [4200/4367], Loss: 0.1428
2025-02-14 23:26:29,264 - Epoch [36/1000], Step [4300/4367], Loss: 0.1806
2025-02-14 23:27:01,846 - Epoch [36/1000], Validation Step [100/1090], Val Loss: 0.0001
2025-02-14 23:27:10,998 - Epoch [36/1000], Validation Step [200/1090], Val Loss: 0.0004
2025-02-14 23:27:20,287 - Epoch [36/1000], Validation Step [300/1090], Val Loss: 0.3168
2025-02-14 23:27:29,910 - Epoch [36/1000], Validation Step [400/1090], Val Loss: 0.1010
2025-02-14 23:27:38,974 - Epoch [36/1000], Validation Step [500/1090], Val Loss: 0.3585
2025-02-14 23:27:48,457 - Epoch [36/1000], Validation Step [600/1090], Val Loss: 0.0883
2025-02-14 23:27:57,960 - Epoch [36/1000], Validation Step [700/1090], Val Loss: 0.0776
2025-02-14 23:28:06,710 - Epoch [36/1000], Validation Step [800/1090], Val Loss: 0.0100
2025-02-14 23:28:15,268 - Epoch [36/1000], Validation Step [900/1090], Val Loss: 0.0070
2025-02-14 23:28:24,517 - Epoch [36/1000], Validation Step [1000/1090], Val Loss: 0.0099
2025-02-14 23:28:33,137 - Epoch 36/1000, Train Loss: 0.1503, Val Loss: 0.1591, Accuracy: 94.20%
2025-02-14 23:28:33,570 - Model saved at /data2/personal/sungjin/korean_dialects/checkpoint_real/checkpoint_epoch_36.pth
2025-02-14 23:29:09,057 - Epoch [37/1000], Step [100/4367], Loss: 0.2088
2025-02-14 23:29:43,491 - Epoch [37/1000], Step [200/4367], Loss: 0.0676
2025-02-14 23:30:18,311 - Epoch [37/1000], Step [300/4367], Loss: 0.5579
2025-02-14 23:30:53,040 - Epoch [37/1000], Step [400/4367], Loss: 0.3635
2025-02-14 23:31:27,514 - Epoch [37/1000], Step [500/4367], Loss: 0.0540
2025-02-14 23:32:02,060 - Epoch [37/1000], Step [600/4367], Loss: 0.0449
2025-02-14 23:32:36,723 - Epoch [37/1000], Step [700/4367], Loss: 0.2551
2025-02-14 23:33:11,539 - Epoch [37/1000], Step [800/4367], Loss: 0.3248
2025-02-14 23:33:46,389 - Epoch [37/1000], Step [900/4367], Loss: 0.4724
2025-02-14 23:34:20,976 - Epoch [37/1000], Step [1000/4367], Loss: 0.1853
2025-02-14 23:34:55,679 - Epoch [37/1000], Step [1100/4367], Loss: 0.1109
2025-02-14 23:35:30,568 - Epoch [37/1000], Step [1200/4367], Loss: 0.2717
2025-02-14 23:36:05,005 - Epoch [37/1000], Step [1300/4367], Loss: 0.0915
2025-02-14 23:36:39,847 - Epoch [37/1000], Step [1400/4367], Loss: 0.1116
2025-02-14 23:37:14,692 - Epoch [37/1000], Step [1500/4367], Loss: 0.1536
2025-02-14 23:37:49,542 - Epoch [37/1000], Step [1600/4367], Loss: 0.2817
2025-02-14 23:38:24,591 - Epoch [37/1000], Step [1700/4367], Loss: 0.1199
2025-02-14 23:38:59,338 - Epoch [37/1000], Step [1800/4367], Loss: 0.1722
2025-02-14 23:39:34,234 - Epoch [37/1000], Step [1900/4367], Loss: 0.3541
2025-02-14 23:40:08,495 - Epoch [37/1000], Step [2000/4367], Loss: 0.0878
2025-02-14 23:40:43,033 - Epoch [37/1000], Step [2100/4367], Loss: 0.2831
2025-02-14 23:41:17,722 - Epoch [37/1000], Step [2200/4367], Loss: 0.2371
2025-02-14 23:41:52,411 - Epoch [37/1000], Step [2300/4367], Loss: 0.1273
2025-02-14 23:42:27,456 - Epoch [37/1000], Step [2400/4367], Loss: 0.0333
2025-02-14 23:43:02,467 - Epoch [37/1000], Step [2500/4367], Loss: 0.0867
2025-02-14 23:43:37,018 - Epoch [37/1000], Step [2600/4367], Loss: 0.0939
2025-02-14 23:44:11,861 - Epoch [37/1000], Step [2700/4367], Loss: 0.1735
2025-02-14 23:44:46,852 - Epoch [37/1000], Step [2800/4367], Loss: 0.2113
2025-02-14 23:45:21,652 - Epoch [37/1000], Step [2900/4367], Loss: 0.0958
2025-02-14 23:45:56,406 - Epoch [37/1000], Step [3000/4367], Loss: 0.0540
2025-02-14 23:46:30,814 - Epoch [37/1000], Step [3100/4367], Loss: 0.2934
2025-02-14 23:47:05,481 - Epoch [37/1000], Step [3200/4367], Loss: 0.1012
2025-02-14 23:47:40,105 - Epoch [37/1000], Step [3300/4367], Loss: 0.1831
2025-02-14 23:48:15,081 - Epoch [37/1000], Step [3400/4367], Loss: 0.1070
2025-02-14 23:48:49,683 - Epoch [37/1000], Step [3500/4367], Loss: 0.1412
2025-02-14 23:49:24,378 - Epoch [37/1000], Step [3600/4367], Loss: 0.1690
2025-02-14 23:49:58,740 - Epoch [37/1000], Step [3700/4367], Loss: 0.1065
2025-02-14 23:50:33,382 - Epoch [37/1000], Step [3800/4367], Loss: 0.1931
2025-02-14 23:51:08,050 - Epoch [37/1000], Step [3900/4367], Loss: 0.1924
2025-02-14 23:51:42,960 - Epoch [37/1000], Step [4000/4367], Loss: 0.2449
2025-02-14 23:52:17,871 - Epoch [37/1000], Step [4100/4367], Loss: 0.1478
2025-02-14 23:52:52,968 - Epoch [37/1000], Step [4200/4367], Loss: 0.1252
2025-02-14 23:53:27,477 - Epoch [37/1000], Step [4300/4367], Loss: 0.0659
2025-02-14 23:54:00,945 - Epoch [37/1000], Validation Step [100/1090], Val Loss: 0.0003
2025-02-14 23:54:10,143 - Epoch [37/1000], Validation Step [200/1090], Val Loss: 0.0024
2025-02-14 23:54:19,483 - Epoch [37/1000], Validation Step [300/1090], Val Loss: 0.2552
2025-02-14 23:54:29,135 - Epoch [37/1000], Validation Step [400/1090], Val Loss: 0.1108
2025-02-14 23:54:38,247 - Epoch [37/1000], Validation Step [500/1090], Val Loss: 0.5518
2025-02-14 23:54:47,760 - Epoch [37/1000], Validation Step [600/1090], Val Loss: 0.1263
2025-02-14 23:54:57,327 - Epoch [37/1000], Validation Step [700/1090], Val Loss: 0.1302
2025-02-14 23:55:06,111 - Epoch [37/1000], Validation Step [800/1090], Val Loss: 0.0038
2025-02-14 23:55:14,670 - Epoch [37/1000], Validation Step [900/1090], Val Loss: 0.0032
2025-02-14 23:55:23,916 - Epoch [37/1000], Validation Step [1000/1090], Val Loss: 0.0010
2025-02-14 23:55:32,547 - Epoch 37/1000, Train Loss: 0.1472, Val Loss: 0.1541, Accuracy: 94.30%
2025-02-14 23:56:07,929 - Epoch [38/1000], Step [100/4367], Loss: 0.0913
2025-02-14 23:56:42,430 - Epoch [38/1000], Step [200/4367], Loss: 0.2135
2025-02-14 23:57:17,138 - Epoch [38/1000], Step [300/4367], Loss: 0.0716
2025-02-14 23:57:51,633 - Epoch [38/1000], Step [400/4367], Loss: 0.1540
2025-02-14 23:58:26,511 - Epoch [38/1000], Step [500/4367], Loss: 0.0627
2025-02-14 23:59:01,301 - Epoch [38/1000], Step [600/4367], Loss: 0.1248
2025-02-14 23:59:36,199 - Epoch [38/1000], Step [700/4367], Loss: 0.0373
2025-02-15 00:00:11,231 - Epoch [38/1000], Step [800/4367], Loss: 0.0633
2025-02-15 00:00:46,092 - Epoch [38/1000], Step [900/4367], Loss: 0.1087
2025-02-15 00:01:20,875 - Epoch [38/1000], Step [1000/4367], Loss: 0.0782
2025-02-15 00:01:55,500 - Epoch [38/1000], Step [1100/4367], Loss: 0.2310
2025-02-15 00:02:30,315 - Epoch [38/1000], Step [1200/4367], Loss: 0.1528
2025-02-15 00:03:05,308 - Epoch [38/1000], Step [1300/4367], Loss: 0.0468
2025-02-15 00:03:39,689 - Epoch [38/1000], Step [1400/4367], Loss: 0.1758
2025-02-15 00:04:14,529 - Epoch [38/1000], Step [1500/4367], Loss: 0.2149
2025-02-15 00:04:49,248 - Epoch [38/1000], Step [1600/4367], Loss: 0.1482
2025-02-15 00:05:23,902 - Epoch [38/1000], Step [1700/4367], Loss: 0.2146
2025-02-15 00:05:58,594 - Epoch [38/1000], Step [1800/4367], Loss: 0.2334
2025-02-15 00:06:33,483 - Epoch [38/1000], Step [1900/4367], Loss: 0.1381
2025-02-15 00:07:07,970 - Epoch [38/1000], Step [2000/4367], Loss: 0.0527
2025-02-15 00:07:42,576 - Epoch [38/1000], Step [2100/4367], Loss: 0.0771
2025-02-15 00:08:17,655 - Epoch [38/1000], Step [2200/4367], Loss: 0.0641
2025-02-15 00:08:51,884 - Epoch [38/1000], Step [2300/4367], Loss: 0.2792
2025-02-15 00:09:26,154 - Epoch [38/1000], Step [2400/4367], Loss: 0.0615
2025-02-15 00:10:01,116 - Epoch [38/1000], Step [2500/4367], Loss: 0.1017
2025-02-15 00:10:35,830 - Epoch [38/1000], Step [2600/4367], Loss: 0.3199
2025-02-15 00:11:09,875 - Epoch [38/1000], Step [2700/4367], Loss: 0.0844
2025-02-15 00:11:44,822 - Epoch [38/1000], Step [2800/4367], Loss: 0.1471
2025-02-15 00:12:18,963 - Epoch [38/1000], Step [2900/4367], Loss: 0.1215
2025-02-15 00:12:53,304 - Epoch [38/1000], Step [3000/4367], Loss: 0.0536
2025-02-15 00:13:27,873 - Epoch [38/1000], Step [3100/4367], Loss: 0.0342
2025-02-15 00:14:02,595 - Epoch [38/1000], Step [3200/4367], Loss: 0.1164
2025-02-15 00:14:37,207 - Epoch [38/1000], Step [3300/4367], Loss: 0.1245
2025-02-15 00:15:11,680 - Epoch [38/1000], Step [3400/4367], Loss: 0.5333
2025-02-15 00:15:46,152 - Epoch [38/1000], Step [3500/4367], Loss: 0.2790
2025-02-15 00:16:20,952 - Epoch [38/1000], Step [3600/4367], Loss: 0.1014
2025-02-15 00:16:55,846 - Epoch [38/1000], Step [3700/4367], Loss: 0.0919
2025-02-15 00:17:30,337 - Epoch [38/1000], Step [3800/4367], Loss: 0.0825
2025-02-15 00:18:04,856 - Epoch [38/1000], Step [3900/4367], Loss: 0.1720
2025-02-15 00:18:39,633 - Epoch [38/1000], Step [4000/4367], Loss: 0.0441
2025-02-15 00:19:14,708 - Epoch [38/1000], Step [4100/4367], Loss: 0.1712
2025-02-15 00:19:49,337 - Epoch [38/1000], Step [4200/4367], Loss: 0.2303
2025-02-15 00:20:23,909 - Epoch [38/1000], Step [4300/4367], Loss: 0.0922
2025-02-15 00:20:56,446 - Epoch [38/1000], Validation Step [100/1090], Val Loss: 0.0008
2025-02-15 00:21:05,627 - Epoch [38/1000], Validation Step [200/1090], Val Loss: 0.0032
2025-02-15 00:21:14,955 - Epoch [38/1000], Validation Step [300/1090], Val Loss: 0.2498
2025-02-15 00:21:24,587 - Epoch [38/1000], Validation Step [400/1090], Val Loss: 0.1663
2025-02-15 00:21:33,675 - Epoch [38/1000], Validation Step [500/1090], Val Loss: 0.8153
2025-02-15 00:21:43,186 - Epoch [38/1000], Validation Step [600/1090], Val Loss: 0.1948
2025-02-15 00:21:52,722 - Epoch [38/1000], Validation Step [700/1090], Val Loss: 0.2162
2025-02-15 00:22:01,494 - Epoch [38/1000], Validation Step [800/1090], Val Loss: 0.0008
2025-02-15 00:22:10,043 - Epoch [38/1000], Validation Step [900/1090], Val Loss: 0.0008
2025-02-15 00:22:19,288 - Epoch [38/1000], Validation Step [1000/1090], Val Loss: 0.0015
2025-02-15 00:22:27,910 - Epoch 38/1000, Train Loss: 0.1463, Val Loss: 0.1734, Accuracy: 93.78%
2025-02-15 00:22:28,313 - Model saved at /data2/personal/sungjin/korean_dialects/checkpoint_real/checkpoint_epoch_38.pth
2025-02-15 00:23:04,062 - Epoch [39/1000], Step [100/4367], Loss: 0.2422
2025-02-15 00:23:38,284 - Epoch [39/1000], Step [200/4367], Loss: 0.2400
2025-02-15 00:24:12,797 - Epoch [39/1000], Step [300/4367], Loss: 0.0352
2025-02-15 00:24:47,832 - Epoch [39/1000], Step [400/4367], Loss: 0.3798
2025-02-15 00:25:22,532 - Epoch [39/1000], Step [500/4367], Loss: 0.1620
2025-02-15 00:25:57,029 - Epoch [39/1000], Step [600/4367], Loss: 0.1417
2025-02-15 00:26:31,663 - Epoch [39/1000], Step [700/4367], Loss: 0.0463
2025-02-15 00:27:06,401 - Epoch [39/1000], Step [800/4367], Loss: 0.1342
2025-02-15 00:27:41,631 - Epoch [39/1000], Step [900/4367], Loss: 0.1478
2025-02-15 00:28:16,483 - Epoch [39/1000], Step [1000/4367], Loss: 0.5406
2025-02-15 00:28:50,885 - Epoch [39/1000], Step [1100/4367], Loss: 0.0874
2025-02-15 00:29:25,772 - Epoch [39/1000], Step [1200/4367], Loss: 0.1234
2025-02-15 00:30:00,462 - Epoch [39/1000], Step [1300/4367], Loss: 0.0942
2025-02-15 00:30:34,948 - Epoch [39/1000], Step [1400/4367], Loss: 0.1656
2025-02-15 00:31:09,029 - Epoch [39/1000], Step [1500/4367], Loss: 0.0877
2025-02-15 00:31:43,096 - Epoch [39/1000], Step [1600/4367], Loss: 0.1202
2025-02-15 00:32:17,961 - Epoch [39/1000], Step [1700/4367], Loss: 0.1628
2025-02-15 00:32:52,871 - Epoch [39/1000], Step [1800/4367], Loss: 0.3566
2025-02-15 00:33:27,301 - Epoch [39/1000], Step [1900/4367], Loss: 0.1215
2025-02-15 00:34:01,793 - Epoch [39/1000], Step [2000/4367], Loss: 0.1132
2025-02-15 00:34:36,989 - Epoch [39/1000], Step [2100/4367], Loss: 0.0717
2025-02-15 00:35:11,567 - Epoch [39/1000], Step [2200/4367], Loss: 0.1346
2025-02-15 00:35:46,250 - Epoch [39/1000], Step [2300/4367], Loss: 0.1872
2025-02-15 00:36:21,212 - Epoch [39/1000], Step [2400/4367], Loss: 0.1265
2025-02-15 00:36:55,710 - Epoch [39/1000], Step [2500/4367], Loss: 0.0856
2025-02-15 00:37:30,391 - Epoch [39/1000], Step [2600/4367], Loss: 0.1448
2025-02-15 00:38:04,423 - Epoch [39/1000], Step [2700/4367], Loss: 0.1800
2025-02-15 00:38:39,790 - Epoch [39/1000], Step [2800/4367], Loss: 0.0677
2025-02-15 00:39:14,718 - Epoch [39/1000], Step [2900/4367], Loss: 0.2515
2025-02-15 00:39:49,358 - Epoch [39/1000], Step [3000/4367], Loss: 0.1823
2025-02-15 00:40:24,187 - Epoch [39/1000], Step [3100/4367], Loss: 0.0782
2025-02-15 00:40:58,916 - Epoch [39/1000], Step [3200/4367], Loss: 0.2105
2025-02-15 00:41:33,604 - Epoch [39/1000], Step [3300/4367], Loss: 0.1795
2025-02-15 00:42:08,503 - Epoch [39/1000], Step [3400/4367], Loss: 0.0842
2025-02-15 00:42:43,290 - Epoch [39/1000], Step [3500/4367], Loss: 0.0751
2025-02-15 00:43:18,072 - Epoch [39/1000], Step [3600/4367], Loss: 0.1300
2025-02-15 00:43:52,787 - Epoch [39/1000], Step [3700/4367], Loss: 0.0551
2025-02-15 00:44:26,952 - Epoch [39/1000], Step [3800/4367], Loss: 0.4028
2025-02-15 00:45:01,601 - Epoch [39/1000], Step [3900/4367], Loss: 0.1262
2025-02-15 00:45:36,742 - Epoch [39/1000], Step [4000/4367], Loss: 0.2584
2025-02-15 00:46:11,311 - Epoch [39/1000], Step [4100/4367], Loss: 0.0768
2025-02-15 00:46:46,365 - Epoch [39/1000], Step [4200/4367], Loss: 0.0214
2025-02-15 00:47:21,044 - Epoch [39/1000], Step [4300/4367], Loss: 0.1900
2025-02-15 00:47:53,950 - Epoch [39/1000], Validation Step [100/1090], Val Loss: 0.0001
2025-02-15 00:48:03,143 - Epoch [39/1000], Validation Step [200/1090], Val Loss: 0.0001
2025-02-15 00:48:12,474 - Epoch [39/1000], Validation Step [300/1090], Val Loss: 0.2756
2025-02-15 00:48:22,124 - Epoch [39/1000], Validation Step [400/1090], Val Loss: 0.1251
2025-02-15 00:48:31,221 - Epoch [39/1000], Validation Step [500/1090], Val Loss: 0.5023
2025-02-15 00:48:40,740 - Epoch [39/1000], Validation Step [600/1090], Val Loss: 0.1761
2025-02-15 00:48:50,303 - Epoch [39/1000], Validation Step [700/1090], Val Loss: 0.1217
2025-02-15 00:48:59,084 - Epoch [39/1000], Validation Step [800/1090], Val Loss: 0.0036
2025-02-15 00:49:07,622 - Epoch [39/1000], Validation Step [900/1090], Val Loss: 0.0026
2025-02-15 00:49:16,817 - Epoch [39/1000], Validation Step [1000/1090], Val Loss: 0.0000
2025-02-15 00:49:25,397 - Epoch 39/1000, Train Loss: 0.1467, Val Loss: 0.1514, Accuracy: 94.50%
2025-02-15 00:50:00,979 - Epoch [40/1000], Step [100/4367], Loss: 0.0787
2025-02-15 00:50:35,691 - Epoch [40/1000], Step [200/4367], Loss: 0.1980
2025-02-15 00:51:09,947 - Epoch [40/1000], Step [300/4367], Loss: 0.3021
2025-02-15 00:51:44,255 - Epoch [40/1000], Step [400/4367], Loss: 0.0160
2025-02-15 00:52:18,570 - Epoch [40/1000], Step [500/4367], Loss: 0.1187
2025-02-15 00:52:53,379 - Epoch [40/1000], Step [600/4367], Loss: 0.1585
2025-02-15 00:53:28,004 - Epoch [40/1000], Step [700/4367], Loss: 0.1934
2025-02-15 00:54:02,311 - Epoch [40/1000], Step [800/4367], Loss: 0.2080
2025-02-15 00:54:36,993 - Epoch [40/1000], Step [900/4367], Loss: 0.0913
2025-02-15 00:55:11,822 - Epoch [40/1000], Step [1000/4367], Loss: 0.2533
2025-02-15 00:55:46,203 - Epoch [40/1000], Step [1100/4367], Loss: 0.2361
2025-02-15 00:56:20,970 - Epoch [40/1000], Step [1200/4367], Loss: 0.1713
2025-02-15 00:56:55,811 - Epoch [40/1000], Step [1300/4367], Loss: 0.1408
2025-02-15 00:57:30,639 - Epoch [40/1000], Step [1400/4367], Loss: 0.1995
2025-02-15 00:58:05,754 - Epoch [40/1000], Step [1500/4367], Loss: 0.0913
2025-02-15 00:58:40,316 - Epoch [40/1000], Step [1600/4367], Loss: 0.1469
2025-02-15 00:59:14,969 - Epoch [40/1000], Step [1700/4367], Loss: 0.1747
2025-02-15 00:59:49,825 - Epoch [40/1000], Step [1800/4367], Loss: 0.1409
2025-02-15 01:00:24,579 - Epoch [40/1000], Step [1900/4367], Loss: 0.0774
2025-02-15 01:00:59,164 - Epoch [40/1000], Step [2000/4367], Loss: 0.2164
2025-02-15 01:01:33,750 - Epoch [40/1000], Step [2100/4367], Loss: 0.2156
2025-02-15 01:02:08,574 - Epoch [40/1000], Step [2200/4367], Loss: 0.0440
2025-02-15 01:02:43,206 - Epoch [40/1000], Step [2300/4367], Loss: 0.0208
2025-02-15 01:03:18,069 - Epoch [40/1000], Step [2400/4367], Loss: 0.0231
2025-02-15 01:03:52,910 - Epoch [40/1000], Step [2500/4367], Loss: 0.3246
2025-02-15 01:04:28,022 - Epoch [40/1000], Step [2600/4367], Loss: 0.1797
2025-02-15 01:05:02,647 - Epoch [40/1000], Step [2700/4367], Loss: 0.1492
2025-02-15 01:05:37,412 - Epoch [40/1000], Step [2800/4367], Loss: 0.2628
2025-02-15 01:06:12,021 - Epoch [40/1000], Step [2900/4367], Loss: 0.1701
2025-02-15 01:06:46,955 - Epoch [40/1000], Step [3000/4367], Loss: 0.1995
2025-02-15 01:07:21,402 - Epoch [40/1000], Step [3100/4367], Loss: 0.1760
2025-02-15 01:07:56,185 - Epoch [40/1000], Step [3200/4367], Loss: 0.2471
2025-02-15 01:08:30,890 - Epoch [40/1000], Step [3300/4367], Loss: 0.0866
2025-02-15 01:09:05,721 - Epoch [40/1000], Step [3400/4367], Loss: 0.0786
2025-02-15 01:09:40,473 - Epoch [40/1000], Step [3500/4367], Loss: 0.1220
2025-02-15 01:10:15,121 - Epoch [40/1000], Step [3600/4367], Loss: 0.0896
2025-02-15 01:10:49,789 - Epoch [40/1000], Step [3700/4367], Loss: 0.1596
2025-02-15 01:11:24,127 - Epoch [40/1000], Step [3800/4367], Loss: 0.0635
2025-02-15 01:11:58,535 - Epoch [40/1000], Step [3900/4367], Loss: 0.2341
2025-02-15 01:12:33,455 - Epoch [40/1000], Step [4000/4367], Loss: 0.0871
2025-02-15 01:13:08,333 - Epoch [40/1000], Step [4100/4367], Loss: 0.0475
2025-02-15 01:13:42,806 - Epoch [40/1000], Step [4200/4367], Loss: 0.1061
2025-02-15 01:14:17,374 - Epoch [40/1000], Step [4300/4367], Loss: 0.2973
2025-02-15 01:14:50,183 - Epoch [40/1000], Validation Step [100/1090], Val Loss: 0.0002
2025-02-15 01:14:59,362 - Epoch [40/1000], Validation Step [200/1090], Val Loss: 0.0003
2025-02-15 01:15:08,694 - Epoch [40/1000], Validation Step [300/1090], Val Loss: 0.3179
2025-02-15 01:15:18,350 - Epoch [40/1000], Validation Step [400/1090], Val Loss: 0.0776
2025-02-15 01:15:27,459 - Epoch [40/1000], Validation Step [500/1090], Val Loss: 0.4280
2025-02-15 01:15:36,979 - Epoch [40/1000], Validation Step [600/1090], Val Loss: 0.0825
2025-02-15 01:15:46,529 - Epoch [40/1000], Validation Step [700/1090], Val Loss: 0.0744
2025-02-15 01:15:55,307 - Epoch [40/1000], Validation Step [800/1090], Val Loss: 0.0054
2025-02-15 01:16:03,854 - Epoch [40/1000], Validation Step [900/1090], Val Loss: 0.0033
2025-02-15 01:16:13,103 - Epoch [40/1000], Validation Step [1000/1090], Val Loss: 0.0001
2025-02-15 01:16:21,725 - Epoch 40/1000, Train Loss: 0.1458, Val Loss: 0.1514, Accuracy: 94.46%
2025-02-15 01:16:22,124 - Model saved at /data2/personal/sungjin/korean_dialects/checkpoint_real/checkpoint_epoch_40.pth
2025-02-15 01:16:57,707 - Epoch [41/1000], Step [100/4367], Loss: 0.0711
2025-02-15 01:17:32,464 - Epoch [41/1000], Step [200/4367], Loss: 0.0733
2025-02-15 01:18:07,173 - Epoch [41/1000], Step [300/4367], Loss: 0.1494
2025-02-15 01:18:42,102 - Epoch [41/1000], Step [400/4367], Loss: 0.1650
2025-02-15 01:19:16,882 - Epoch [41/1000], Step [500/4367], Loss: 0.0906
2025-02-15 01:19:51,464 - Epoch [41/1000], Step [600/4367], Loss: 0.1811
2025-02-15 01:20:26,144 - Epoch [41/1000], Step [700/4367], Loss: 0.2432
2025-02-15 01:21:00,791 - Epoch [41/1000], Step [800/4367], Loss: 0.1539
2025-02-15 01:21:35,623 - Epoch [41/1000], Step [900/4367], Loss: 0.0352
2025-02-15 01:22:10,253 - Epoch [41/1000], Step [1000/4367], Loss: 0.0983
2025-02-15 01:22:44,941 - Epoch [41/1000], Step [1100/4367], Loss: 0.1464
2025-02-15 01:23:19,611 - Epoch [41/1000], Step [1200/4367], Loss: 0.1353
2025-02-15 01:23:54,224 - Epoch [41/1000], Step [1300/4367], Loss: 0.1297
2025-02-15 01:24:29,476 - Epoch [41/1000], Step [1400/4367], Loss: 0.2156
2025-02-15 01:25:04,450 - Epoch [41/1000], Step [1500/4367], Loss: 0.0494
2025-02-15 01:25:39,029 - Epoch [41/1000], Step [1600/4367], Loss: 0.1314
2025-02-15 01:26:13,529 - Epoch [41/1000], Step [1700/4367], Loss: 0.0841
2025-02-15 01:26:48,349 - Epoch [41/1000], Step [1800/4367], Loss: 0.1810
2025-02-15 01:27:22,995 - Epoch [41/1000], Step [1900/4367], Loss: 0.0613
2025-02-15 01:27:57,807 - Epoch [41/1000], Step [2000/4367], Loss: 0.2614
2025-02-15 01:28:32,184 - Epoch [41/1000], Step [2100/4367], Loss: 0.0959
2025-02-15 01:29:06,923 - Epoch [41/1000], Step [2200/4367], Loss: 0.2164
2025-02-15 01:29:41,657 - Epoch [41/1000], Step [2300/4367], Loss: 0.1030
2025-02-15 01:30:16,362 - Epoch [41/1000], Step [2400/4367], Loss: 0.0705
2025-02-15 01:30:50,968 - Epoch [41/1000], Step [2500/4367], Loss: 0.2672
2025-02-15 01:31:25,399 - Epoch [41/1000], Step [2600/4367], Loss: 0.1249
2025-02-15 01:31:59,876 - Epoch [41/1000], Step [2700/4367], Loss: 0.3085
2025-02-15 01:32:34,137 - Epoch [41/1000], Step [2800/4367], Loss: 0.0572
2025-02-15 01:33:08,717 - Epoch [41/1000], Step [2900/4367], Loss: 0.1396
2025-02-15 01:33:43,475 - Epoch [41/1000], Step [3000/4367], Loss: 0.0994
2025-02-15 01:34:17,826 - Epoch [41/1000], Step [3100/4367], Loss: 0.0805
2025-02-15 01:34:52,450 - Epoch [41/1000], Step [3200/4367], Loss: 0.0385
2025-02-15 01:35:27,064 - Epoch [41/1000], Step [3300/4367], Loss: 0.1699
2025-02-15 01:36:01,792 - Epoch [41/1000], Step [3400/4367], Loss: 0.1589
2025-02-15 01:36:36,393 - Epoch [41/1000], Step [3500/4367], Loss: 0.1001
2025-02-15 01:37:11,704 - Epoch [41/1000], Step [3600/4367], Loss: 0.0906
2025-02-15 01:37:46,517 - Epoch [41/1000], Step [3700/4367], Loss: 0.1811
2025-02-15 01:38:21,136 - Epoch [41/1000], Step [3800/4367], Loss: 0.1443
2025-02-15 01:38:56,271 - Epoch [41/1000], Step [3900/4367], Loss: 0.1832
2025-02-15 01:39:31,038 - Epoch [41/1000], Step [4000/4367], Loss: 0.0757
2025-02-15 01:40:05,811 - Epoch [41/1000], Step [4100/4367], Loss: 0.0921
2025-02-15 01:40:41,014 - Epoch [41/1000], Step [4200/4367], Loss: 0.0634
2025-02-15 01:41:15,731 - Epoch [41/1000], Step [4300/4367], Loss: 0.1516
2025-02-15 01:41:48,652 - Epoch [41/1000], Validation Step [100/1090], Val Loss: 0.0004
2025-02-15 01:41:57,863 - Epoch [41/1000], Validation Step [200/1090], Val Loss: 0.0011
2025-02-15 01:42:07,203 - Epoch [41/1000], Validation Step [300/1090], Val Loss: 0.3049
2025-02-15 01:42:16,864 - Epoch [41/1000], Validation Step [400/1090], Val Loss: 0.1040
2025-02-15 01:42:25,958 - Epoch [41/1000], Validation Step [500/1090], Val Loss: 0.4620
2025-02-15 01:42:35,467 - Epoch [41/1000], Validation Step [600/1090], Val Loss: 0.1296
2025-02-15 01:42:45,020 - Epoch [41/1000], Validation Step [700/1090], Val Loss: 0.1053
2025-02-15 01:42:53,790 - Epoch [41/1000], Validation Step [800/1090], Val Loss: 0.0046
2025-02-15 01:43:02,342 - Epoch [41/1000], Validation Step [900/1090], Val Loss: 0.0026
2025-02-15 01:43:11,587 - Epoch [41/1000], Validation Step [1000/1090], Val Loss: 0.0002
2025-02-15 01:43:20,211 - Epoch 41/1000, Train Loss: 0.1444, Val Loss: 0.1526, Accuracy: 94.41%
2025-02-15 01:43:56,431 - Epoch [42/1000], Step [100/4367], Loss: 0.3027
2025-02-15 01:44:30,794 - Epoch [42/1000], Step [200/4367], Loss: 0.0562
2025-02-15 01:45:05,227 - Epoch [42/1000], Step [300/4367], Loss: 0.0308
2025-02-15 01:45:39,926 - Epoch [42/1000], Step [400/4367], Loss: 0.0885
2025-02-15 01:46:14,432 - Epoch [42/1000], Step [500/4367], Loss: 0.1811
2025-02-15 01:46:48,664 - Epoch [42/1000], Step [600/4367], Loss: 0.0671
2025-02-15 01:47:23,399 - Epoch [42/1000], Step [700/4367], Loss: 0.2957
2025-02-15 01:47:58,221 - Epoch [42/1000], Step [800/4367], Loss: 0.1953
2025-02-15 01:48:33,197 - Epoch [42/1000], Step [900/4367], Loss: 0.1183
2025-02-15 01:49:07,439 - Epoch [42/1000], Step [1000/4367], Loss: 0.0844
2025-02-15 01:49:41,999 - Epoch [42/1000], Step [1100/4367], Loss: 0.2352
2025-02-15 01:50:16,598 - Epoch [42/1000], Step [1200/4367], Loss: 0.1092
2025-02-15 01:50:51,243 - Epoch [42/1000], Step [1300/4367], Loss: 0.0598
2025-02-15 01:51:26,010 - Epoch [42/1000], Step [1400/4367], Loss: 0.0497
2025-02-15 01:52:00,589 - Epoch [42/1000], Step [1500/4367], Loss: 0.2468
2025-02-15 01:52:35,398 - Epoch [42/1000], Step [1600/4367], Loss: 0.1712
2025-02-15 01:53:09,985 - Epoch [42/1000], Step [1700/4367], Loss: 0.1718
2025-02-15 01:53:44,502 - Epoch [42/1000], Step [1800/4367], Loss: 0.1521
2025-02-15 01:54:19,468 - Epoch [42/1000], Step [1900/4367], Loss: 0.0430
2025-02-15 01:54:54,069 - Epoch [42/1000], Step [2000/4367], Loss: 0.0965
2025-02-15 01:55:28,360 - Epoch [42/1000], Step [2100/4367], Loss: 0.0872
2025-02-15 01:56:03,191 - Epoch [42/1000], Step [2200/4367], Loss: 0.0743
2025-02-15 01:56:37,597 - Epoch [42/1000], Step [2300/4367], Loss: 0.0263
2025-02-15 01:57:12,714 - Epoch [42/1000], Step [2400/4367], Loss: 0.2250
2025-02-15 01:57:47,115 - Epoch [42/1000], Step [2500/4367], Loss: 0.2203
2025-02-15 01:58:21,561 - Epoch [42/1000], Step [2600/4367], Loss: 0.0394
2025-02-15 01:58:56,105 - Epoch [42/1000], Step [2700/4367], Loss: 0.0503
2025-02-15 01:59:30,662 - Epoch [42/1000], Step [2800/4367], Loss: 0.0653
2025-02-15 02:00:05,360 - Epoch [42/1000], Step [2900/4367], Loss: 0.1254
2025-02-15 02:00:39,754 - Epoch [42/1000], Step [3000/4367], Loss: 0.1694
2025-02-15 02:01:14,648 - Epoch [42/1000], Step [3100/4367], Loss: 0.0387
2025-02-15 02:01:49,272 - Epoch [42/1000], Step [3200/4367], Loss: 0.1127
2025-02-15 02:02:23,980 - Epoch [42/1000], Step [3300/4367], Loss: 0.1036
2025-02-15 02:02:58,433 - Epoch [42/1000], Step [3400/4367], Loss: 0.1196
2025-02-15 02:03:32,982 - Epoch [42/1000], Step [3500/4367], Loss: 0.1489
2025-02-15 02:04:07,563 - Epoch [42/1000], Step [3600/4367], Loss: 0.1439
2025-02-15 02:04:42,182 - Epoch [42/1000], Step [3700/4367], Loss: 0.0991
2025-02-15 02:05:17,168 - Epoch [42/1000], Step [3800/4367], Loss: 0.0605
2025-02-15 02:05:52,546 - Epoch [42/1000], Step [3900/4367], Loss: 0.0601
2025-02-15 02:06:27,205 - Epoch [42/1000], Step [4000/4367], Loss: 0.1361
2025-02-15 02:07:01,866 - Epoch [42/1000], Step [4100/4367], Loss: 0.0990
2025-02-15 02:07:36,462 - Epoch [42/1000], Step [4200/4367], Loss: 0.0661
2025-02-15 02:08:11,178 - Epoch [42/1000], Step [4300/4367], Loss: 0.1008
2025-02-15 02:08:43,849 - Epoch [42/1000], Validation Step [100/1090], Val Loss: 0.0002
2025-02-15 02:08:53,048 - Epoch [42/1000], Validation Step [200/1090], Val Loss: 0.0005
2025-02-15 02:09:02,386 - Epoch [42/1000], Validation Step [300/1090], Val Loss: 0.2695
2025-02-15 02:09:12,041 - Epoch [42/1000], Validation Step [400/1090], Val Loss: 0.0694
2025-02-15 02:09:21,138 - Epoch [42/1000], Validation Step [500/1090], Val Loss: 0.4810
2025-02-15 02:09:30,670 - Epoch [42/1000], Validation Step [600/1090], Val Loss: 0.1112
2025-02-15 02:09:40,225 - Epoch [42/1000], Validation Step [700/1090], Val Loss: 0.1017
2025-02-15 02:09:48,997 - Epoch [42/1000], Validation Step [800/1090], Val Loss: 0.0052
2025-02-15 02:09:57,551 - Epoch [42/1000], Validation Step [900/1090], Val Loss: 0.0036
2025-02-15 02:10:06,797 - Epoch [42/1000], Validation Step [1000/1090], Val Loss: 0.0001
2025-02-15 02:10:15,424 - Epoch 42/1000, Train Loss: 0.1425, Val Loss: 0.1458, Accuracy: 94.57%
2025-02-15 02:10:15,866 - Model saved at /data2/personal/sungjin/korean_dialects/checkpoint_real/checkpoint_epoch_42.pth
2025-02-15 02:10:51,045 - Epoch [43/1000], Step [100/4367], Loss: 0.1476
2025-02-15 02:11:25,756 - Epoch [43/1000], Step [200/4367], Loss: 0.0734
2025-02-15 02:12:00,211 - Epoch [43/1000], Step [300/4367], Loss: 0.2579
2025-02-15 02:12:35,224 - Epoch [43/1000], Step [400/4367], Loss: 0.0991
2025-02-15 02:13:09,964 - Epoch [43/1000], Step [500/4367], Loss: 0.1320
2025-02-15 02:13:44,524 - Epoch [43/1000], Step [600/4367], Loss: 0.0811
2025-02-15 02:14:19,476 - Epoch [43/1000], Step [700/4367], Loss: 0.0586
2025-02-15 02:14:54,350 - Epoch [43/1000], Step [800/4367], Loss: 0.1838
2025-02-15 02:15:29,090 - Epoch [43/1000], Step [900/4367], Loss: 0.1354
2025-02-15 02:16:03,714 - Epoch [43/1000], Step [1000/4367], Loss: 0.1890
2025-02-15 02:16:38,303 - Epoch [43/1000], Step [1100/4367], Loss: 0.1872
2025-02-15 02:17:13,110 - Epoch [43/1000], Step [1200/4367], Loss: 0.0881
2025-02-15 02:17:47,733 - Epoch [43/1000], Step [1300/4367], Loss: 0.1336
2025-02-15 02:18:22,069 - Epoch [43/1000], Step [1400/4367], Loss: 0.2004
2025-02-15 02:18:56,197 - Epoch [43/1000], Step [1500/4367], Loss: 0.1179
2025-02-15 02:19:30,754 - Epoch [43/1000], Step [1600/4367], Loss: 0.2090
2025-02-15 02:20:05,044 - Epoch [43/1000], Step [1700/4367], Loss: 0.2683
2025-02-15 02:20:39,482 - Epoch [43/1000], Step [1800/4367], Loss: 0.1152
2025-02-15 02:21:14,340 - Epoch [43/1000], Step [1900/4367], Loss: 0.0638
2025-02-15 02:21:49,190 - Epoch [43/1000], Step [2000/4367], Loss: 0.1245
2025-02-15 02:22:23,754 - Epoch [43/1000], Step [2100/4367], Loss: 0.1096
2025-02-15 02:22:58,659 - Epoch [43/1000], Step [2200/4367], Loss: 0.2471
2025-02-15 02:23:33,526 - Epoch [43/1000], Step [2300/4367], Loss: 0.0130
2025-02-15 02:24:08,055 - Epoch [43/1000], Step [2400/4367], Loss: 0.2923
2025-02-15 02:24:42,374 - Epoch [43/1000], Step [2500/4367], Loss: 0.0460
2025-02-15 02:25:17,210 - Epoch [43/1000], Step [2600/4367], Loss: 0.1279
2025-02-15 02:25:52,044 - Epoch [43/1000], Step [2700/4367], Loss: 0.2782
2025-02-15 02:26:26,677 - Epoch [43/1000], Step [2800/4367], Loss: 0.0304
2025-02-15 02:27:01,543 - Epoch [43/1000], Step [2900/4367], Loss: 0.0674
2025-02-15 02:27:36,520 - Epoch [43/1000], Step [3000/4367], Loss: 0.2175
2025-02-15 02:28:11,501 - Epoch [43/1000], Step [3100/4367], Loss: 0.3133
2025-02-15 02:28:46,297 - Epoch [43/1000], Step [3200/4367], Loss: 0.0808
2025-02-15 02:29:20,962 - Epoch [43/1000], Step [3300/4367], Loss: 0.0450
2025-02-15 02:29:56,145 - Epoch [43/1000], Step [3400/4367], Loss: 0.0311
2025-02-15 02:30:31,114 - Epoch [43/1000], Step [3500/4367], Loss: 0.0598
2025-02-15 02:31:05,717 - Epoch [43/1000], Step [3600/4367], Loss: 0.0150
2025-02-15 02:31:40,260 - Epoch [43/1000], Step [3700/4367], Loss: 0.2581
2025-02-15 02:32:15,070 - Epoch [43/1000], Step [3800/4367], Loss: 0.0256
2025-02-15 02:32:49,652 - Epoch [43/1000], Step [3900/4367], Loss: 0.1816
2025-02-15 02:33:24,373 - Epoch [43/1000], Step [4000/4367], Loss: 0.1277
2025-02-15 02:33:59,263 - Epoch [43/1000], Step [4100/4367], Loss: 0.2060
2025-02-15 02:34:34,454 - Epoch [43/1000], Step [4200/4367], Loss: 0.0205
2025-02-15 02:35:08,980 - Epoch [43/1000], Step [4300/4367], Loss: 0.0506
2025-02-15 02:35:42,107 - Epoch [43/1000], Validation Step [100/1090], Val Loss: 0.0001
2025-02-15 02:35:51,301 - Epoch [43/1000], Validation Step [200/1090], Val Loss: 0.0003
2025-02-15 02:36:00,641 - Epoch [43/1000], Validation Step [300/1090], Val Loss: 0.2550
2025-02-15 02:36:10,299 - Epoch [43/1000], Validation Step [400/1090], Val Loss: 0.0728
2025-02-15 02:36:19,395 - Epoch [43/1000], Validation Step [500/1090], Val Loss: 0.5182
2025-02-15 02:36:28,912 - Epoch [43/1000], Validation Step [600/1090], Val Loss: 0.1268
2025-02-15 02:36:38,475 - Epoch [43/1000], Validation Step [700/1090], Val Loss: 0.1094
2025-02-15 02:36:47,250 - Epoch [43/1000], Validation Step [800/1090], Val Loss: 0.0058
2025-02-15 02:36:55,806 - Epoch [43/1000], Validation Step [900/1090], Val Loss: 0.0046
2025-02-15 02:37:05,057 - Epoch [43/1000], Validation Step [1000/1090], Val Loss: 0.0003
2025-02-15 02:37:13,672 - Epoch 43/1000, Train Loss: 0.1432, Val Loss: 0.1497, Accuracy: 94.49%
2025-02-15 02:37:49,252 - Epoch [44/1000], Step [100/4367], Loss: 0.1390
2025-02-15 02:38:23,825 - Epoch [44/1000], Step [200/4367], Loss: 0.1157
2025-02-15 02:38:58,305 - Epoch [44/1000], Step [300/4367], Loss: 0.1118
2025-02-15 02:39:32,908 - Epoch [44/1000], Step [400/4367], Loss: 0.3021
2025-02-15 02:40:07,624 - Epoch [44/1000], Step [500/4367], Loss: 0.0261
2025-02-15 02:40:42,106 - Epoch [44/1000], Step [600/4367], Loss: 0.1070
2025-02-15 02:41:16,215 - Epoch [44/1000], Step [700/4367], Loss: 0.1082
2025-02-15 02:41:50,850 - Epoch [44/1000], Step [800/4367], Loss: 0.1201
2025-02-15 02:42:25,747 - Epoch [44/1000], Step [900/4367], Loss: 0.2992
2025-02-15 02:43:00,131 - Epoch [44/1000], Step [1000/4367], Loss: 0.1287
2025-02-15 02:43:34,675 - Epoch [44/1000], Step [1100/4367], Loss: 0.1601
2025-02-15 02:44:09,764 - Epoch [44/1000], Step [1200/4367], Loss: 0.1129
2025-02-15 02:44:44,031 - Epoch [44/1000], Step [1300/4367], Loss: 0.0168
2025-02-15 02:45:18,286 - Epoch [44/1000], Step [1400/4367], Loss: 0.0946
2025-02-15 02:45:52,657 - Epoch [44/1000], Step [1500/4367], Loss: 0.0713
2025-02-15 02:46:27,117 - Epoch [44/1000], Step [1600/4367], Loss: 0.0963
2025-02-15 02:47:02,155 - Epoch [44/1000], Step [1700/4367], Loss: 0.0765
2025-02-15 02:47:36,826 - Epoch [44/1000], Step [1800/4367], Loss: 0.2469
2025-02-15 02:48:11,091 - Epoch [44/1000], Step [1900/4367], Loss: 0.2822
2025-02-15 02:48:45,603 - Epoch [44/1000], Step [2000/4367], Loss: 0.0597
2025-02-15 02:49:20,027 - Epoch [44/1000], Step [2100/4367], Loss: 0.1435
2025-02-15 02:49:54,330 - Epoch [44/1000], Step [2200/4367], Loss: 0.0688
2025-02-15 02:50:28,463 - Epoch [44/1000], Step [2300/4367], Loss: 0.1471
2025-02-15 02:51:03,292 - Epoch [44/1000], Step [2400/4367], Loss: 0.1126
2025-02-15 02:51:37,303 - Epoch [44/1000], Step [2500/4367], Loss: 0.2637
2025-02-15 02:52:11,538 - Epoch [44/1000], Step [2600/4367], Loss: 0.1133
2025-02-15 02:52:46,881 - Epoch [44/1000], Step [2700/4367], Loss: 0.0555
2025-02-15 02:53:21,646 - Epoch [44/1000], Step [2800/4367], Loss: 0.0340
2025-02-15 02:53:56,699 - Epoch [44/1000], Step [2900/4367], Loss: 0.1444
2025-02-15 02:54:31,614 - Epoch [44/1000], Step [3000/4367], Loss: 0.0582
2025-02-15 02:55:06,261 - Epoch [44/1000], Step [3100/4367], Loss: 0.2309
2025-02-15 02:55:40,667 - Epoch [44/1000], Step [3200/4367], Loss: 0.2107
2025-02-15 02:56:15,456 - Epoch [44/1000], Step [3300/4367], Loss: 0.0793
2025-02-15 02:56:50,116 - Epoch [44/1000], Step [3400/4367], Loss: 0.1431
2025-02-15 02:57:24,869 - Epoch [44/1000], Step [3500/4367], Loss: 0.1913
2025-02-15 02:57:59,648 - Epoch [44/1000], Step [3600/4367], Loss: 0.3573
2025-02-15 02:58:34,436 - Epoch [44/1000], Step [3700/4367], Loss: 0.1880
2025-02-15 02:59:09,197 - Epoch [44/1000], Step [3800/4367], Loss: 0.2051
2025-02-15 02:59:43,470 - Epoch [44/1000], Step [3900/4367], Loss: 0.1673
2025-02-15 03:00:18,260 - Epoch [44/1000], Step [4000/4367], Loss: 0.1693
2025-02-15 03:00:53,127 - Epoch [44/1000], Step [4100/4367], Loss: 0.1056
2025-02-15 03:01:28,158 - Epoch [44/1000], Step [4200/4367], Loss: 0.1115
2025-02-15 03:02:02,557 - Epoch [44/1000], Step [4300/4367], Loss: 0.2907
2025-02-15 03:02:35,088 - Epoch [44/1000], Validation Step [100/1090], Val Loss: 0.0001
2025-02-15 03:02:44,283 - Epoch [44/1000], Validation Step [200/1090], Val Loss: 0.0005
2025-02-15 03:02:53,620 - Epoch [44/1000], Validation Step [300/1090], Val Loss: 0.2499
2025-02-15 03:03:03,276 - Epoch [44/1000], Validation Step [400/1090], Val Loss: 0.0642
2025-02-15 03:03:12,371 - Epoch [44/1000], Validation Step [500/1090], Val Loss: 0.4698
2025-02-15 03:03:21,886 - Epoch [44/1000], Validation Step [600/1090], Val Loss: 0.1650
2025-02-15 03:03:31,450 - Epoch [44/1000], Validation Step [700/1090], Val Loss: 0.1422
2025-02-15 03:03:40,227 - Epoch [44/1000], Validation Step [800/1090], Val Loss: 0.0045
2025-02-15 03:03:48,762 - Epoch [44/1000], Validation Step [900/1090], Val Loss: 0.0037
2025-02-15 03:03:58,000 - Epoch [44/1000], Validation Step [1000/1090], Val Loss: 0.0001
2025-02-15 03:04:06,620 - Epoch 44/1000, Train Loss: 0.1421, Val Loss: 0.1457, Accuracy: 94.67%
2025-02-15 03:04:07,089 - Model saved at /data2/personal/sungjin/korean_dialects/checkpoint_real/checkpoint_epoch_44.pth
2025-02-15 03:04:42,640 - Epoch [45/1000], Step [100/4367], Loss: 0.3080
2025-02-15 03:05:17,197 - Epoch [45/1000], Step [200/4367], Loss: 0.0405
2025-02-15 03:05:51,824 - Epoch [45/1000], Step [300/4367], Loss: 0.0356
2025-02-15 03:06:26,215 - Epoch [45/1000], Step [400/4367], Loss: 0.0480
2025-02-15 03:07:00,925 - Epoch [45/1000], Step [500/4367], Loss: 0.1829
2025-02-15 03:07:35,964 - Epoch [45/1000], Step [600/4367], Loss: 0.2827
2025-02-15 03:08:10,835 - Epoch [45/1000], Step [700/4367], Loss: 0.3634
2025-02-15 03:08:45,511 - Epoch [45/1000], Step [800/4367], Loss: 0.0760
2025-02-15 03:09:19,384 - Epoch [45/1000], Step [900/4367], Loss: 0.1263
2025-02-15 03:09:54,273 - Epoch [45/1000], Step [1000/4367], Loss: 0.2062
2025-02-15 03:10:29,151 - Epoch [45/1000], Step [1100/4367], Loss: 0.0496
2025-02-15 03:11:03,989 - Epoch [45/1000], Step [1200/4367], Loss: 0.1575
2025-02-15 03:11:39,132 - Epoch [45/1000], Step [1300/4367], Loss: 0.4526
2025-02-15 03:12:13,412 - Epoch [45/1000], Step [1400/4367], Loss: 0.0874
2025-02-15 03:12:48,079 - Epoch [45/1000], Step [1500/4367], Loss: 0.2616
2025-02-15 03:13:22,957 - Epoch [45/1000], Step [1600/4367], Loss: 0.0374
2025-02-15 03:13:57,428 - Epoch [45/1000], Step [1700/4367], Loss: 0.4176
2025-02-15 03:14:32,014 - Epoch [45/1000], Step [1800/4367], Loss: 0.0752
2025-02-15 03:15:06,520 - Epoch [45/1000], Step [1900/4367], Loss: 0.0466
2025-02-15 03:15:41,264 - Epoch [45/1000], Step [2000/4367], Loss: 0.0544
2025-02-15 03:16:15,932 - Epoch [45/1000], Step [2100/4367], Loss: 0.3323
2025-02-15 03:16:50,475 - Epoch [45/1000], Step [2200/4367], Loss: 0.1525
2025-02-15 03:17:24,609 - Epoch [45/1000], Step [2300/4367], Loss: 0.1338
2025-02-15 03:17:59,467 - Epoch [45/1000], Step [2400/4367], Loss: 0.1224
2025-02-15 03:18:34,259 - Epoch [45/1000], Step [2500/4367], Loss: 0.1159
2025-02-15 03:19:09,131 - Epoch [45/1000], Step [2600/4367], Loss: 0.1177
2025-02-15 03:19:43,762 - Epoch [45/1000], Step [2700/4367], Loss: 0.2416
2025-02-15 03:20:18,852 - Epoch [45/1000], Step [2800/4367], Loss: 0.0772
2025-02-15 03:20:53,562 - Epoch [45/1000], Step [2900/4367], Loss: 0.1235
2025-02-15 03:21:28,326 - Epoch [45/1000], Step [3000/4367], Loss: 0.1598
2025-02-15 03:22:02,641 - Epoch [45/1000], Step [3100/4367], Loss: 0.1447
2025-02-15 03:22:37,034 - Epoch [45/1000], Step [3200/4367], Loss: 0.1831
2025-02-15 03:23:11,546 - Epoch [45/1000], Step [3300/4367], Loss: 0.0626
2025-02-15 03:23:46,545 - Epoch [45/1000], Step [3400/4367], Loss: 0.0257
2025-02-15 03:24:21,457 - Epoch [45/1000], Step [3500/4367], Loss: 0.1540
2025-02-15 03:24:56,124 - Epoch [45/1000], Step [3600/4367], Loss: 0.0827
2025-02-15 03:25:30,956 - Epoch [45/1000], Step [3700/4367], Loss: 0.1379
2025-02-15 03:26:05,719 - Epoch [45/1000], Step [3800/4367], Loss: 0.0833
2025-02-15 03:26:40,449 - Epoch [45/1000], Step [3900/4367], Loss: 0.0419
2025-02-15 03:27:14,357 - Epoch [45/1000], Step [4000/4367], Loss: 0.2137
2025-02-15 03:27:48,924 - Epoch [45/1000], Step [4100/4367], Loss: 0.1290
2025-02-15 03:28:22,964 - Epoch [45/1000], Step [4200/4367], Loss: 0.0896
2025-02-15 03:28:57,816 - Epoch [45/1000], Step [4300/4367], Loss: 0.0596
2025-02-15 03:29:30,770 - Epoch [45/1000], Validation Step [100/1090], Val Loss: 0.0010
2025-02-15 03:29:39,961 - Epoch [45/1000], Validation Step [200/1090], Val Loss: 0.0114
2025-02-15 03:29:49,298 - Epoch [45/1000], Validation Step [300/1090], Val Loss: 0.2249
2025-02-15 03:29:58,958 - Epoch [45/1000], Validation Step [400/1090], Val Loss: 0.0827
2025-02-15 03:30:08,061 - Epoch [45/1000], Validation Step [500/1090], Val Loss: 0.3817
2025-02-15 03:30:17,588 - Epoch [45/1000], Validation Step [600/1090], Val Loss: 0.2070
2025-02-15 03:30:27,142 - Epoch [45/1000], Validation Step [700/1090], Val Loss: 0.2170
2025-02-15 03:30:35,920 - Epoch [45/1000], Validation Step [800/1090], Val Loss: 0.0085
2025-02-15 03:30:44,478 - Epoch [45/1000], Validation Step [900/1090], Val Loss: 0.0077
2025-02-15 03:30:53,720 - Epoch [45/1000], Validation Step [1000/1090], Val Loss: 0.0007
2025-02-15 03:31:02,337 - Epoch 45/1000, Train Loss: 0.1410, Val Loss: 0.1524, Accuracy: 94.35%
2025-02-15 03:31:38,242 - Epoch [46/1000], Step [100/4367], Loss: 0.1701
2025-02-15 03:32:12,567 - Epoch [46/1000], Step [200/4367], Loss: 0.1176
2025-02-15 03:32:47,488 - Epoch [46/1000], Step [300/4367], Loss: 0.0529
2025-02-15 03:33:22,253 - Epoch [46/1000], Step [400/4367], Loss: 0.0856
2025-02-15 03:33:57,142 - Epoch [46/1000], Step [500/4367], Loss: 0.1161
2025-02-15 03:34:31,724 - Epoch [46/1000], Step [600/4367], Loss: 0.3112
2025-02-15 03:35:06,349 - Epoch [46/1000], Step [700/4367], Loss: 0.1556
2025-02-15 03:35:41,197 - Epoch [46/1000], Step [800/4367], Loss: 0.1481
2025-02-15 03:36:15,964 - Epoch [46/1000], Step [900/4367], Loss: 0.0282
2025-02-15 03:36:50,721 - Epoch [46/1000], Step [1000/4367], Loss: 0.0625
2025-02-15 03:37:25,539 - Epoch [46/1000], Step [1100/4367], Loss: 0.2555
2025-02-15 03:38:00,026 - Epoch [46/1000], Step [1200/4367], Loss: 0.0505
2025-02-15 03:38:34,852 - Epoch [46/1000], Step [1300/4367], Loss: 0.0401
2025-02-15 03:39:09,555 - Epoch [46/1000], Step [1400/4367], Loss: 0.0220
2025-02-15 03:39:44,046 - Epoch [46/1000], Step [1500/4367], Loss: 0.1651
2025-02-15 03:40:18,991 - Epoch [46/1000], Step [1600/4367], Loss: 0.0532
2025-02-15 03:40:53,806 - Epoch [46/1000], Step [1700/4367], Loss: 0.2107
2025-02-15 03:41:28,193 - Epoch [46/1000], Step [1800/4367], Loss: 0.3576
2025-02-15 03:42:03,084 - Epoch [46/1000], Step [1900/4367], Loss: 0.0888
2025-02-15 03:42:37,662 - Epoch [46/1000], Step [2000/4367], Loss: 0.1270
2025-02-15 03:43:12,120 - Epoch [46/1000], Step [2100/4367], Loss: 0.0805
2025-02-15 03:43:46,390 - Epoch [46/1000], Step [2200/4367], Loss: 0.0433
2025-02-15 03:44:21,099 - Epoch [46/1000], Step [2300/4367], Loss: 0.1560
2025-02-15 03:44:55,869 - Epoch [46/1000], Step [2400/4367], Loss: 0.1858
2025-02-15 03:45:30,322 - Epoch [46/1000], Step [2500/4367], Loss: 0.3541
2025-02-15 03:46:05,324 - Epoch [46/1000], Step [2600/4367], Loss: 0.2687
2025-02-15 03:46:39,529 - Epoch [46/1000], Step [2700/4367], Loss: 0.0162
2025-02-15 03:47:14,198 - Epoch [46/1000], Step [2800/4367], Loss: 0.0559
2025-02-15 03:47:48,549 - Epoch [46/1000], Step [2900/4367], Loss: 0.1430
2025-02-15 03:48:22,753 - Epoch [46/1000], Step [3000/4367], Loss: 0.0166
2025-02-15 03:48:57,365 - Epoch [46/1000], Step [3100/4367], Loss: 0.1838
2025-02-15 03:49:31,721 - Epoch [46/1000], Step [3200/4367], Loss: 0.0728
2025-02-15 03:50:06,279 - Epoch [46/1000], Step [3300/4367], Loss: 0.1935
2025-02-15 03:50:41,338 - Epoch [46/1000], Step [3400/4367], Loss: 0.0671
2025-02-15 03:51:16,248 - Epoch [46/1000], Step [3500/4367], Loss: 0.1318
2025-02-15 03:51:50,870 - Epoch [46/1000], Step [3600/4367], Loss: 0.2049
2025-02-15 03:52:25,854 - Epoch [46/1000], Step [3700/4367], Loss: 0.1068
2025-02-15 03:53:00,425 - Epoch [46/1000], Step [3800/4367], Loss: 0.1059
2025-02-15 03:53:35,148 - Epoch [46/1000], Step [3900/4367], Loss: 0.2615
2025-02-15 03:54:09,482 - Epoch [46/1000], Step [4000/4367], Loss: 0.1862
2025-02-15 03:54:44,054 - Epoch [46/1000], Step [4100/4367], Loss: 0.2678
2025-02-15 03:55:19,027 - Epoch [46/1000], Step [4200/4367], Loss: 0.0799
2025-02-15 03:55:53,649 - Epoch [46/1000], Step [4300/4367], Loss: 0.0615
2025-02-15 03:56:26,586 - Epoch [46/1000], Validation Step [100/1090], Val Loss: 0.0001
2025-02-15 03:56:35,773 - Epoch [46/1000], Validation Step [200/1090], Val Loss: 0.0001
2025-02-15 03:56:45,099 - Epoch [46/1000], Validation Step [300/1090], Val Loss: 0.2831
2025-02-15 03:56:54,744 - Epoch [46/1000], Validation Step [400/1090], Val Loss: 0.0855
2025-02-15 03:57:03,835 - Epoch [46/1000], Validation Step [500/1090], Val Loss: 0.4830
2025-02-15 03:57:13,356 - Epoch [46/1000], Validation Step [600/1090], Val Loss: 0.1292
2025-02-15 03:57:22,901 - Epoch [46/1000], Validation Step [700/1090], Val Loss: 0.1520
2025-02-15 03:57:31,680 - Epoch [46/1000], Validation Step [800/1090], Val Loss: 0.0047
2025-02-15 03:57:40,223 - Epoch [46/1000], Validation Step [900/1090], Val Loss: 0.0036
2025-02-15 03:57:49,454 - Epoch [46/1000], Validation Step [1000/1090], Val Loss: 0.0002
2025-02-15 03:57:58,072 - Epoch 46/1000, Train Loss: 0.1410, Val Loss: 0.1438, Accuracy: 94.72%
2025-02-15 03:57:58,467 - Model saved at /data2/personal/sungjin/korean_dialects/checkpoint_real/checkpoint_epoch_46.pth
2025-02-15 03:58:34,433 - Epoch [47/1000], Step [100/4367], Loss: 0.1480
2025-02-15 03:59:09,065 - Epoch [47/1000], Step [200/4367], Loss: 0.1412
2025-02-15 03:59:43,626 - Epoch [47/1000], Step [300/4367], Loss: 0.1035
2025-02-15 04:00:17,984 - Epoch [47/1000], Step [400/4367], Loss: 0.1287
2025-02-15 04:00:52,582 - Epoch [47/1000], Step [500/4367], Loss: 0.0853
2025-02-15 04:01:27,319 - Epoch [47/1000], Step [600/4367], Loss: 0.1133
2025-02-15 04:02:02,140 - Epoch [47/1000], Step [700/4367], Loss: 0.1874
2025-02-15 04:02:36,875 - Epoch [47/1000], Step [800/4367], Loss: 0.3655
2025-02-15 04:03:11,278 - Epoch [47/1000], Step [900/4367], Loss: 0.2059
2025-02-15 04:03:46,035 - Epoch [47/1000], Step [1000/4367], Loss: 0.1132
2025-02-15 04:04:20,440 - Epoch [47/1000], Step [1100/4367], Loss: 0.0665
2025-02-15 04:04:54,673 - Epoch [47/1000], Step [1200/4367], Loss: 0.0400
2025-02-15 04:05:29,427 - Epoch [47/1000], Step [1300/4367], Loss: 0.0379
2025-02-15 04:06:04,201 - Epoch [47/1000], Step [1400/4367], Loss: 0.1158
2025-02-15 04:06:39,004 - Epoch [47/1000], Step [1500/4367], Loss: 0.1593
2025-02-15 04:07:13,834 - Epoch [47/1000], Step [1600/4367], Loss: 0.1423
2025-02-15 04:07:48,805 - Epoch [47/1000], Step [1700/4367], Loss: 0.0352
2025-02-15 04:08:23,332 - Epoch [47/1000], Step [1800/4367], Loss: 0.0540
2025-02-15 04:08:57,872 - Epoch [47/1000], Step [1900/4367], Loss: 0.1537
2025-02-15 04:09:32,327 - Epoch [47/1000], Step [2000/4367], Loss: 0.1370
2025-02-15 04:10:07,161 - Epoch [47/1000], Step [2100/4367], Loss: 0.0627
2025-02-15 04:10:41,761 - Epoch [47/1000], Step [2200/4367], Loss: 0.1667
2025-02-15 04:11:16,355 - Epoch [47/1000], Step [2300/4367], Loss: 0.0320
2025-02-15 04:11:50,976 - Epoch [47/1000], Step [2400/4367], Loss: 0.1541
2025-02-15 04:12:26,052 - Epoch [47/1000], Step [2500/4367], Loss: 0.0437
2025-02-15 04:13:00,721 - Epoch [47/1000], Step [2600/4367], Loss: 0.2108
2025-02-15 04:13:35,414 - Epoch [47/1000], Step [2700/4367], Loss: 0.1898
2025-02-15 04:14:10,153 - Epoch [47/1000], Step [2800/4367], Loss: 0.1799
2025-02-15 04:14:44,733 - Epoch [47/1000], Step [2900/4367], Loss: 0.0457
2025-02-15 04:15:19,499 - Epoch [47/1000], Step [3000/4367], Loss: 0.0773
2025-02-15 04:15:54,135 - Epoch [47/1000], Step [3100/4367], Loss: 0.2791
2025-02-15 04:16:28,898 - Epoch [47/1000], Step [3200/4367], Loss: 0.0820
2025-02-15 04:17:03,340 - Epoch [47/1000], Step [3300/4367], Loss: 0.1014
2025-02-15 04:17:38,420 - Epoch [47/1000], Step [3400/4367], Loss: 0.0940
2025-02-15 04:18:13,297 - Epoch [47/1000], Step [3500/4367], Loss: 0.0533
2025-02-15 04:18:48,359 - Epoch [47/1000], Step [3600/4367], Loss: 0.2293
2025-02-15 04:19:23,217 - Epoch [47/1000], Step [3700/4367], Loss: 0.0907
2025-02-15 04:19:58,205 - Epoch [47/1000], Step [3800/4367], Loss: 0.0793
2025-02-15 04:20:32,811 - Epoch [47/1000], Step [3900/4367], Loss: 0.0423
2025-02-15 04:21:07,100 - Epoch [47/1000], Step [4000/4367], Loss: 0.0299
2025-02-15 04:21:41,633 - Epoch [47/1000], Step [4100/4367], Loss: 0.1030
2025-02-15 04:22:15,786 - Epoch [47/1000], Step [4200/4367], Loss: 0.1880
2025-02-15 04:22:50,698 - Epoch [47/1000], Step [4300/4367], Loss: 0.1638
2025-02-15 04:23:24,116 - Epoch [47/1000], Validation Step [100/1090], Val Loss: 0.0005
2025-02-15 04:23:33,331 - Epoch [47/1000], Validation Step [200/1090], Val Loss: 0.0029
2025-02-15 04:23:42,685 - Epoch [47/1000], Validation Step [300/1090], Val Loss: 0.2502
2025-02-15 04:23:52,356 - Epoch [47/1000], Validation Step [400/1090], Val Loss: 0.0888
2025-02-15 04:24:01,469 - Epoch [47/1000], Validation Step [500/1090], Val Loss: 0.4495
2025-02-15 04:24:11,004 - Epoch [47/1000], Validation Step [600/1090], Val Loss: 0.1571
2025-02-15 04:24:20,561 - Epoch [47/1000], Validation Step [700/1090], Val Loss: 0.1896
2025-02-15 04:24:29,340 - Epoch [47/1000], Validation Step [800/1090], Val Loss: 0.0051
2025-02-15 04:24:37,897 - Epoch [47/1000], Validation Step [900/1090], Val Loss: 0.0060
2025-02-15 04:24:47,159 - Epoch [47/1000], Validation Step [1000/1090], Val Loss: 0.0000
2025-02-15 04:24:55,792 - Epoch 47/1000, Train Loss: 0.1401, Val Loss: 0.1464, Accuracy: 94.65%
2025-02-15 04:25:31,656 - Epoch [48/1000], Step [100/4367], Loss: 0.0729
2025-02-15 04:26:06,728 - Epoch [48/1000], Step [200/4367], Loss: 0.0966
2025-02-15 04:26:41,234 - Epoch [48/1000], Step [300/4367], Loss: 0.3461
2025-02-15 04:27:15,924 - Epoch [48/1000], Step [400/4367], Loss: 0.2437
2025-02-15 04:27:50,800 - Epoch [48/1000], Step [500/4367], Loss: 0.0380
2025-02-15 04:28:25,884 - Epoch [48/1000], Step [600/4367], Loss: 0.1608
2025-02-15 04:29:00,683 - Epoch [48/1000], Step [700/4367], Loss: 0.0900
2025-02-15 04:29:35,195 - Epoch [48/1000], Step [800/4367], Loss: 0.0500
2025-02-15 04:30:10,095 - Epoch [48/1000], Step [900/4367], Loss: 0.0922
2025-02-15 04:30:44,702 - Epoch [48/1000], Step [1000/4367], Loss: 0.0585
2025-02-15 04:31:19,654 - Epoch [48/1000], Step [1100/4367], Loss: 0.1405
2025-02-15 04:31:54,141 - Epoch [48/1000], Step [1200/4367], Loss: 0.2569
2025-02-15 04:32:28,890 - Epoch [48/1000], Step [1300/4367], Loss: 0.0793
2025-02-15 04:33:03,730 - Epoch [48/1000], Step [1400/4367], Loss: 0.1229
2025-02-15 04:33:38,310 - Epoch [48/1000], Step [1500/4367], Loss: 0.0390
2025-02-15 04:34:12,615 - Epoch [48/1000], Step [1600/4367], Loss: 0.0258
2025-02-15 04:34:47,471 - Epoch [48/1000], Step [1700/4367], Loss: 0.3116
2025-02-15 04:35:22,230 - Epoch [48/1000], Step [1800/4367], Loss: 0.0889
2025-02-15 04:35:57,065 - Epoch [48/1000], Step [1900/4367], Loss: 0.1558
2025-02-15 04:36:31,936 - Epoch [48/1000], Step [2000/4367], Loss: 0.2661
2025-02-15 04:37:06,439 - Epoch [48/1000], Step [2100/4367], Loss: 0.1237
2025-02-15 04:37:41,085 - Epoch [48/1000], Step [2200/4367], Loss: 0.0793
2025-02-15 04:38:15,781 - Epoch [48/1000], Step [2300/4367], Loss: 0.1110
2025-02-15 04:38:50,602 - Epoch [48/1000], Step [2400/4367], Loss: 0.1397
2025-02-15 04:39:25,410 - Epoch [48/1000], Step [2500/4367], Loss: 0.0502
2025-02-15 04:40:00,377 - Epoch [48/1000], Step [2600/4367], Loss: 0.0966
2025-02-15 04:40:35,369 - Epoch [48/1000], Step [2700/4367], Loss: 0.0576
2025-02-15 04:41:10,568 - Epoch [48/1000], Step [2800/4367], Loss: 0.0316
2025-02-15 04:41:44,681 - Epoch [48/1000], Step [2900/4367], Loss: 0.2230
2025-02-15 04:42:19,346 - Epoch [48/1000], Step [3000/4367], Loss: 0.1002
2025-02-15 04:42:53,800 - Epoch [48/1000], Step [3100/4367], Loss: 0.1392
2025-02-15 04:43:28,032 - Epoch [48/1000], Step [3200/4367], Loss: 0.0503
2025-02-15 04:44:02,757 - Epoch [48/1000], Step [3300/4367], Loss: 0.1770
2025-02-15 04:44:37,276 - Epoch [48/1000], Step [3400/4367], Loss: 0.1705
2025-02-15 04:45:12,028 - Epoch [48/1000], Step [3500/4367], Loss: 0.2368
2025-02-15 04:45:46,560 - Epoch [48/1000], Step [3600/4367], Loss: 0.1432
2025-02-15 04:46:21,361 - Epoch [48/1000], Step [3700/4367], Loss: 0.0791
2025-02-15 04:46:55,692 - Epoch [48/1000], Step [3800/4367], Loss: 0.0669
2025-02-15 04:47:30,228 - Epoch [48/1000], Step [3900/4367], Loss: 0.2958
2025-02-15 04:48:05,348 - Epoch [48/1000], Step [4000/4367], Loss: 0.1254
2025-02-15 04:48:39,610 - Epoch [48/1000], Step [4100/4367], Loss: 0.2363
2025-02-15 04:49:14,408 - Epoch [48/1000], Step [4200/4367], Loss: 0.0715
2025-02-15 04:49:49,084 - Epoch [48/1000], Step [4300/4367], Loss: 0.0699
2025-02-15 04:50:22,572 - Epoch [48/1000], Validation Step [100/1090], Val Loss: 0.0004
2025-02-15 04:50:31,877 - Epoch [48/1000], Validation Step [200/1090], Val Loss: 0.0017
2025-02-15 04:50:41,410 - Epoch [48/1000], Validation Step [300/1090], Val Loss: 0.2560
2025-02-15 04:50:51,139 - Epoch [48/1000], Validation Step [400/1090], Val Loss: 0.1121
2025-02-15 04:51:00,329 - Epoch [48/1000], Validation Step [500/1090], Val Loss: 0.6278
2025-02-15 04:51:09,941 - Epoch [48/1000], Validation Step [600/1090], Val Loss: 0.0813
2025-02-15 04:51:19,622 - Epoch [48/1000], Validation Step [700/1090], Val Loss: 0.1078
2025-02-15 04:51:28,543 - Epoch [48/1000], Validation Step [800/1090], Val Loss: 0.0015
2025-02-15 04:51:37,289 - Epoch [48/1000], Validation Step [900/1090], Val Loss: 0.0015
2025-02-15 04:51:46,697 - Epoch [48/1000], Validation Step [1000/1090], Val Loss: 0.0001
2025-02-15 04:51:55,381 - Epoch 48/1000, Train Loss: 0.1394, Val Loss: 0.1527, Accuracy: 94.43%
2025-02-15 04:51:55,843 - Model saved at /data2/personal/sungjin/korean_dialects/checkpoint_real/checkpoint_epoch_48.pth
2025-02-15 04:52:32,608 - Epoch [49/1000], Step [100/4367], Loss: 0.1048
2025-02-15 04:53:07,672 - Epoch [49/1000], Step [200/4367], Loss: 0.0816
2025-02-15 04:53:42,545 - Epoch [49/1000], Step [300/4367], Loss: 0.0194
2025-02-15 04:54:17,587 - Epoch [49/1000], Step [400/4367], Loss: 0.1544
2025-02-15 04:54:52,622 - Epoch [49/1000], Step [500/4367], Loss: 0.1839
2025-02-15 04:55:27,689 - Epoch [49/1000], Step [600/4367], Loss: 0.1958
2025-02-15 04:56:02,684 - Epoch [49/1000], Step [700/4367], Loss: 0.0890
2025-02-15 04:56:37,234 - Epoch [49/1000], Step [800/4367], Loss: 0.1431
2025-02-15 04:57:11,914 - Epoch [49/1000], Step [900/4367], Loss: 0.0594
2025-02-15 04:57:46,884 - Epoch [49/1000], Step [1000/4367], Loss: 0.0795
2025-02-15 04:58:21,878 - Epoch [49/1000], Step [1100/4367], Loss: 0.1267
2025-02-15 04:58:56,422 - Epoch [49/1000], Step [1200/4367], Loss: 0.0408
2025-02-15 04:59:30,918 - Epoch [49/1000], Step [1300/4367], Loss: 0.1623
2025-02-15 05:00:05,764 - Epoch [49/1000], Step [1400/4367], Loss: 0.1548
2025-02-15 05:00:40,725 - Epoch [49/1000], Step [1500/4367], Loss: 0.1949
2025-02-15 05:01:15,584 - Epoch [49/1000], Step [1600/4367], Loss: 0.0424
2025-02-15 05:01:50,381 - Epoch [49/1000], Step [1700/4367], Loss: 0.1484
2025-02-15 05:02:25,087 - Epoch [49/1000], Step [1800/4367], Loss: 0.0869
2025-02-15 05:02:59,715 - Epoch [49/1000], Step [1900/4367], Loss: 0.1040
2025-02-15 05:03:34,956 - Epoch [49/1000], Step [2000/4367], Loss: 0.0727
2025-02-15 05:04:09,453 - Epoch [49/1000], Step [2100/4367], Loss: 0.1396
2025-02-15 05:04:44,374 - Epoch [49/1000], Step [2200/4367], Loss: 0.0335
2025-02-15 05:05:18,323 - Epoch [49/1000], Step [2300/4367], Loss: 0.1568
2025-02-15 05:05:52,689 - Epoch [49/1000], Step [2400/4367], Loss: 0.0586
2025-02-15 05:06:27,589 - Epoch [49/1000], Step [2500/4367], Loss: 0.0806
2025-02-15 05:07:02,822 - Epoch [49/1000], Step [2600/4367], Loss: 0.1928
2025-02-15 05:07:37,760 - Epoch [49/1000], Step [2700/4367], Loss: 0.1305
2025-02-15 05:08:12,631 - Epoch [49/1000], Step [2800/4367], Loss: 0.3921
2025-02-15 05:08:46,805 - Epoch [49/1000], Step [2900/4367], Loss: 0.0804
2025-02-15 05:09:21,712 - Epoch [49/1000], Step [3000/4367], Loss: 0.1592
2025-02-15 05:09:55,662 - Epoch [49/1000], Step [3100/4367], Loss: 0.0845
2025-02-15 05:10:30,293 - Epoch [49/1000], Step [3200/4367], Loss: 0.1115
2025-02-15 05:11:05,442 - Epoch [49/1000], Step [3300/4367], Loss: 0.1467
2025-02-15 05:11:40,308 - Epoch [49/1000], Step [3400/4367], Loss: 0.1487
2025-02-15 05:12:14,876 - Epoch [49/1000], Step [3500/4367], Loss: 0.3546
2025-02-15 05:12:49,078 - Epoch [49/1000], Step [3600/4367], Loss: 0.2935
2025-02-15 05:13:24,181 - Epoch [49/1000], Step [3700/4367], Loss: 0.1283
2025-02-15 05:13:58,940 - Epoch [49/1000], Step [3800/4367], Loss: 0.1942
2025-02-15 05:14:33,299 - Epoch [49/1000], Step [3900/4367], Loss: 0.0747
2025-02-15 05:15:08,044 - Epoch [49/1000], Step [4000/4367], Loss: 0.0710
2025-02-15 05:15:42,554 - Epoch [49/1000], Step [4100/4367], Loss: 0.1446
2025-02-15 05:16:17,307 - Epoch [49/1000], Step [4200/4367], Loss: 0.1030
2025-02-15 05:16:51,906 - Epoch [49/1000], Step [4300/4367], Loss: 0.0345
2025-02-15 05:17:25,409 - Epoch [49/1000], Validation Step [100/1090], Val Loss: 0.0001
2025-02-15 05:17:34,623 - Epoch [49/1000], Validation Step [200/1090], Val Loss: 0.0006
2025-02-15 05:17:43,972 - Epoch [49/1000], Validation Step [300/1090], Val Loss: 0.2387
2025-02-15 05:17:53,641 - Epoch [49/1000], Validation Step [400/1090], Val Loss: 0.1092
2025-02-15 05:18:02,758 - Epoch [49/1000], Validation Step [500/1090], Val Loss: 0.4856
2025-02-15 05:18:12,302 - Epoch [49/1000], Validation Step [600/1090], Val Loss: 0.1648
2025-02-15 05:18:21,878 - Epoch [49/1000], Validation Step [700/1090], Val Loss: 0.1470
2025-02-15 05:18:30,665 - Epoch [49/1000], Validation Step [800/1090], Val Loss: 0.0034
2025-02-15 05:18:39,226 - Epoch [49/1000], Validation Step [900/1090], Val Loss: 0.0041
2025-02-15 05:18:48,497 - Epoch [49/1000], Validation Step [1000/1090], Val Loss: 0.0001
2025-02-15 05:18:57,145 - Epoch 49/1000, Train Loss: 0.1393, Val Loss: 0.1444, Accuracy: 94.75%
2025-02-15 05:19:32,919 - Epoch [50/1000], Step [100/4367], Loss: 0.0748
2025-02-15 05:20:07,758 - Epoch [50/1000], Step [200/4367], Loss: 0.1075
2025-02-15 05:20:42,766 - Epoch [50/1000], Step [300/4367], Loss: 0.1194
2025-02-15 05:21:17,941 - Epoch [50/1000], Step [400/4367], Loss: 0.1809
2025-02-15 05:21:52,875 - Epoch [50/1000], Step [500/4367], Loss: 0.0666
2025-02-15 05:22:27,888 - Epoch [50/1000], Step [600/4367], Loss: 0.0833
2025-02-15 05:23:02,672 - Epoch [50/1000], Step [700/4367], Loss: 0.0782
2025-02-15 05:23:36,853 - Epoch [50/1000], Step [800/4367], Loss: 0.2502
2025-02-15 05:24:11,238 - Epoch [50/1000], Step [900/4367], Loss: 0.1498
2025-02-15 05:24:45,942 - Epoch [50/1000], Step [1000/4367], Loss: 0.4757
2025-02-15 05:25:20,315 - Epoch [50/1000], Step [1100/4367], Loss: 0.1615
2025-02-15 05:25:55,088 - Epoch [50/1000], Step [1200/4367], Loss: 0.2943
2025-02-15 05:26:29,998 - Epoch [50/1000], Step [1300/4367], Loss: 0.2511
2025-02-15 05:27:04,784 - Epoch [50/1000], Step [1400/4367], Loss: 0.1010
2025-02-15 05:27:39,804 - Epoch [50/1000], Step [1500/4367], Loss: 0.1035
2025-02-15 05:28:14,548 - Epoch [50/1000], Step [1600/4367], Loss: 0.1132
2025-02-15 05:28:49,495 - Epoch [50/1000], Step [1700/4367], Loss: 0.1046
2025-02-15 05:29:24,396 - Epoch [50/1000], Step [1800/4367], Loss: 0.2852
2025-02-15 05:29:58,716 - Epoch [50/1000], Step [1900/4367], Loss: 0.0161
2025-02-15 05:30:33,610 - Epoch [50/1000], Step [2000/4367], Loss: 0.2250
2025-02-15 05:31:07,709 - Epoch [50/1000], Step [2100/4367], Loss: 0.1115
2025-02-15 05:31:42,578 - Epoch [50/1000], Step [2200/4367], Loss: 0.2824
2025-02-15 05:32:17,041 - Epoch [50/1000], Step [2300/4367], Loss: 0.1081
2025-02-15 05:32:51,728 - Epoch [50/1000], Step [2400/4367], Loss: 0.0354
2025-02-15 05:33:26,884 - Epoch [50/1000], Step [2500/4367], Loss: 0.1464
2025-02-15 05:34:01,246 - Epoch [50/1000], Step [2600/4367], Loss: 0.1135
2025-02-15 05:34:35,836 - Epoch [50/1000], Step [2700/4367], Loss: 0.1511
2025-02-15 05:35:10,424 - Epoch [50/1000], Step [2800/4367], Loss: 0.1402
2025-02-15 05:35:45,286 - Epoch [50/1000], Step [2900/4367], Loss: 0.0279
2025-02-15 05:36:20,084 - Epoch [50/1000], Step [3000/4367], Loss: 0.1520
2025-02-15 05:36:55,356 - Epoch [50/1000], Step [3100/4367], Loss: 0.2246
2025-02-15 05:37:30,259 - Epoch [50/1000], Step [3200/4367], Loss: 0.0761
2025-02-15 05:38:05,368 - Epoch [50/1000], Step [3300/4367], Loss: 0.0119
2025-02-15 05:38:39,973 - Epoch [50/1000], Step [3400/4367], Loss: 0.4617
2025-02-15 05:39:14,758 - Epoch [50/1000], Step [3500/4367], Loss: 0.1284
2025-02-15 05:39:49,258 - Epoch [50/1000], Step [3600/4367], Loss: 0.1437
2025-02-15 05:40:23,819 - Epoch [50/1000], Step [3700/4367], Loss: 0.2036
2025-02-15 05:40:58,245 - Epoch [50/1000], Step [3800/4367], Loss: 0.2036
2025-02-15 05:41:32,919 - Epoch [50/1000], Step [3900/4367], Loss: 0.1591
2025-02-15 05:42:07,820 - Epoch [50/1000], Step [4000/4367], Loss: 0.2582
2025-02-15 05:42:42,218 - Epoch [50/1000], Step [4100/4367], Loss: 0.2122
2025-02-15 05:43:17,424 - Epoch [50/1000], Step [4200/4367], Loss: 0.0627
2025-02-15 05:43:52,052 - Epoch [50/1000], Step [4300/4367], Loss: 0.0560
2025-02-15 05:44:25,386 - Epoch [50/1000], Validation Step [100/1090], Val Loss: 0.0000
2025-02-15 05:44:34,575 - Epoch [50/1000], Validation Step [200/1090], Val Loss: 0.0002
2025-02-15 05:44:43,946 - Epoch [50/1000], Validation Step [300/1090], Val Loss: 0.2835
2025-02-15 05:44:53,645 - Epoch [50/1000], Validation Step [400/1090], Val Loss: 0.0888
2025-02-15 05:45:02,767 - Epoch [50/1000], Validation Step [500/1090], Val Loss: 0.5045
2025-02-15 05:45:12,314 - Epoch [50/1000], Validation Step [600/1090], Val Loss: 0.1291
2025-02-15 05:45:21,896 - Epoch [50/1000], Validation Step [700/1090], Val Loss: 0.1103
2025-02-15 05:45:30,698 - Epoch [50/1000], Validation Step [800/1090], Val Loss: 0.0020
2025-02-15 05:45:39,265 - Epoch [50/1000], Validation Step [900/1090], Val Loss: 0.0025
2025-02-15 05:45:48,538 - Epoch [50/1000], Validation Step [1000/1090], Val Loss: 0.0000
2025-02-15 05:45:57,171 - Epoch 50/1000, Train Loss: 0.1389, Val Loss: 0.1462, Accuracy: 94.64%
2025-02-15 05:45:57,642 - Model saved at /data2/personal/sungjin/korean_dialects/checkpoint_real/checkpoint_epoch_50.pth
2025-02-15 05:46:33,535 - Epoch [51/1000], Step [100/4367], Loss: 0.1436
2025-02-15 05:47:08,009 - Epoch [51/1000], Step [200/4367], Loss: 0.1523
2025-02-15 05:47:42,772 - Epoch [51/1000], Step [300/4367], Loss: 0.1119
2025-02-15 05:48:17,916 - Epoch [51/1000], Step [400/4367], Loss: 0.0997
2025-02-15 05:48:52,939 - Epoch [51/1000], Step [500/4367], Loss: 0.2100
2025-02-15 05:49:27,619 - Epoch [51/1000], Step [600/4367], Loss: 0.0868
2025-02-15 05:50:02,533 - Epoch [51/1000], Step [700/4367], Loss: 0.2109
2025-02-15 05:50:37,724 - Epoch [51/1000], Step [800/4367], Loss: 0.1456
2025-02-15 05:51:12,480 - Epoch [51/1000], Step [900/4367], Loss: 0.0507
2025-02-15 05:51:47,325 - Epoch [51/1000], Step [1000/4367], Loss: 0.2012
2025-02-15 05:52:22,248 - Epoch [51/1000], Step [1100/4367], Loss: 0.1599
2025-02-15 05:52:57,121 - Epoch [51/1000], Step [1200/4367], Loss: 0.1030
2025-02-15 05:53:31,728 - Epoch [51/1000], Step [1300/4367], Loss: 0.0949
2025-02-15 05:54:06,457 - Epoch [51/1000], Step [1400/4367], Loss: 0.1906
2025-02-15 05:54:41,541 - Epoch [51/1000], Step [1500/4367], Loss: 0.1653
2025-02-15 05:55:16,550 - Epoch [51/1000], Step [1600/4367], Loss: 0.0757
2025-02-15 05:55:51,439 - Epoch [51/1000], Step [1700/4367], Loss: 0.0325
2025-02-15 05:56:26,282 - Epoch [51/1000], Step [1800/4367], Loss: 0.3478
2025-02-15 05:57:01,001 - Epoch [51/1000], Step [1900/4367], Loss: 0.1255
2025-02-15 05:57:35,372 - Epoch [51/1000], Step [2000/4367], Loss: 0.1031
2025-02-15 05:58:10,321 - Epoch [51/1000], Step [2100/4367], Loss: 0.4606
2025-02-15 05:58:45,264 - Epoch [51/1000], Step [2200/4367], Loss: 0.2224
2025-02-15 05:59:19,811 - Epoch [51/1000], Step [2300/4367], Loss: 0.0852
2025-02-15 05:59:54,723 - Epoch [51/1000], Step [2400/4367], Loss: 0.1141
2025-02-15 06:00:29,241 - Epoch [51/1000], Step [2500/4367], Loss: 0.0161
2025-02-15 06:01:04,018 - Epoch [51/1000], Step [2600/4367], Loss: 0.1326
2025-02-15 06:01:39,022 - Epoch [51/1000], Step [2700/4367], Loss: 0.3441
2025-02-15 06:02:13,793 - Epoch [51/1000], Step [2800/4367], Loss: 0.0608
2025-02-15 06:02:48,627 - Epoch [51/1000], Step [2900/4367], Loss: 0.0964
2025-02-15 06:03:23,405 - Epoch [51/1000], Step [3000/4367], Loss: 0.0284
2025-02-15 06:03:57,886 - Epoch [51/1000], Step [3100/4367], Loss: 0.1204
2025-02-15 06:04:32,924 - Epoch [51/1000], Step [3200/4367], Loss: 0.1335
2025-02-15 06:05:08,179 - Epoch [51/1000], Step [3300/4367], Loss: 0.0137
2025-02-15 06:05:43,030 - Epoch [51/1000], Step [3400/4367], Loss: 0.1158
2025-02-15 06:06:18,239 - Epoch [51/1000], Step [3500/4367], Loss: 0.0820
2025-02-15 06:06:53,080 - Epoch [51/1000], Step [3600/4367], Loss: 0.1150
2025-02-15 06:07:27,725 - Epoch [51/1000], Step [3700/4367], Loss: 0.1451
2025-02-15 06:08:02,901 - Epoch [51/1000], Step [3800/4367], Loss: 0.0311
2025-02-15 06:08:37,533 - Epoch [51/1000], Step [3900/4367], Loss: 0.1429
2025-02-15 06:09:12,302 - Epoch [51/1000], Step [4000/4367], Loss: 0.1065
2025-02-15 06:09:46,941 - Epoch [51/1000], Step [4100/4367], Loss: 0.1194
2025-02-15 06:10:21,297 - Epoch [51/1000], Step [4200/4367], Loss: 0.1921
2025-02-15 06:10:56,012 - Epoch [51/1000], Step [4300/4367], Loss: 0.2333
2025-02-15 06:11:28,781 - Epoch [51/1000], Validation Step [100/1090], Val Loss: 0.0002
2025-02-15 06:11:37,978 - Epoch [51/1000], Validation Step [200/1090], Val Loss: 0.0021
2025-02-15 06:11:47,331 - Epoch [51/1000], Validation Step [300/1090], Val Loss: 0.2524
2025-02-15 06:11:57,006 - Epoch [51/1000], Validation Step [400/1090], Val Loss: 0.0680
2025-02-15 06:12:06,120 - Epoch [51/1000], Validation Step [500/1090], Val Loss: 0.4855
2025-02-15 06:12:15,649 - Epoch [51/1000], Validation Step [600/1090], Val Loss: 0.1545
2025-02-15 06:12:25,235 - Epoch [51/1000], Validation Step [700/1090], Val Loss: 0.1772
2025-02-15 06:12:34,037 - Epoch [51/1000], Validation Step [800/1090], Val Loss: 0.0028
2025-02-15 06:12:42,602 - Epoch [51/1000], Validation Step [900/1090], Val Loss: 0.0033
2025-02-15 06:12:51,877 - Epoch [51/1000], Validation Step [1000/1090], Val Loss: 0.0001
2025-02-15 06:13:00,526 - Epoch 51/1000, Train Loss: 0.1377, Val Loss: 0.1464, Accuracy: 94.64%
2025-02-15 06:13:36,395 - Epoch [52/1000], Step [100/4367], Loss: 0.2621
2025-02-15 06:14:11,531 - Epoch [52/1000], Step [200/4367], Loss: 0.0766
2025-02-15 06:14:46,182 - Epoch [52/1000], Step [300/4367], Loss: 0.2821
2025-02-15 06:15:20,432 - Epoch [52/1000], Step [400/4367], Loss: 0.1978
2025-02-15 06:15:55,157 - Epoch [52/1000], Step [500/4367], Loss: 0.2464
2025-02-15 06:16:30,081 - Epoch [52/1000], Step [600/4367], Loss: 0.0975
2025-02-15 06:17:04,982 - Epoch [52/1000], Step [700/4367], Loss: 0.2310
2025-02-15 06:17:39,793 - Epoch [52/1000], Step [800/4367], Loss: 0.1198
2025-02-15 06:18:14,849 - Epoch [52/1000], Step [900/4367], Loss: 0.1041
2025-02-15 06:18:49,525 - Epoch [52/1000], Step [1000/4367], Loss: 0.0622
2025-02-15 06:19:24,370 - Epoch [52/1000], Step [1100/4367], Loss: 0.1510
2025-02-15 06:19:59,295 - Epoch [52/1000], Step [1200/4367], Loss: 0.2609
2025-02-15 06:20:33,888 - Epoch [52/1000], Step [1300/4367], Loss: 0.0759
2025-02-15 06:21:08,817 - Epoch [52/1000], Step [1400/4367], Loss: 0.0354
2025-02-15 06:21:43,300 - Epoch [52/1000], Step [1500/4367], Loss: 0.1123
2025-02-15 06:22:18,128 - Epoch [52/1000], Step [1600/4367], Loss: 0.0878
2025-02-15 06:22:53,310 - Epoch [52/1000], Step [1700/4367], Loss: 0.1443
2025-02-15 06:23:27,870 - Epoch [52/1000], Step [1800/4367], Loss: 0.1275
2025-02-15 06:24:02,391 - Epoch [52/1000], Step [1900/4367], Loss: 0.1727
2025-02-15 06:24:37,032 - Epoch [52/1000], Step [2000/4367], Loss: 0.3162
2025-02-15 06:25:10,919 - Epoch [52/1000], Step [2100/4367], Loss: 0.1137
2025-02-15 06:25:45,514 - Epoch [52/1000], Step [2200/4367], Loss: 0.1006
2025-02-15 06:26:20,571 - Epoch [52/1000], Step [2300/4367], Loss: 0.1122
2025-02-15 06:26:55,402 - Epoch [52/1000], Step [2400/4367], Loss: 0.0172
2025-02-15 06:27:30,101 - Epoch [52/1000], Step [2500/4367], Loss: 0.0615
2025-02-15 06:28:04,990 - Epoch [52/1000], Step [2600/4367], Loss: 0.1685
2025-02-15 06:28:40,119 - Epoch [52/1000], Step [2700/4367], Loss: 0.2650
2025-02-15 06:29:14,732 - Epoch [52/1000], Step [2800/4367], Loss: 0.2096
2025-02-15 06:29:49,468 - Epoch [52/1000], Step [2900/4367], Loss: 0.0583
2025-02-15 06:30:24,367 - Epoch [52/1000], Step [3000/4367], Loss: 0.1223
2025-02-15 06:30:58,839 - Epoch [52/1000], Step [3100/4367], Loss: 0.1618
2025-02-15 06:31:33,628 - Epoch [52/1000], Step [3200/4367], Loss: 0.0703
2025-02-15 06:32:08,137 - Epoch [52/1000], Step [3300/4367], Loss: 0.2518
2025-02-15 06:32:42,855 - Epoch [52/1000], Step [3400/4367], Loss: 0.1000
2025-02-15 06:33:18,092 - Epoch [52/1000], Step [3500/4367], Loss: 0.1369
2025-02-15 06:33:52,960 - Epoch [52/1000], Step [3600/4367], Loss: 0.0726
2025-02-15 06:34:27,670 - Epoch [52/1000], Step [3700/4367], Loss: 0.0785
2025-02-15 06:35:02,350 - Epoch [52/1000], Step [3800/4367], Loss: 0.1963
2025-02-15 06:35:36,555 - Epoch [52/1000], Step [3900/4367], Loss: 0.1234
2025-02-15 06:36:11,192 - Epoch [52/1000], Step [4000/4367], Loss: 0.2945
2025-02-15 06:36:45,843 - Epoch [52/1000], Step [4100/4367], Loss: 0.1689
2025-02-15 06:37:20,882 - Epoch [52/1000], Step [4200/4367], Loss: 0.0539
2025-02-15 06:37:55,809 - Epoch [52/1000], Step [4300/4367], Loss: 0.2434
2025-02-15 06:38:28,950 - Epoch [52/1000], Validation Step [100/1090], Val Loss: 0.0004
2025-02-15 06:38:38,158 - Epoch [52/1000], Validation Step [200/1090], Val Loss: 0.0021
2025-02-15 06:38:47,506 - Epoch [52/1000], Validation Step [300/1090], Val Loss: 0.2498
2025-02-15 06:38:57,183 - Epoch [52/1000], Validation Step [400/1090], Val Loss: 0.0481
2025-02-15 06:39:06,302 - Epoch [52/1000], Validation Step [500/1090], Val Loss: 0.3994
2025-02-15 06:39:15,834 - Epoch [52/1000], Validation Step [600/1090], Val Loss: 0.1356
2025-02-15 06:39:25,388 - Epoch [52/1000], Validation Step [700/1090], Val Loss: 0.1155
2025-02-15 06:39:34,132 - Epoch [52/1000], Validation Step [800/1090], Val Loss: 0.0036
2025-02-15 06:39:42,655 - Epoch [52/1000], Validation Step [900/1090], Val Loss: 0.0036
2025-02-15 06:39:51,861 - Epoch [52/1000], Validation Step [1000/1090], Val Loss: 0.0000
2025-02-15 06:40:00,444 - Epoch 52/1000, Train Loss: 0.1376, Val Loss: 0.1421, Accuracy: 94.82%
2025-02-15 06:40:00,870 - Model saved at /data2/personal/sungjin/korean_dialects/checkpoint_real/checkpoint_epoch_52.pth
2025-02-15 06:40:36,809 - Epoch [53/1000], Step [100/4367], Loss: 0.1968
2025-02-15 06:41:11,789 - Epoch [53/1000], Step [200/4367], Loss: 0.3370
2025-02-15 06:41:46,396 - Epoch [53/1000], Step [300/4367], Loss: 0.1341
2025-02-15 06:42:21,170 - Epoch [53/1000], Step [400/4367], Loss: 0.2032
2025-02-15 06:42:55,580 - Epoch [53/1000], Step [500/4367], Loss: 0.1946
2025-02-15 06:43:30,529 - Epoch [53/1000], Step [600/4367], Loss: 0.1255
2025-02-15 06:44:05,168 - Epoch [53/1000], Step [700/4367], Loss: 0.1108
2025-02-15 06:44:40,460 - Epoch [53/1000], Step [800/4367], Loss: 0.0391
2025-02-15 06:45:15,290 - Epoch [53/1000], Step [900/4367], Loss: 0.1794
2025-02-15 06:45:50,157 - Epoch [53/1000], Step [1000/4367], Loss: 0.1148
2025-02-15 06:46:25,107 - Epoch [53/1000], Step [1100/4367], Loss: 0.1688
2025-02-15 06:46:59,754 - Epoch [53/1000], Step [1200/4367], Loss: 0.0749
2025-02-15 06:47:34,606 - Epoch [53/1000], Step [1300/4367], Loss: 0.0320
2025-02-15 06:48:08,964 - Epoch [53/1000], Step [1400/4367], Loss: 0.1662
2025-02-15 06:48:43,979 - Epoch [53/1000], Step [1500/4367], Loss: 0.0598
2025-02-15 06:49:18,737 - Epoch [53/1000], Step [1600/4367], Loss: 0.0559
2025-02-15 06:49:53,805 - Epoch [53/1000], Step [1700/4367], Loss: 0.4453
2025-02-15 06:50:28,218 - Epoch [53/1000], Step [1800/4367], Loss: 0.1504
2025-02-15 06:51:03,069 - Epoch [53/1000], Step [1900/4367], Loss: 0.1978
2025-02-15 06:51:37,844 - Epoch [53/1000], Step [2000/4367], Loss: 0.1111
2025-02-15 06:52:12,831 - Epoch [53/1000], Step [2100/4367], Loss: 0.0349
2025-02-15 06:52:47,889 - Epoch [53/1000], Step [2200/4367], Loss: 0.1420
2025-02-15 06:53:21,803 - Epoch [53/1000], Step [2300/4367], Loss: 0.0555
2025-02-15 06:53:56,694 - Epoch [53/1000], Step [2400/4367], Loss: 0.2412
2025-02-15 06:54:31,480 - Epoch [53/1000], Step [2500/4367], Loss: 0.0525
2025-02-15 06:55:06,035 - Epoch [53/1000], Step [2600/4367], Loss: 0.0277
2025-02-15 06:55:40,917 - Epoch [53/1000], Step [2700/4367], Loss: 0.0415
2025-02-15 06:56:15,608 - Epoch [53/1000], Step [2800/4367], Loss: 0.0636
2025-02-15 06:56:49,644 - Epoch [53/1000], Step [2900/4367], Loss: 0.0532
2025-02-15 06:57:24,502 - Epoch [53/1000], Step [3000/4367], Loss: 0.1464
2025-02-15 06:57:59,305 - Epoch [53/1000], Step [3100/4367], Loss: 0.1866
2025-02-15 06:58:34,150 - Epoch [53/1000], Step [3200/4367], Loss: 0.0875
2025-02-15 06:59:09,072 - Epoch [53/1000], Step [3300/4367], Loss: 0.1183
2025-02-15 06:59:43,869 - Epoch [53/1000], Step [3400/4367], Loss: 0.0507
2025-02-15 07:00:18,316 - Epoch [53/1000], Step [3500/4367], Loss: 0.0517
2025-02-15 07:00:53,165 - Epoch [53/1000], Step [3600/4367], Loss: 0.2269
2025-02-15 07:01:28,136 - Epoch [53/1000], Step [3700/4367], Loss: 0.0824
2025-02-15 07:02:02,918 - Epoch [53/1000], Step [3800/4367], Loss: 0.0657
2025-02-15 07:02:37,426 - Epoch [53/1000], Step [3900/4367], Loss: 0.1870
2025-02-15 07:03:12,092 - Epoch [53/1000], Step [4000/4367], Loss: 0.1662
2025-02-15 07:03:46,492 - Epoch [53/1000], Step [4100/4367], Loss: 0.0663
2025-02-15 07:04:21,301 - Epoch [53/1000], Step [4200/4367], Loss: 0.1570
2025-02-15 07:04:55,815 - Epoch [53/1000], Step [4300/4367], Loss: 0.1116
2025-02-15 07:05:29,020 - Epoch [53/1000], Validation Step [100/1090], Val Loss: 0.0002
2025-02-15 07:05:38,229 - Epoch [53/1000], Validation Step [200/1090], Val Loss: 0.0005
2025-02-15 07:05:47,587 - Epoch [53/1000], Validation Step [300/1090], Val Loss: 0.2630
2025-02-15 07:05:57,248 - Epoch [53/1000], Validation Step [400/1090], Val Loss: 0.0435
2025-02-15 07:06:06,361 - Epoch [53/1000], Validation Step [500/1090], Val Loss: 0.3828
2025-02-15 07:06:15,916 - Epoch [53/1000], Validation Step [600/1090], Val Loss: 0.1468
2025-02-15 07:06:25,507 - Epoch [53/1000], Validation Step [700/1090], Val Loss: 0.1411
2025-02-15 07:06:34,313 - Epoch [53/1000], Validation Step [800/1090], Val Loss: 0.0055
2025-02-15 07:06:42,885 - Epoch [53/1000], Validation Step [900/1090], Val Loss: 0.0046
2025-02-15 07:06:52,166 - Epoch [53/1000], Validation Step [1000/1090], Val Loss: 0.0000
2025-02-15 07:07:00,789 - Epoch 53/1000, Train Loss: 0.1364, Val Loss: 0.1452, Accuracy: 94.74%
2025-02-15 07:07:36,388 - Epoch [54/1000], Step [100/4367], Loss: 0.2043
2025-02-15 07:08:11,102 - Epoch [54/1000], Step [200/4367], Loss: 0.1009
2025-02-15 07:08:45,713 - Epoch [54/1000], Step [300/4367], Loss: 0.1213
2025-02-15 07:09:20,470 - Epoch [54/1000], Step [400/4367], Loss: 0.1199
2025-02-15 07:09:55,113 - Epoch [54/1000], Step [500/4367], Loss: 0.0796
2025-02-15 07:10:29,979 - Epoch [54/1000], Step [600/4367], Loss: 0.1829
2025-02-15 07:11:04,606 - Epoch [54/1000], Step [700/4367], Loss: 0.0634
2025-02-15 07:11:39,050 - Epoch [54/1000], Step [800/4367], Loss: 0.1995
2025-02-15 07:12:13,619 - Epoch [54/1000], Step [900/4367], Loss: 0.1292
2025-02-15 07:12:48,238 - Epoch [54/1000], Step [1000/4367], Loss: 0.2329
2025-02-15 07:13:23,015 - Epoch [54/1000], Step [1100/4367], Loss: 0.0498
2025-02-15 07:13:57,963 - Epoch [54/1000], Step [1200/4367], Loss: 0.0558
2025-02-15 07:14:33,025 - Epoch [54/1000], Step [1300/4367], Loss: 0.0568
2025-02-15 07:15:08,054 - Epoch [54/1000], Step [1400/4367], Loss: 0.1159
2025-02-15 07:15:42,788 - Epoch [54/1000], Step [1500/4367], Loss: 0.0987
2025-02-15 07:16:17,662 - Epoch [54/1000], Step [1600/4367], Loss: 0.0853
2025-02-15 07:16:52,390 - Epoch [54/1000], Step [1700/4367], Loss: 0.1562
2025-02-15 07:17:27,202 - Epoch [54/1000], Step [1800/4367], Loss: 0.0861
2025-02-15 07:18:02,295 - Epoch [54/1000], Step [1900/4367], Loss: 0.3276
2025-02-15 07:18:36,365 - Epoch [54/1000], Step [2000/4367], Loss: 0.1617
2025-02-15 07:19:10,926 - Epoch [54/1000], Step [2100/4367], Loss: 0.1205
2025-02-15 07:19:45,826 - Epoch [54/1000], Step [2200/4367], Loss: 0.1790
2025-02-15 07:20:20,655 - Epoch [54/1000], Step [2300/4367], Loss: 0.1126
2025-02-15 07:20:55,515 - Epoch [54/1000], Step [2400/4367], Loss: 0.0404
2025-02-15 07:21:30,320 - Epoch [54/1000], Step [2500/4367], Loss: 0.2382
2025-02-15 07:22:05,131 - Epoch [54/1000], Step [2600/4367], Loss: 0.0780
2025-02-15 07:22:39,461 - Epoch [54/1000], Step [2700/4367], Loss: 0.1250
2025-02-15 07:23:14,488 - Epoch [54/1000], Step [2800/4367], Loss: 0.0495
2025-02-15 07:23:49,641 - Epoch [54/1000], Step [2900/4367], Loss: 0.0512
2025-02-15 07:24:24,514 - Epoch [54/1000], Step [3000/4367], Loss: 0.0946
2025-02-15 07:24:59,146 - Epoch [54/1000], Step [3100/4367], Loss: 0.1411
2025-02-15 07:25:34,070 - Epoch [54/1000], Step [3200/4367], Loss: 0.0830
2025-02-15 07:26:09,099 - Epoch [54/1000], Step [3300/4367], Loss: 0.3280
2025-02-15 07:26:43,931 - Epoch [54/1000], Step [3400/4367], Loss: 0.2064
2025-02-15 07:27:18,528 - Epoch [54/1000], Step [3500/4367], Loss: 0.1802
2025-02-15 07:27:53,051 - Epoch [54/1000], Step [3600/4367], Loss: 0.0594
2025-02-15 07:28:27,822 - Epoch [54/1000], Step [3700/4367], Loss: 0.0842
2025-02-15 07:29:02,638 - Epoch [54/1000], Step [3800/4367], Loss: 0.1110
2025-02-15 07:29:37,082 - Epoch [54/1000], Step [3900/4367], Loss: 0.0494
2025-02-15 07:30:12,039 - Epoch [54/1000], Step [4000/4367], Loss: 0.0801
2025-02-15 07:30:46,888 - Epoch [54/1000], Step [4100/4367], Loss: 0.1430
2025-02-15 07:31:21,811 - Epoch [54/1000], Step [4200/4367], Loss: 0.1199
2025-02-15 07:31:56,921 - Epoch [54/1000], Step [4300/4367], Loss: 0.1531
2025-02-15 07:32:30,010 - Epoch [54/1000], Validation Step [100/1090], Val Loss: 0.0002
2025-02-15 07:32:39,238 - Epoch [54/1000], Validation Step [200/1090], Val Loss: 0.0004
2025-02-15 07:32:48,606 - Epoch [54/1000], Validation Step [300/1090], Val Loss: 0.2847
2025-02-15 07:32:58,286 - Epoch [54/1000], Validation Step [400/1090], Val Loss: 0.0589
2025-02-15 07:33:07,415 - Epoch [54/1000], Validation Step [500/1090], Val Loss: 0.4188
2025-02-15 07:33:16,967 - Epoch [54/1000], Validation Step [600/1090], Val Loss: 0.1079
2025-02-15 07:33:26,554 - Epoch [54/1000], Validation Step [700/1090], Val Loss: 0.1227
2025-02-15 07:33:35,366 - Epoch [54/1000], Validation Step [800/1090], Val Loss: 0.0040
2025-02-15 07:33:43,958 - Epoch [54/1000], Validation Step [900/1090], Val Loss: 0.0037
2025-02-15 07:33:53,219 - Epoch [54/1000], Validation Step [1000/1090], Val Loss: 0.0001
2025-02-15 07:34:01,881 - Epoch 54/1000, Train Loss: 0.1354, Val Loss: 0.1417, Accuracy: 94.84%
2025-02-15 07:34:02,314 - Model saved at /data2/personal/sungjin/korean_dialects/checkpoint_real/checkpoint_epoch_54.pth
2025-02-15 07:34:38,322 - Epoch [55/1000], Step [100/4367], Loss: 0.0455
2025-02-15 07:35:12,662 - Epoch [55/1000], Step [200/4367], Loss: 0.0317
2025-02-15 07:35:47,658 - Epoch [55/1000], Step [300/4367], Loss: 0.1469
2025-02-15 07:36:22,020 - Epoch [55/1000], Step [400/4367], Loss: 0.0923
2025-02-15 07:36:56,679 - Epoch [55/1000], Step [500/4367], Loss: 0.1473
2025-02-15 07:37:31,724 - Epoch [55/1000], Step [600/4367], Loss: 0.2968
2025-02-15 07:38:06,668 - Epoch [55/1000], Step [700/4367], Loss: 0.2124
2025-02-15 07:38:41,378 - Epoch [55/1000], Step [800/4367], Loss: 0.0712
2025-02-15 07:39:16,086 - Epoch [55/1000], Step [900/4367], Loss: 0.0699
2025-02-15 07:39:50,981 - Epoch [55/1000], Step [1000/4367], Loss: 0.1610
2025-02-15 07:40:25,943 - Epoch [55/1000], Step [1100/4367], Loss: 0.1912
2025-02-15 07:41:00,491 - Epoch [55/1000], Step [1200/4367], Loss: 0.1905
2025-02-15 07:41:35,383 - Epoch [55/1000], Step [1300/4367], Loss: 0.1101
2025-02-15 07:42:10,280 - Epoch [55/1000], Step [1400/4367], Loss: 0.0821
2025-02-15 07:42:44,889 - Epoch [55/1000], Step [1500/4367], Loss: 0.1744
2025-02-15 07:43:19,479 - Epoch [55/1000], Step [1600/4367], Loss: 0.1104
2025-02-15 07:43:54,305 - Epoch [55/1000], Step [1700/4367], Loss: 0.2720
2025-02-15 07:44:28,664 - Epoch [55/1000], Step [1800/4367], Loss: 0.0619
2025-02-15 07:45:03,804 - Epoch [55/1000], Step [1900/4367], Loss: 0.0179
2025-02-15 07:45:38,757 - Epoch [55/1000], Step [2000/4367], Loss: 0.2615
2025-02-15 07:46:13,519 - Epoch [55/1000], Step [2100/4367], Loss: 0.0620
2025-02-15 07:46:48,639 - Epoch [55/1000], Step [2200/4367], Loss: 0.1509
2025-02-15 07:47:23,491 - Epoch [55/1000], Step [2300/4367], Loss: 0.0347
2025-02-15 07:47:58,487 - Epoch [55/1000], Step [2400/4367], Loss: 0.0226
2025-02-15 07:48:32,482 - Epoch [55/1000], Step [2500/4367], Loss: 0.0553
2025-02-15 07:49:07,184 - Epoch [55/1000], Step [2600/4367], Loss: 0.2186
2025-02-15 07:49:42,300 - Epoch [55/1000], Step [2700/4367], Loss: 0.2241
2025-02-15 07:50:17,151 - Epoch [55/1000], Step [2800/4367], Loss: 0.3342
2025-02-15 07:50:52,246 - Epoch [55/1000], Step [2900/4367], Loss: 0.1367
2025-02-15 07:51:27,050 - Epoch [55/1000], Step [3000/4367], Loss: 0.0722
2025-02-15 07:52:01,770 - Epoch [55/1000], Step [3100/4367], Loss: 0.0672
2025-02-15 07:52:36,564 - Epoch [55/1000], Step [3200/4367], Loss: 0.0275
2025-02-15 07:53:10,934 - Epoch [55/1000], Step [3300/4367], Loss: 0.3968
2025-02-15 07:53:45,788 - Epoch [55/1000], Step [3400/4367], Loss: 0.1399
2025-02-15 07:54:20,394 - Epoch [55/1000], Step [3500/4367], Loss: 0.0721
2025-02-15 07:54:54,441 - Epoch [55/1000], Step [3600/4367], Loss: 0.2043
2025-02-15 07:55:29,056 - Epoch [55/1000], Step [3700/4367], Loss: 0.2299
2025-02-15 07:56:03,755 - Epoch [55/1000], Step [3800/4367], Loss: 0.0422
2025-02-15 07:56:38,423 - Epoch [55/1000], Step [3900/4367], Loss: 0.2444
2025-02-15 07:57:12,921 - Epoch [55/1000], Step [4000/4367], Loss: 0.1380
2025-02-15 07:57:47,761 - Epoch [55/1000], Step [4100/4367], Loss: 0.1607
2025-02-15 07:58:22,465 - Epoch [55/1000], Step [4200/4367], Loss: 0.1222
2025-02-15 07:58:57,205 - Epoch [55/1000], Step [4300/4367], Loss: 0.2463
2025-02-15 07:59:30,929 - Epoch [55/1000], Validation Step [100/1090], Val Loss: 0.0001
2025-02-15 07:59:40,163 - Epoch [55/1000], Validation Step [200/1090], Val Loss: 0.0005
2025-02-15 07:59:49,526 - Epoch [55/1000], Validation Step [300/1090], Val Loss: 0.2816
2025-02-15 07:59:59,220 - Epoch [55/1000], Validation Step [400/1090], Val Loss: 0.0635
2025-02-15 08:00:08,356 - Epoch [55/1000], Validation Step [500/1090], Val Loss: 0.4323
2025-02-15 08:00:17,918 - Epoch [55/1000], Validation Step [600/1090], Val Loss: 0.1114
2025-02-15 08:00:27,502 - Epoch [55/1000], Validation Step [700/1090], Val Loss: 0.1310
2025-02-15 08:00:36,307 - Epoch [55/1000], Validation Step [800/1090], Val Loss: 0.0036
2025-02-15 08:00:44,891 - Epoch [55/1000], Validation Step [900/1090], Val Loss: 0.0031
2025-02-15 08:00:54,174 - Epoch [55/1000], Validation Step [1000/1090], Val Loss: 0.0000
2025-02-15 08:01:02,826 - Epoch 55/1000, Train Loss: 0.1363, Val Loss: 0.1418, Accuracy: 94.79%
2025-02-15 08:01:38,866 - Epoch [56/1000], Step [100/4367], Loss: 0.0247
2025-02-15 08:02:14,151 - Epoch [56/1000], Step [200/4367], Loss: 0.0406
2025-02-15 08:02:48,959 - Epoch [56/1000], Step [300/4367], Loss: 0.2328
2025-02-15 08:03:23,688 - Epoch [56/1000], Step [400/4367], Loss: 0.0277
2025-02-15 08:03:58,935 - Epoch [56/1000], Step [500/4367], Loss: 0.0156
2025-02-15 08:04:34,252 - Epoch [56/1000], Step [600/4367], Loss: 0.0564
2025-02-15 08:05:09,242 - Epoch [56/1000], Step [700/4367], Loss: 0.1491
2025-02-15 08:05:43,753 - Epoch [56/1000], Step [800/4367], Loss: 0.2111
2025-02-15 08:06:18,411 - Epoch [56/1000], Step [900/4367], Loss: 0.0475
2025-02-15 08:06:52,942 - Epoch [56/1000], Step [1000/4367], Loss: 0.2270
2025-02-15 08:07:27,871 - Epoch [56/1000], Step [1100/4367], Loss: 0.0436
2025-02-15 08:08:02,952 - Epoch [56/1000], Step [1200/4367], Loss: 0.1079
2025-02-15 08:08:37,647 - Epoch [56/1000], Step [1300/4367], Loss: 0.0650
2025-02-15 08:09:11,995 - Epoch [56/1000], Step [1400/4367], Loss: 0.0936
2025-02-15 08:09:46,406 - Epoch [56/1000], Step [1500/4367], Loss: 0.0748
2025-02-15 08:10:20,963 - Epoch [56/1000], Step [1600/4367], Loss: 0.1746
2025-02-15 08:10:55,874 - Epoch [56/1000], Step [1700/4367], Loss: 0.1723
2025-02-15 08:11:30,388 - Epoch [56/1000], Step [1800/4367], Loss: 0.0876
2025-02-15 08:12:05,321 - Epoch [56/1000], Step [1900/4367], Loss: 0.0704
2025-02-15 08:12:39,602 - Epoch [56/1000], Step [2000/4367], Loss: 0.1529
2025-02-15 08:13:14,059 - Epoch [56/1000], Step [2100/4367], Loss: 0.1853
2025-02-15 08:13:48,653 - Epoch [56/1000], Step [2200/4367], Loss: 0.2544
2025-02-15 08:14:22,869 - Epoch [56/1000], Step [2300/4367], Loss: 0.1182
2025-02-15 08:14:57,039 - Epoch [56/1000], Step [2400/4367], Loss: 0.0237
2025-02-15 08:15:31,610 - Epoch [56/1000], Step [2500/4367], Loss: 0.0087
2025-02-15 08:16:06,313 - Epoch [56/1000], Step [2600/4367], Loss: 0.1540
2025-02-15 08:16:40,907 - Epoch [56/1000], Step [2700/4367], Loss: 0.0543
2025-02-15 08:17:15,359 - Epoch [56/1000], Step [2800/4367], Loss: 0.1563
2025-02-15 08:17:49,800 - Epoch [56/1000], Step [2900/4367], Loss: 0.0919
2025-02-15 08:18:24,541 - Epoch [56/1000], Step [3000/4367], Loss: 0.1669
2025-02-15 08:18:59,177 - Epoch [56/1000], Step [3100/4367], Loss: 0.0693
2025-02-15 08:19:33,675 - Epoch [56/1000], Step [3200/4367], Loss: 0.0775
2025-02-15 08:20:08,502 - Epoch [56/1000], Step [3300/4367], Loss: 0.0981
2025-02-15 08:20:43,147 - Epoch [56/1000], Step [3400/4367], Loss: 0.0884
2025-02-15 08:21:17,931 - Epoch [56/1000], Step [3500/4367], Loss: 0.2147
2025-02-15 08:21:52,657 - Epoch [56/1000], Step [3600/4367], Loss: 0.3166
2025-02-15 08:22:27,435 - Epoch [56/1000], Step [3700/4367], Loss: 0.2433
2025-02-15 08:23:02,607 - Epoch [56/1000], Step [3800/4367], Loss: 0.0450
2025-02-15 08:23:37,228 - Epoch [56/1000], Step [3900/4367], Loss: 0.0606
2025-02-15 08:24:11,862 - Epoch [56/1000], Step [4000/4367], Loss: 0.2870
2025-02-15 08:24:46,780 - Epoch [56/1000], Step [4100/4367], Loss: 0.0313
2025-02-15 08:25:21,949 - Epoch [56/1000], Step [4200/4367], Loss: 0.0895
2025-02-15 08:25:56,947 - Epoch [56/1000], Step [4300/4367], Loss: 0.1787
2025-02-15 08:26:30,113 - Epoch [56/1000], Validation Step [100/1090], Val Loss: 0.0000
2025-02-15 08:26:39,311 - Epoch [56/1000], Validation Step [200/1090], Val Loss: 0.0002
2025-02-15 08:26:48,664 - Epoch [56/1000], Validation Step [300/1090], Val Loss: 0.2486
2025-02-15 08:26:58,337 - Epoch [56/1000], Validation Step [400/1090], Val Loss: 0.0598
2025-02-15 08:27:07,442 - Epoch [56/1000], Validation Step [500/1090], Val Loss: 0.4310
2025-02-15 08:27:16,972 - Epoch [56/1000], Validation Step [600/1090], Val Loss: 0.1358
2025-02-15 08:27:26,539 - Epoch [56/1000], Validation Step [700/1090], Val Loss: 0.1398
2025-02-15 08:27:35,344 - Epoch [56/1000], Validation Step [800/1090], Val Loss: 0.0032
2025-02-15 08:27:43,929 - Epoch [56/1000], Validation Step [900/1090], Val Loss: 0.0034
2025-02-15 08:27:53,201 - Epoch [56/1000], Validation Step [1000/1090], Val Loss: 0.0000
2025-02-15 08:28:01,843 - Epoch 56/1000, Train Loss: 0.1381, Val Loss: 0.1417, Accuracy: 94.75%
2025-02-15 08:28:02,258 - Model saved at /data2/personal/sungjin/korean_dialects/checkpoint_real/checkpoint_epoch_56.pth
2025-02-15 08:28:38,305 - Epoch [57/1000], Step [100/4367], Loss: 0.0829
2025-02-15 08:29:13,154 - Epoch [57/1000], Step [200/4367], Loss: 0.0975
2025-02-15 08:29:48,358 - Epoch [57/1000], Step [300/4367], Loss: 0.0782
2025-02-15 08:30:23,373 - Epoch [57/1000], Step [400/4367], Loss: 0.0600
2025-02-15 08:30:58,260 - Epoch [57/1000], Step [500/4367], Loss: 0.1929
2025-02-15 08:31:32,916 - Epoch [57/1000], Step [600/4367], Loss: 0.1550
2025-02-15 08:32:07,763 - Epoch [57/1000], Step [700/4367], Loss: 0.2357
2025-02-15 08:32:42,868 - Epoch [57/1000], Step [800/4367], Loss: 0.1344
2025-02-15 08:33:17,652 - Epoch [57/1000], Step [900/4367], Loss: 0.1623
2025-02-15 08:33:52,928 - Epoch [57/1000], Step [1000/4367], Loss: 0.1105
2025-02-15 08:34:28,247 - Epoch [57/1000], Step [1100/4367], Loss: 0.0585
2025-02-15 08:35:02,806 - Epoch [57/1000], Step [1200/4367], Loss: 0.0480
2025-02-15 08:35:37,711 - Epoch [57/1000], Step [1300/4367], Loss: 0.1115
2025-02-15 08:36:12,612 - Epoch [57/1000], Step [1400/4367], Loss: 0.2087
2025-02-15 08:36:47,026 - Epoch [57/1000], Step [1500/4367], Loss: 0.2622
2025-02-15 08:37:21,819 - Epoch [57/1000], Step [1600/4367], Loss: 0.1312
2025-02-15 08:37:56,842 - Epoch [57/1000], Step [1700/4367], Loss: 0.0634
2025-02-15 08:38:31,023 - Epoch [57/1000], Step [1800/4367], Loss: 0.3063
2025-02-15 08:39:05,902 - Epoch [57/1000], Step [1900/4367], Loss: 0.1325
2025-02-15 08:39:40,916 - Epoch [57/1000], Step [2000/4367], Loss: 0.1344
2025-02-15 08:40:15,880 - Epoch [57/1000], Step [2100/4367], Loss: 0.1789
2025-02-15 08:40:50,785 - Epoch [57/1000], Step [2200/4367], Loss: 0.1379
2025-02-15 08:41:25,437 - Epoch [57/1000], Step [2300/4367], Loss: 0.0727
2025-02-15 08:42:00,174 - Epoch [57/1000], Step [2400/4367], Loss: 0.0830
2025-02-15 08:42:35,043 - Epoch [57/1000], Step [2500/4367], Loss: 0.3139
2025-02-15 08:43:09,716 - Epoch [57/1000], Step [2600/4367], Loss: 0.1469
2025-02-15 08:43:44,565 - Epoch [57/1000], Step [2700/4367], Loss: 0.2588
2025-02-15 08:44:19,642 - Epoch [57/1000], Step [2800/4367], Loss: 0.3092
2025-02-15 08:44:54,488 - Epoch [57/1000], Step [2900/4367], Loss: 0.1345
2025-02-15 08:45:28,938 - Epoch [57/1000], Step [3000/4367], Loss: 0.0394
2025-02-15 08:46:03,647 - Epoch [57/1000], Step [3100/4367], Loss: 0.0516
2025-02-15 08:46:38,334 - Epoch [57/1000], Step [3200/4367], Loss: 0.1798
2025-02-15 08:47:13,224 - Epoch [57/1000], Step [3300/4367], Loss: 0.1943
2025-02-15 08:47:47,684 - Epoch [57/1000], Step [3400/4367], Loss: 0.2431
2025-02-15 08:48:22,583 - Epoch [57/1000], Step [3500/4367], Loss: 0.2239
2025-02-15 08:48:57,434 - Epoch [57/1000], Step [3600/4367], Loss: 0.1475
2025-02-15 08:49:32,292 - Epoch [57/1000], Step [3700/4367], Loss: 0.1865
2025-02-15 08:50:06,892 - Epoch [57/1000], Step [3800/4367], Loss: 0.1183
2025-02-15 08:50:41,569 - Epoch [57/1000], Step [3900/4367], Loss: 0.2271
2025-02-15 08:51:16,487 - Epoch [57/1000], Step [4000/4367], Loss: 0.2798
2025-02-15 08:51:51,245 - Epoch [57/1000], Step [4100/4367], Loss: 0.0975
2025-02-15 08:52:26,481 - Epoch [57/1000], Step [4200/4367], Loss: 0.2491
2025-02-15 08:53:01,393 - Epoch [57/1000], Step [4300/4367], Loss: 0.3809
2025-02-15 08:53:34,656 - Epoch [57/1000], Validation Step [100/1090], Val Loss: 0.0002
2025-02-15 08:53:43,876 - Epoch [57/1000], Validation Step [200/1090], Val Loss: 0.0011
2025-02-15 08:53:53,233 - Epoch [57/1000], Validation Step [300/1090], Val Loss: 0.2479
2025-02-15 08:54:02,916 - Epoch [57/1000], Validation Step [400/1090], Val Loss: 0.0581
2025-02-15 08:54:12,046 - Epoch [57/1000], Validation Step [500/1090], Val Loss: 0.4394
2025-02-15 08:54:21,601 - Epoch [57/1000], Validation Step [600/1090], Val Loss: 0.1370
2025-02-15 08:54:31,188 - Epoch [57/1000], Validation Step [700/1090], Val Loss: 0.1407
2025-02-15 08:54:40,011 - Epoch [57/1000], Validation Step [800/1090], Val Loss: 0.0042
2025-02-15 08:54:48,606 - Epoch [57/1000], Validation Step [900/1090], Val Loss: 0.0040
2025-02-15 08:54:57,870 - Epoch [57/1000], Validation Step [1000/1090], Val Loss: 0.0001
2025-02-15 08:55:06,514 - Epoch 57/1000, Train Loss: 0.1363, Val Loss: 0.1421, Accuracy: 94.80%
2025-02-15 08:55:42,197 - Epoch [58/1000], Step [100/4367], Loss: 0.0651
2025-02-15 08:56:16,832 - Epoch [58/1000], Step [200/4367], Loss: 0.0688
2025-02-15 08:56:51,299 - Epoch [58/1000], Step [300/4367], Loss: 0.0709
2025-02-15 08:57:26,251 - Epoch [58/1000], Step [400/4367], Loss: 0.1731
2025-02-15 08:58:00,531 - Epoch [58/1000], Step [500/4367], Loss: 0.1557
2025-02-15 08:58:35,159 - Epoch [58/1000], Step [600/4367], Loss: 0.1068
2025-02-15 08:59:09,678 - Epoch [58/1000], Step [700/4367], Loss: 0.0992
2025-02-15 08:59:44,495 - Epoch [58/1000], Step [800/4367], Loss: 0.2213
2025-02-15 09:00:18,987 - Epoch [58/1000], Step [900/4367], Loss: 0.1033
2025-02-15 09:00:53,769 - Epoch [58/1000], Step [1000/4367], Loss: 0.1508
2025-02-15 09:01:28,627 - Epoch [58/1000], Step [1100/4367], Loss: 0.1522
2025-02-15 09:02:03,743 - Epoch [58/1000], Step [1200/4367], Loss: 0.0949
2025-02-15 09:02:38,591 - Epoch [58/1000], Step [1300/4367], Loss: 0.2346
2025-02-15 09:03:13,134 - Epoch [58/1000], Step [1400/4367], Loss: 0.0358
2025-02-15 09:03:47,977 - Epoch [58/1000], Step [1500/4367], Loss: 0.1164
2025-02-15 09:04:22,701 - Epoch [58/1000], Step [1600/4367], Loss: 0.0613
2025-02-15 09:04:57,180 - Epoch [58/1000], Step [1700/4367], Loss: 0.0966
2025-02-15 09:05:31,492 - Epoch [58/1000], Step [1800/4367], Loss: 0.2036
2025-02-15 09:06:06,099 - Epoch [58/1000], Step [1900/4367], Loss: 0.2031
2025-02-15 09:06:40,902 - Epoch [58/1000], Step [2000/4367], Loss: 0.0659
2025-02-15 09:07:15,686 - Epoch [58/1000], Step [2100/4367], Loss: 0.1739
2025-02-15 09:07:50,302 - Epoch [58/1000], Step [2200/4367], Loss: 0.1224
2025-02-15 09:08:25,269 - Epoch [58/1000], Step [2300/4367], Loss: 0.0671
2025-02-15 09:09:00,326 - Epoch [58/1000], Step [2400/4367], Loss: 0.0508
2025-02-15 09:09:35,027 - Epoch [58/1000], Step [2500/4367], Loss: 0.1346
2025-02-15 09:10:10,005 - Epoch [58/1000], Step [2600/4367], Loss: 0.0998
2025-02-15 09:10:45,055 - Epoch [58/1000], Step [2700/4367], Loss: 0.1748
2025-02-15 09:11:19,788 - Epoch [58/1000], Step [2800/4367], Loss: 0.1137
2025-02-15 09:11:54,865 - Epoch [58/1000], Step [2900/4367], Loss: 0.2620
2025-02-15 09:12:29,470 - Epoch [58/1000], Step [3000/4367], Loss: 0.1072
2025-02-15 09:13:04,429 - Epoch [58/1000], Step [3100/4367], Loss: 0.1521
2025-02-15 09:13:39,611 - Epoch [58/1000], Step [3200/4367], Loss: 0.1424
2025-02-15 09:14:14,481 - Epoch [58/1000], Step [3300/4367], Loss: 0.1194
2025-02-15 09:14:49,438 - Epoch [58/1000], Step [3400/4367], Loss: 0.1365
2025-02-15 09:15:24,411 - Epoch [58/1000], Step [3500/4367], Loss: 0.0467
2025-02-15 09:15:59,254 - Epoch [58/1000], Step [3600/4367], Loss: 0.2781
2025-02-15 09:16:34,261 - Epoch [58/1000], Step [3700/4367], Loss: 0.3958
2025-02-15 09:17:09,467 - Epoch [58/1000], Step [3800/4367], Loss: 0.0220
2025-02-15 09:17:44,590 - Epoch [58/1000], Step [3900/4367], Loss: 0.1611
2025-02-15 09:18:19,055 - Epoch [58/1000], Step [4000/4367], Loss: 0.1398
2025-02-15 09:18:54,035 - Epoch [58/1000], Step [4100/4367], Loss: 0.2366
2025-02-15 09:19:28,948 - Epoch [58/1000], Step [4200/4367], Loss: 0.0794
2025-02-15 09:20:03,744 - Epoch [58/1000], Step [4300/4367], Loss: 0.1394
2025-02-15 09:20:37,132 - Epoch [58/1000], Validation Step [100/1090], Val Loss: 0.0002
2025-02-15 09:20:46,353 - Epoch [58/1000], Validation Step [200/1090], Val Loss: 0.0012
2025-02-15 09:20:55,729 - Epoch [58/1000], Validation Step [300/1090], Val Loss: 0.2680
2025-02-15 09:21:05,410 - Epoch [58/1000], Validation Step [400/1090], Val Loss: 0.0767
2025-02-15 09:21:14,542 - Epoch [58/1000], Validation Step [500/1090], Val Loss: 0.4519
2025-02-15 09:21:24,091 - Epoch [58/1000], Validation Step [600/1090], Val Loss: 0.1500
2025-02-15 09:21:33,659 - Epoch [58/1000], Validation Step [700/1090], Val Loss: 0.1540
2025-02-15 09:21:42,464 - Epoch [58/1000], Validation Step [800/1090], Val Loss: 0.0045
2025-02-15 09:21:51,045 - Epoch [58/1000], Validation Step [900/1090], Val Loss: 0.0049
2025-02-15 09:22:00,318 - Epoch [58/1000], Validation Step [1000/1090], Val Loss: 0.0004
2025-02-15 09:22:08,968 - Epoch 58/1000, Train Loss: 0.1354, Val Loss: 0.1452, Accuracy: 94.67%
2025-02-15 09:22:09,401 - Model saved at /data2/personal/sungjin/korean_dialects/checkpoint_real/checkpoint_epoch_58.pth
2025-02-15 09:22:45,217 - Epoch [59/1000], Step [100/4367], Loss: 0.0647
2025-02-15 09:23:20,012 - Epoch [59/1000], Step [200/4367], Loss: 0.1143
2025-02-15 09:23:54,699 - Epoch [59/1000], Step [300/4367], Loss: 0.0222
2025-02-15 09:24:29,599 - Epoch [59/1000], Step [400/4367], Loss: 0.0886
2025-02-15 09:25:04,451 - Epoch [59/1000], Step [500/4367], Loss: 0.1201
2025-02-15 09:25:39,083 - Epoch [59/1000], Step [600/4367], Loss: 0.2872
2025-02-15 09:26:14,000 - Epoch [59/1000], Step [700/4367], Loss: 0.0103
2025-02-15 09:26:49,092 - Epoch [59/1000], Step [800/4367], Loss: 0.1236
2025-02-15 09:27:24,038 - Epoch [59/1000], Step [900/4367], Loss: 0.0973
2025-02-15 09:27:59,082 - Epoch [59/1000], Step [1000/4367], Loss: 0.2103
2025-02-15 09:28:33,715 - Epoch [59/1000], Step [1100/4367], Loss: 0.0378
2025-02-15 09:29:08,376 - Epoch [59/1000], Step [1200/4367], Loss: 0.0939
2025-02-15 09:29:42,811 - Epoch [59/1000], Step [1300/4367], Loss: 0.1356
2025-02-15 09:30:17,351 - Epoch [59/1000], Step [1400/4367], Loss: 0.1466
2025-02-15 09:30:51,968 - Epoch [59/1000], Step [1500/4367], Loss: 0.2715
2025-02-15 09:31:26,518 - Epoch [59/1000], Step [1600/4367], Loss: 0.0866
2025-02-15 09:32:00,739 - Epoch [59/1000], Step [1700/4367], Loss: 0.0961
2025-02-15 09:32:35,286 - Epoch [59/1000], Step [1800/4367], Loss: 0.0335
2025-02-15 09:33:10,032 - Epoch [59/1000], Step [1900/4367], Loss: 0.2766
2025-02-15 09:33:44,950 - Epoch [59/1000], Step [2000/4367], Loss: 0.0459
2025-02-15 09:34:19,579 - Epoch [59/1000], Step [2100/4367], Loss: 0.1791
2025-02-15 09:34:54,529 - Epoch [59/1000], Step [2200/4367], Loss: 0.0802
2025-02-15 09:35:29,552 - Epoch [59/1000], Step [2300/4367], Loss: 0.1335
2025-02-15 09:36:04,326 - Epoch [59/1000], Step [2400/4367], Loss: 0.2784
2025-02-15 09:36:39,542 - Epoch [59/1000], Step [2500/4367], Loss: 0.1349
2025-02-15 09:37:14,209 - Epoch [59/1000], Step [2600/4367], Loss: 0.1295
2025-02-15 09:37:48,536 - Epoch [59/1000], Step [2700/4367], Loss: 0.2952
2025-02-15 09:38:23,334 - Epoch [59/1000], Step [2800/4367], Loss: 0.1083
2025-02-15 09:38:58,111 - Epoch [59/1000], Step [2900/4367], Loss: 0.0427
2025-02-15 09:39:32,921 - Epoch [59/1000], Step [3000/4367], Loss: 0.1313
2025-02-15 09:40:07,465 - Epoch [59/1000], Step [3100/4367], Loss: 0.1777
2025-02-15 09:40:42,122 - Epoch [59/1000], Step [3200/4367], Loss: 0.0434
2025-02-15 09:41:16,345 - Epoch [59/1000], Step [3300/4367], Loss: 0.4221
2025-02-15 09:41:51,284 - Epoch [59/1000], Step [3400/4367], Loss: 0.0954
2025-02-15 09:42:25,863 - Epoch [59/1000], Step [3500/4367], Loss: 0.1388
2025-02-15 09:43:00,932 - Epoch [59/1000], Step [3600/4367], Loss: 0.0904
2025-02-15 09:43:35,938 - Epoch [59/1000], Step [3700/4367], Loss: 0.0929
2025-02-15 09:44:10,024 - Epoch [59/1000], Step [3800/4367], Loss: 0.1344
2025-02-15 09:44:44,267 - Epoch [59/1000], Step [3900/4367], Loss: 0.1026
2025-02-15 09:45:18,696 - Epoch [59/1000], Step [4000/4367], Loss: 0.0247
2025-02-15 09:45:53,820 - Epoch [59/1000], Step [4100/4367], Loss: 0.0668
2025-02-15 09:46:28,008 - Epoch [59/1000], Step [4200/4367], Loss: 0.0638
2025-02-15 09:47:02,395 - Epoch [59/1000], Step [4300/4367], Loss: 0.1002
2025-02-15 09:47:35,576 - Epoch [59/1000], Validation Step [100/1090], Val Loss: 0.0011
2025-02-15 09:47:44,796 - Epoch [59/1000], Validation Step [200/1090], Val Loss: 0.0054
2025-02-15 09:47:54,152 - Epoch [59/1000], Validation Step [300/1090], Val Loss: 0.2575
2025-02-15 09:48:03,820 - Epoch [59/1000], Validation Step [400/1090], Val Loss: 0.0681
2025-02-15 09:48:12,944 - Epoch [59/1000], Validation Step [500/1090], Val Loss: 0.4387
2025-02-15 09:48:22,480 - Epoch [59/1000], Validation Step [600/1090], Val Loss: 0.1354
2025-02-15 09:48:32,054 - Epoch [59/1000], Validation Step [700/1090], Val Loss: 0.1579
2025-02-15 09:48:40,857 - Epoch [59/1000], Validation Step [800/1090], Val Loss: 0.0047
2025-02-15 09:48:49,434 - Epoch [59/1000], Validation Step [900/1090], Val Loss: 0.0050
2025-02-15 09:48:58,697 - Epoch [59/1000], Validation Step [1000/1090], Val Loss: 0.0005
2025-02-15 09:49:07,328 - Epoch 59/1000, Train Loss: 0.1348, Val Loss: 0.1496, Accuracy: 94.53%
2025-02-15 09:49:42,791 - Epoch [60/1000], Step [100/4367], Loss: 0.0686
2025-02-15 09:50:17,932 - Epoch [60/1000], Step [200/4367], Loss: 0.0576
2025-02-15 09:50:52,393 - Epoch [60/1000], Step [300/4367], Loss: 0.1585
2025-02-15 09:51:27,080 - Epoch [60/1000], Step [400/4367], Loss: 0.3713
2025-02-15 09:52:01,698 - Epoch [60/1000], Step [500/4367], Loss: 0.0978
2025-02-15 09:52:36,396 - Epoch [60/1000], Step [600/4367], Loss: 0.1957
2025-02-15 09:53:11,385 - Epoch [60/1000], Step [700/4367], Loss: 0.1245
2025-02-15 09:53:46,671 - Epoch [60/1000], Step [800/4367], Loss: 0.1511
2025-02-15 09:54:21,168 - Epoch [60/1000], Step [900/4367], Loss: 0.1935
2025-02-15 09:54:55,234 - Epoch [60/1000], Step [1000/4367], Loss: 0.0975
2025-02-15 09:55:29,795 - Epoch [60/1000], Step [1100/4367], Loss: 0.2046
2025-02-15 09:56:04,671 - Epoch [60/1000], Step [1200/4367], Loss: 0.4363
2025-02-15 09:56:38,939 - Epoch [60/1000], Step [1300/4367], Loss: 0.2586
2025-02-15 09:57:13,718 - Epoch [60/1000], Step [1400/4367], Loss: 0.0723
2025-02-15 09:57:48,630 - Epoch [60/1000], Step [1500/4367], Loss: 0.1136
2025-02-15 09:58:23,383 - Epoch [60/1000], Step [1600/4367], Loss: 0.1776
2025-02-15 09:58:58,220 - Epoch [60/1000], Step [1700/4367], Loss: 0.1800
2025-02-15 09:59:32,885 - Epoch [60/1000], Step [1800/4367], Loss: 0.1612
2025-02-15 10:00:07,869 - Epoch [60/1000], Step [1900/4367], Loss: 0.3643
2025-02-15 10:00:42,569 - Epoch [60/1000], Step [2000/4367], Loss: 0.0856
2025-02-15 10:01:17,970 - Epoch [60/1000], Step [2100/4367], Loss: 0.1465
2025-02-15 10:01:52,668 - Epoch [60/1000], Step [2200/4367], Loss: 0.1157
2025-02-15 10:02:27,184 - Epoch [60/1000], Step [2300/4367], Loss: 0.0449
2025-02-15 10:03:01,680 - Epoch [60/1000], Step [2400/4367], Loss: 0.0817
2025-02-15 10:03:36,388 - Epoch [60/1000], Step [2500/4367], Loss: 0.0794
2025-02-15 10:04:11,303 - Epoch [60/1000], Step [2600/4367], Loss: 0.2227
2025-02-15 10:04:46,010 - Epoch [60/1000], Step [2700/4367], Loss: 0.1330
2025-02-15 10:05:21,216 - Epoch [60/1000], Step [2800/4367], Loss: 0.0633
2025-02-15 10:05:56,089 - Epoch [60/1000], Step [2900/4367], Loss: 0.4215
2025-02-15 10:06:30,471 - Epoch [60/1000], Step [3000/4367], Loss: 0.0951
2025-02-15 10:07:05,055 - Epoch [60/1000], Step [3100/4367], Loss: 0.0495
2025-02-15 10:07:39,547 - Epoch [60/1000], Step [3200/4367], Loss: 0.0726
2025-02-15 10:08:13,890 - Epoch [60/1000], Step [3300/4367], Loss: 0.2338
2025-02-15 10:08:48,797 - Epoch [60/1000], Step [3400/4367], Loss: 0.1225
2025-02-15 10:09:23,906 - Epoch [60/1000], Step [3500/4367], Loss: 0.1509
2025-02-15 10:09:58,686 - Epoch [60/1000], Step [3600/4367], Loss: 0.1258
2025-02-15 10:10:33,420 - Epoch [60/1000], Step [3700/4367], Loss: 0.1060
2025-02-15 10:11:08,694 - Epoch [60/1000], Step [3800/4367], Loss: 0.1363
2025-02-15 10:11:43,545 - Epoch [60/1000], Step [3900/4367], Loss: 0.0823
2025-02-15 10:12:18,613 - Epoch [60/1000], Step [4000/4367], Loss: 0.1363
2025-02-15 10:12:53,757 - Epoch [60/1000], Step [4100/4367], Loss: 0.0642
2025-02-15 10:13:28,759 - Epoch [60/1000], Step [4200/4367], Loss: 0.1145
2025-02-15 10:14:03,356 - Epoch [60/1000], Step [4300/4367], Loss: 0.1413
2025-02-15 10:14:36,702 - Epoch [60/1000], Validation Step [100/1090], Val Loss: 0.0002
2025-02-15 10:14:45,928 - Epoch [60/1000], Validation Step [200/1090], Val Loss: 0.0003
2025-02-15 10:14:55,298 - Epoch [60/1000], Validation Step [300/1090], Val Loss: 0.2485
2025-02-15 10:15:04,991 - Epoch [60/1000], Validation Step [400/1090], Val Loss: 0.0445
2025-02-15 10:15:14,130 - Epoch [60/1000], Validation Step [500/1090], Val Loss: 0.4996
2025-02-15 10:15:23,682 - Epoch [60/1000], Validation Step [600/1090], Val Loss: 0.1305
2025-02-15 10:15:33,279 - Epoch [60/1000], Validation Step [700/1090], Val Loss: 0.1233
2025-02-15 10:15:42,088 - Epoch [60/1000], Validation Step [800/1090], Val Loss: 0.0029
2025-02-15 10:15:50,669 - Epoch [60/1000], Validation Step [900/1090], Val Loss: 0.0024
2025-02-15 10:15:59,960 - Epoch [60/1000], Validation Step [1000/1090], Val Loss: 0.0000
2025-02-15 10:16:08,604 - Epoch 60/1000, Train Loss: 0.1348, Val Loss: 0.1413, Accuracy: 94.78%
2025-02-15 10:16:09,050 - Model saved at /data2/personal/sungjin/korean_dialects/checkpoint_real/checkpoint_epoch_60.pth
2025-02-15 10:16:44,955 - Epoch [61/1000], Step [100/4367], Loss: 0.2991
2025-02-15 10:17:19,253 - Epoch [61/1000], Step [200/4367], Loss: 0.2212
2025-02-15 10:17:54,494 - Epoch [61/1000], Step [300/4367], Loss: 0.0615
2025-02-15 10:18:29,453 - Epoch [61/1000], Step [400/4367], Loss: 0.0905
2025-02-15 10:19:04,032 - Epoch [61/1000], Step [500/4367], Loss: 0.0701
2025-02-15 10:19:38,815 - Epoch [61/1000], Step [600/4367], Loss: 0.1122
2025-02-15 10:20:13,410 - Epoch [61/1000], Step [700/4367], Loss: 0.0766
2025-02-15 10:20:47,990 - Epoch [61/1000], Step [800/4367], Loss: 0.0361
2025-02-15 10:21:22,559 - Epoch [61/1000], Step [900/4367], Loss: 0.1047
2025-02-15 10:21:57,500 - Epoch [61/1000], Step [1000/4367], Loss: 0.1520
2025-02-15 10:22:32,527 - Epoch [61/1000], Step [1100/4367], Loss: 0.1168
2025-02-15 10:23:07,374 - Epoch [61/1000], Step [1200/4367], Loss: 0.1107
2025-02-15 10:23:42,098 - Epoch [61/1000], Step [1300/4367], Loss: 0.2133
2025-02-15 10:24:17,218 - Epoch [61/1000], Step [1400/4367], Loss: 0.0356
2025-02-15 10:24:52,371 - Epoch [61/1000], Step [1500/4367], Loss: 0.1366
2025-02-15 10:25:26,881 - Epoch [61/1000], Step [1600/4367], Loss: 0.0333
2025-02-15 10:26:01,589 - Epoch [61/1000], Step [1700/4367], Loss: 0.2833
2025-02-15 10:26:36,521 - Epoch [61/1000], Step [1800/4367], Loss: 0.0735
2025-02-15 10:27:11,447 - Epoch [61/1000], Step [1900/4367], Loss: 0.1544
2025-02-15 10:27:45,970 - Epoch [61/1000], Step [2000/4367], Loss: 0.1931
2025-02-15 10:28:20,660 - Epoch [61/1000], Step [2100/4367], Loss: 0.1586
2025-02-15 10:28:55,742 - Epoch [61/1000], Step [2200/4367], Loss: 0.1634
2025-02-15 10:29:30,067 - Epoch [61/1000], Step [2300/4367], Loss: 0.1073
2025-02-15 10:30:04,845 - Epoch [61/1000], Step [2400/4367], Loss: 0.0932
2025-02-15 10:30:40,166 - Epoch [61/1000], Step [2500/4367], Loss: 0.0584
2025-02-15 10:31:15,116 - Epoch [61/1000], Step [2600/4367], Loss: 0.0970
2025-02-15 10:31:50,019 - Epoch [61/1000], Step [2700/4367], Loss: 0.0902
2025-02-15 10:32:24,502 - Epoch [61/1000], Step [2800/4367], Loss: 0.1644
2025-02-15 10:32:59,077 - Epoch [61/1000], Step [2900/4367], Loss: 0.1593
2025-02-15 10:33:33,906 - Epoch [61/1000], Step [3000/4367], Loss: 0.1113
2025-02-15 10:34:08,716 - Epoch [61/1000], Step [3100/4367], Loss: 0.2442
2025-02-15 10:34:43,247 - Epoch [61/1000], Step [3200/4367], Loss: 0.0625
2025-02-15 10:35:18,427 - Epoch [61/1000], Step [3300/4367], Loss: 0.0895
2025-02-15 10:35:53,421 - Epoch [61/1000], Step [3400/4367], Loss: 0.2414
2025-02-15 10:36:28,469 - Epoch [61/1000], Step [3500/4367], Loss: 0.1699
2025-02-15 10:37:03,323 - Epoch [61/1000], Step [3600/4367], Loss: 0.0683
2025-02-15 10:37:37,906 - Epoch [61/1000], Step [3700/4367], Loss: 0.1086
2025-02-15 10:38:12,715 - Epoch [61/1000], Step [3800/4367], Loss: 0.0217
2025-02-15 10:38:48,011 - Epoch [61/1000], Step [3900/4367], Loss: 0.2252
2025-02-15 10:39:22,880 - Epoch [61/1000], Step [4000/4367], Loss: 0.0803
2025-02-15 10:39:57,786 - Epoch [61/1000], Step [4100/4367], Loss: 0.1098
2025-02-15 10:40:32,555 - Epoch [61/1000], Step [4200/4367], Loss: 0.1868
2025-02-15 10:41:07,597 - Epoch [61/1000], Step [4300/4367], Loss: 0.0700
2025-02-15 10:41:40,896 - Epoch [61/1000], Validation Step [100/1090], Val Loss: 0.0001
2025-02-15 10:41:50,122 - Epoch [61/1000], Validation Step [200/1090], Val Loss: 0.0009
2025-02-15 10:41:59,490 - Epoch [61/1000], Validation Step [300/1090], Val Loss: 0.2548
2025-02-15 10:42:09,183 - Epoch [61/1000], Validation Step [400/1090], Val Loss: 0.0672
2025-02-15 10:42:18,313 - Epoch [61/1000], Validation Step [500/1090], Val Loss: 0.5047
2025-02-15 10:42:27,865 - Epoch [61/1000], Validation Step [600/1090], Val Loss: 0.1260
2025-02-15 10:42:37,461 - Epoch [61/1000], Validation Step [700/1090], Val Loss: 0.1100
2025-02-15 10:42:46,268 - Epoch [61/1000], Validation Step [800/1090], Val Loss: 0.0026
2025-02-15 10:42:54,857 - Epoch [61/1000], Validation Step [900/1090], Val Loss: 0.0028
2025-02-15 10:43:04,133 - Epoch [61/1000], Validation Step [1000/1090], Val Loss: 0.0002
2025-02-15 10:43:12,770 - Epoch 61/1000, Train Loss: 0.1342, Val Loss: 0.1435, Accuracy: 94.69%
2025-02-15 10:43:48,693 - Epoch [62/1000], Step [100/4367], Loss: 0.1403
2025-02-15 10:44:23,278 - Epoch [62/1000], Step [200/4367], Loss: 0.1970
2025-02-15 10:44:57,918 - Epoch [62/1000], Step [300/4367], Loss: 0.1508
2025-02-15 10:45:32,872 - Epoch [62/1000], Step [400/4367], Loss: 0.0650
2025-02-15 10:46:08,066 - Epoch [62/1000], Step [500/4367], Loss: 0.1505
2025-02-15 10:46:42,846 - Epoch [62/1000], Step [600/4367], Loss: 0.0788
2025-02-15 10:47:17,659 - Epoch [62/1000], Step [700/4367], Loss: 0.0215
2025-02-15 10:47:52,608 - Epoch [62/1000], Step [800/4367], Loss: 0.1583
2025-02-15 10:48:27,283 - Epoch [62/1000], Step [900/4367], Loss: 0.0902
2025-02-15 10:49:02,268 - Epoch [62/1000], Step [1000/4367], Loss: 0.2077
2025-02-15 10:49:36,690 - Epoch [62/1000], Step [1100/4367], Loss: 0.1106
2025-02-15 10:50:11,312 - Epoch [62/1000], Step [1200/4367], Loss: 0.1460
2025-02-15 10:50:46,172 - Epoch [62/1000], Step [1300/4367], Loss: 0.0842
2025-02-15 10:51:20,606 - Epoch [62/1000], Step [1400/4367], Loss: 0.1237
2025-02-15 10:51:55,442 - Epoch [62/1000], Step [1500/4367], Loss: 0.1176
2025-02-15 10:52:30,305 - Epoch [62/1000], Step [1600/4367], Loss: 0.0347
2025-02-15 10:53:05,206 - Epoch [62/1000], Step [1700/4367], Loss: 0.0758
2025-02-15 10:53:39,472 - Epoch [62/1000], Step [1800/4367], Loss: 0.1324
2025-02-15 10:54:14,031 - Epoch [62/1000], Step [1900/4367], Loss: 0.0904
2025-02-15 10:54:48,380 - Epoch [62/1000], Step [2000/4367], Loss: 0.1288
2025-02-15 10:55:22,796 - Epoch [62/1000], Step [2100/4367], Loss: 0.1740
2025-02-15 10:55:57,425 - Epoch [62/1000], Step [2200/4367], Loss: 0.1257
2025-02-15 10:56:32,731 - Epoch [62/1000], Step [2300/4367], Loss: 0.0633
2025-02-15 10:57:07,766 - Epoch [62/1000], Step [2400/4367], Loss: 0.2062
2025-02-15 10:57:42,229 - Epoch [62/1000], Step [2500/4367], Loss: 0.1180
2025-02-15 10:58:16,986 - Epoch [62/1000], Step [2600/4367], Loss: 0.0341
2025-02-15 10:58:51,493 - Epoch [62/1000], Step [2700/4367], Loss: 0.0805
2025-02-15 10:59:26,263 - Epoch [62/1000], Step [2800/4367], Loss: 0.1422
2025-02-15 11:00:00,926 - Epoch [62/1000], Step [2900/4367], Loss: 0.0327
2025-02-15 11:00:35,396 - Epoch [62/1000], Step [3000/4367], Loss: 0.0773
2025-02-15 11:01:10,330 - Epoch [62/1000], Step [3100/4367], Loss: 0.1773
2025-02-15 11:01:44,723 - Epoch [62/1000], Step [3200/4367], Loss: 0.0302
2025-02-15 11:02:20,104 - Epoch [62/1000], Step [3300/4367], Loss: 0.3348
2025-02-15 11:02:54,569 - Epoch [62/1000], Step [3400/4367], Loss: 0.1902
2025-02-15 11:03:29,157 - Epoch [62/1000], Step [3500/4367], Loss: 0.0276
2025-02-15 11:04:04,011 - Epoch [62/1000], Step [3600/4367], Loss: 0.0384
2025-02-15 11:04:38,455 - Epoch [62/1000], Step [3700/4367], Loss: 0.0792
2025-02-15 11:05:13,084 - Epoch [62/1000], Step [3800/4367], Loss: 0.1967
2025-02-15 11:05:47,669 - Epoch [62/1000], Step [3900/4367], Loss: 0.0774
2025-02-15 11:06:22,516 - Epoch [62/1000], Step [4000/4367], Loss: 0.0872
2025-02-15 11:06:56,752 - Epoch [62/1000], Step [4100/4367], Loss: 0.0733
2025-02-15 11:07:31,849 - Epoch [62/1000], Step [4200/4367], Loss: 0.2025
2025-02-15 11:08:06,548 - Epoch [62/1000], Step [4300/4367], Loss: 0.4710
2025-02-15 11:08:39,776 - Epoch [62/1000], Validation Step [100/1090], Val Loss: 0.0000
2025-02-15 11:08:48,978 - Epoch [62/1000], Validation Step [200/1090], Val Loss: 0.0002
2025-02-15 11:08:58,325 - Epoch [62/1000], Validation Step [300/1090], Val Loss: 0.2381
2025-02-15 11:09:07,978 - Epoch [62/1000], Validation Step [400/1090], Val Loss: 0.0509
2025-02-15 11:09:17,031 - Epoch [62/1000], Validation Step [500/1090], Val Loss: 0.4746
2025-02-15 11:09:26,506 - Epoch [62/1000], Validation Step [600/1090], Val Loss: 0.1728
2025-02-15 11:09:36,018 - Epoch [62/1000], Validation Step [700/1090], Val Loss: 0.1391
2025-02-15 11:09:44,745 - Epoch [62/1000], Validation Step [800/1090], Val Loss: 0.0032
2025-02-15 11:09:53,251 - Epoch [62/1000], Validation Step [900/1090], Val Loss: 0.0033
2025-02-15 11:10:02,447 - Epoch [62/1000], Validation Step [1000/1090], Val Loss: 0.0000
2025-02-15 11:10:11,016 - Epoch 62/1000, Train Loss: 0.1334, Val Loss: 0.1405, Accuracy: 94.77%
2025-02-15 11:10:11,444 - Model saved at /data2/personal/sungjin/korean_dialects/checkpoint_real/checkpoint_epoch_62.pth
2025-02-15 11:10:47,680 - Epoch [63/1000], Step [100/4367], Loss: 0.0686
2025-02-15 11:11:22,049 - Epoch [63/1000], Step [200/4367], Loss: 0.0884
2025-02-15 11:11:56,912 - Epoch [63/1000], Step [300/4367], Loss: 0.0577
2025-02-15 11:12:31,384 - Epoch [63/1000], Step [400/4367], Loss: 0.0843
2025-02-15 11:13:05,549 - Epoch [63/1000], Step [500/4367], Loss: 0.1253
2025-02-15 11:13:40,513 - Epoch [63/1000], Step [600/4367], Loss: 0.1058
2025-02-15 11:14:14,963 - Epoch [63/1000], Step [700/4367], Loss: 0.1014
2025-02-15 11:14:49,708 - Epoch [63/1000], Step [800/4367], Loss: 0.1286
2025-02-15 11:15:24,205 - Epoch [63/1000], Step [900/4367], Loss: 0.0528
2025-02-15 11:15:58,710 - Epoch [63/1000], Step [1000/4367], Loss: 0.0351
2025-02-15 11:16:33,819 - Epoch [63/1000], Step [1100/4367], Loss: 0.1909
2025-02-15 11:17:08,518 - Epoch [63/1000], Step [1200/4367], Loss: 0.2171
2025-02-15 11:17:43,340 - Epoch [63/1000], Step [1300/4367], Loss: 0.0960
2025-02-15 11:18:18,025 - Epoch [63/1000], Step [1400/4367], Loss: 0.1304
2025-02-15 11:18:52,627 - Epoch [63/1000], Step [1500/4367], Loss: 0.0549
2025-02-15 11:19:27,325 - Epoch [63/1000], Step [1600/4367], Loss: 0.0708
2025-02-15 11:20:02,057 - Epoch [63/1000], Step [1700/4367], Loss: 0.1901
2025-02-15 11:20:36,768 - Epoch [63/1000], Step [1800/4367], Loss: 0.0834
2025-02-15 11:21:11,715 - Epoch [63/1000], Step [1900/4367], Loss: 0.1313
2025-02-15 11:21:46,200 - Epoch [63/1000], Step [2000/4367], Loss: 0.2089
2025-02-15 11:22:21,035 - Epoch [63/1000], Step [2100/4367], Loss: 0.1364
2025-02-15 11:22:56,288 - Epoch [63/1000], Step [2200/4367], Loss: 0.1923
2025-02-15 11:23:31,030 - Epoch [63/1000], Step [2300/4367], Loss: 0.2822
2025-02-15 11:24:05,814 - Epoch [63/1000], Step [2400/4367], Loss: 0.0226
2025-02-15 11:24:40,018 - Epoch [63/1000], Step [2500/4367], Loss: 0.2093
2025-02-15 11:25:14,875 - Epoch [63/1000], Step [2600/4367], Loss: 0.0138
2025-02-15 11:25:49,432 - Epoch [63/1000], Step [2700/4367], Loss: 0.1889
2025-02-15 11:26:23,978 - Epoch [63/1000], Step [2800/4367], Loss: 0.1297
2025-02-15 11:26:58,666 - Epoch [63/1000], Step [2900/4367], Loss: 0.1096
2025-02-15 11:27:33,096 - Epoch [63/1000], Step [3000/4367], Loss: 0.2277
2025-02-15 11:28:07,813 - Epoch [63/1000], Step [3100/4367], Loss: 0.1109
2025-02-15 11:28:42,946 - Epoch [63/1000], Step [3200/4367], Loss: 0.0587
2025-02-15 11:29:17,465 - Epoch [63/1000], Step [3300/4367], Loss: 0.1393
2025-02-15 11:29:52,059 - Epoch [63/1000], Step [3400/4367], Loss: 0.1226
2025-02-15 11:30:26,817 - Epoch [63/1000], Step [3500/4367], Loss: 0.0303
2025-02-15 11:31:01,850 - Epoch [63/1000], Step [3600/4367], Loss: 0.0333
2025-02-15 11:31:36,575 - Epoch [63/1000], Step [3700/4367], Loss: 0.1210
2025-02-15 11:32:11,203 - Epoch [63/1000], Step [3800/4367], Loss: 0.1159
2025-02-15 11:32:45,905 - Epoch [63/1000], Step [3900/4367], Loss: 0.2332
2025-02-15 11:33:20,614 - Epoch [63/1000], Step [4000/4367], Loss: 0.4405
2025-02-15 11:33:55,222 - Epoch [63/1000], Step [4100/4367], Loss: 0.0907
2025-02-15 11:34:29,917 - Epoch [63/1000], Step [4200/4367], Loss: 0.0222
2025-02-15 11:35:04,803 - Epoch [63/1000], Step [4300/4367], Loss: 0.2433
2025-02-15 11:35:37,606 - Epoch [63/1000], Validation Step [100/1090], Val Loss: 0.0001
2025-02-15 11:35:46,798 - Epoch [63/1000], Validation Step [200/1090], Val Loss: 0.0003
2025-02-15 11:35:56,152 - Epoch [63/1000], Validation Step [300/1090], Val Loss: 0.2364
2025-02-15 11:36:05,822 - Epoch [63/1000], Validation Step [400/1090], Val Loss: 0.0606
2025-02-15 11:36:14,936 - Epoch [63/1000], Validation Step [500/1090], Val Loss: 0.4350
2025-02-15 11:36:24,464 - Epoch [63/1000], Validation Step [600/1090], Val Loss: 0.1503
2025-02-15 11:36:34,035 - Epoch [63/1000], Validation Step [700/1090], Val Loss: 0.1379
2025-02-15 11:36:42,823 - Epoch [63/1000], Validation Step [800/1090], Val Loss: 0.0047
2025-02-15 11:36:51,391 - Epoch [63/1000], Validation Step [900/1090], Val Loss: 0.0044
2025-02-15 11:37:00,662 - Epoch [63/1000], Validation Step [1000/1090], Val Loss: 0.0000
2025-02-15 11:37:09,296 - Epoch 63/1000, Train Loss: 0.1344, Val Loss: 0.1403, Accuracy: 94.77%
2025-02-15 11:37:45,729 - Epoch [64/1000], Step [100/4367], Loss: 0.1115
2025-02-15 11:38:20,583 - Epoch [64/1000], Step [200/4367], Loss: 0.0559
2025-02-15 11:38:55,145 - Epoch [64/1000], Step [300/4367], Loss: 0.1076
2025-02-15 11:39:29,637 - Epoch [64/1000], Step [400/4367], Loss: 0.3867
2025-02-15 11:40:04,165 - Epoch [64/1000], Step [500/4367], Loss: 0.1409
2025-02-15 11:40:38,885 - Epoch [64/1000], Step [600/4367], Loss: 0.0936
2025-02-15 11:41:13,351 - Epoch [64/1000], Step [700/4367], Loss: 0.0948
2025-02-15 11:41:48,344 - Epoch [64/1000], Step [800/4367], Loss: 0.2531
2025-02-15 11:42:23,293 - Epoch [64/1000], Step [900/4367], Loss: 0.1713
2025-02-15 11:42:58,248 - Epoch [64/1000], Step [1000/4367], Loss: 0.1474
2025-02-15 11:43:32,815 - Epoch [64/1000], Step [1100/4367], Loss: 0.0176
2025-02-15 11:44:07,739 - Epoch [64/1000], Step [1200/4367], Loss: 0.2256
2025-02-15 11:44:42,392 - Epoch [64/1000], Step [1300/4367], Loss: 0.1565
2025-02-15 11:45:17,239 - Epoch [64/1000], Step [1400/4367], Loss: 0.1998
2025-02-15 11:45:51,766 - Epoch [64/1000], Step [1500/4367], Loss: 0.1190
2025-02-15 11:46:26,279 - Epoch [64/1000], Step [1600/4367], Loss: 0.1452
2025-02-15 11:47:00,797 - Epoch [64/1000], Step [1700/4367], Loss: 0.1075
2025-02-15 11:47:35,492 - Epoch [64/1000], Step [1800/4367], Loss: 0.2982
2025-02-15 11:48:10,475 - Epoch [64/1000], Step [1900/4367], Loss: 0.2409
2025-02-15 11:48:45,164 - Epoch [64/1000], Step [2000/4367], Loss: 0.1199
2025-02-15 11:49:19,675 - Epoch [64/1000], Step [2100/4367], Loss: 0.1460
2025-02-15 11:49:54,310 - Epoch [64/1000], Step [2200/4367], Loss: 0.0416
2025-02-15 11:50:28,706 - Epoch [64/1000], Step [2300/4367], Loss: 0.2679
2025-02-15 11:51:03,340 - Epoch [64/1000], Step [2400/4367], Loss: 0.3793
2025-02-15 11:51:38,177 - Epoch [64/1000], Step [2500/4367], Loss: 0.1482
2025-02-15 11:52:13,249 - Epoch [64/1000], Step [2600/4367], Loss: 0.1833
2025-02-15 11:52:48,004 - Epoch [64/1000], Step [2700/4367], Loss: 0.1745
2025-02-15 11:53:23,113 - Epoch [64/1000], Step [2800/4367], Loss: 0.0583
2025-02-15 11:53:57,770 - Epoch [64/1000], Step [2900/4367], Loss: 0.0399
2025-02-15 11:54:32,369 - Epoch [64/1000], Step [3000/4367], Loss: 0.0846
2025-02-15 11:55:06,635 - Epoch [64/1000], Step [3100/4367], Loss: 0.1216
2025-02-15 11:55:41,507 - Epoch [64/1000], Step [3200/4367], Loss: 0.0795
2025-02-15 11:56:16,375 - Epoch [64/1000], Step [3300/4367], Loss: 0.1381
2025-02-15 11:56:51,378 - Epoch [64/1000], Step [3400/4367], Loss: 0.0625
2025-02-15 11:57:26,257 - Epoch [64/1000], Step [3500/4367], Loss: 0.1287
2025-02-15 11:58:01,257 - Epoch [64/1000], Step [3600/4367], Loss: 0.1481
2025-02-15 11:58:36,015 - Epoch [64/1000], Step [3700/4367], Loss: 0.1193
2025-02-15 11:59:10,624 - Epoch [64/1000], Step [3800/4367], Loss: 0.0668
2025-02-15 11:59:45,037 - Epoch [64/1000], Step [3900/4367], Loss: 0.1325
2025-02-15 12:00:19,883 - Epoch [64/1000], Step [4000/4367], Loss: 0.0547
2025-02-15 12:00:55,119 - Epoch [64/1000], Step [4100/4367], Loss: 0.0449
2025-02-15 12:01:29,715 - Epoch [64/1000], Step [4200/4367], Loss: 0.1516
2025-02-15 12:02:04,023 - Epoch [64/1000], Step [4300/4367], Loss: 0.0451
2025-02-15 12:02:37,135 - Epoch [64/1000], Validation Step [100/1090], Val Loss: 0.0001
2025-02-15 12:02:46,350 - Epoch [64/1000], Validation Step [200/1090], Val Loss: 0.0003
2025-02-15 12:02:55,702 - Epoch [64/1000], Validation Step [300/1090], Val Loss: 0.2329
2025-02-15 12:03:05,363 - Epoch [64/1000], Validation Step [400/1090], Val Loss: 0.0573
2025-02-15 12:03:14,466 - Epoch [64/1000], Validation Step [500/1090], Val Loss: 0.4887
2025-02-15 12:03:23,996 - Epoch [64/1000], Validation Step [600/1090], Val Loss: 0.1275
2025-02-15 12:03:33,577 - Epoch [64/1000], Validation Step [700/1090], Val Loss: 0.1406
2025-02-15 12:03:42,371 - Epoch [64/1000], Validation Step [800/1090], Val Loss: 0.0051
2025-02-15 12:03:50,934 - Epoch [64/1000], Validation Step [900/1090], Val Loss: 0.0044
2025-02-15 12:04:00,191 - Epoch [64/1000], Validation Step [1000/1090], Val Loss: 0.0001
2025-02-15 12:04:08,816 - Epoch 64/1000, Train Loss: 0.1341, Val Loss: 0.1433, Accuracy: 94.71%
2025-02-15 12:04:09,248 - Model saved at /data2/personal/sungjin/korean_dialects/checkpoint_real/checkpoint_epoch_64.pth
2025-02-15 12:04:45,595 - Epoch [65/1000], Step [100/4367], Loss: 0.2168
2025-02-15 12:05:20,174 - Epoch [65/1000], Step [200/4367], Loss: 0.2400
2025-02-15 12:05:54,713 - Epoch [65/1000], Step [300/4367], Loss: 0.0761
2025-02-15 12:06:29,883 - Epoch [65/1000], Step [400/4367], Loss: 0.1054
2025-02-15 12:07:04,669 - Epoch [65/1000], Step [500/4367], Loss: 0.0964
2025-02-15 12:07:39,229 - Epoch [65/1000], Step [600/4367], Loss: 0.0411
2025-02-15 12:08:14,177 - Epoch [65/1000], Step [700/4367], Loss: 0.0190
2025-02-15 12:08:49,043 - Epoch [65/1000], Step [800/4367], Loss: 0.0834
2025-02-15 12:09:23,626 - Epoch [65/1000], Step [900/4367], Loss: 0.2773
2025-02-15 12:09:57,974 - Epoch [65/1000], Step [1000/4367], Loss: 0.0681
2025-02-15 12:10:32,943 - Epoch [65/1000], Step [1100/4367], Loss: 0.2038
2025-02-15 12:11:07,492 - Epoch [65/1000], Step [1200/4367], Loss: 0.0633
2025-02-15 12:11:42,427 - Epoch [65/1000], Step [1300/4367], Loss: 0.0597
2025-02-15 12:12:16,709 - Epoch [65/1000], Step [1400/4367], Loss: 0.1198
2025-02-15 12:12:51,261 - Epoch [65/1000], Step [1500/4367], Loss: 0.2193
2025-02-15 12:13:25,982 - Epoch [65/1000], Step [1600/4367], Loss: 0.2329
2025-02-15 12:14:00,677 - Epoch [65/1000], Step [1700/4367], Loss: 0.1406
2025-02-15 12:14:35,319 - Epoch [65/1000], Step [1800/4367], Loss: 0.2641
2025-02-15 12:15:10,136 - Epoch [65/1000], Step [1900/4367], Loss: 0.0296
2025-02-15 12:15:44,849 - Epoch [65/1000], Step [2000/4367], Loss: 0.1857
2025-02-15 12:16:19,542 - Epoch [65/1000], Step [2100/4367], Loss: 0.0483
2025-02-15 12:16:54,266 - Epoch [65/1000], Step [2200/4367], Loss: 0.1069
2025-02-15 12:17:28,949 - Epoch [65/1000], Step [2300/4367], Loss: 0.0454
2025-02-15 12:18:03,717 - Epoch [65/1000], Step [2400/4367], Loss: 0.0209
2025-02-15 12:18:38,952 - Epoch [65/1000], Step [2500/4367], Loss: 0.1154
2025-02-15 12:19:13,678 - Epoch [65/1000], Step [2600/4367], Loss: 0.2260
2025-02-15 12:19:48,370 - Epoch [65/1000], Step [2700/4367], Loss: 0.1097
2025-02-15 12:20:22,980 - Epoch [65/1000], Step [2800/4367], Loss: 0.1142
2025-02-15 12:20:58,000 - Epoch [65/1000], Step [2900/4367], Loss: 0.0883
2025-02-15 12:21:32,287 - Epoch [65/1000], Step [3000/4367], Loss: 0.3744
2025-02-15 12:22:06,757 - Epoch [65/1000], Step [3100/4367], Loss: 0.1350
2025-02-15 12:22:41,429 - Epoch [65/1000], Step [3200/4367], Loss: 0.0982
2025-02-15 12:23:16,411 - Epoch [65/1000], Step [3300/4367], Loss: 0.1543
2025-02-15 12:23:51,083 - Epoch [65/1000], Step [3400/4367], Loss: 0.0839
2025-02-15 12:24:25,726 - Epoch [65/1000], Step [3500/4367], Loss: 0.0383
2025-02-15 12:25:00,499 - Epoch [65/1000], Step [3600/4367], Loss: 0.0415
2025-02-15 12:25:35,188 - Epoch [65/1000], Step [3700/4367], Loss: 0.0431
2025-02-15 12:26:09,613 - Epoch [65/1000], Step [3800/4367], Loss: 0.0194
2025-02-15 12:26:44,283 - Epoch [65/1000], Step [3900/4367], Loss: 0.1104
2025-02-15 12:27:18,956 - Epoch [65/1000], Step [4000/4367], Loss: 0.1301
2025-02-15 12:27:53,835 - Epoch [65/1000], Step [4100/4367], Loss: 0.2291
2025-02-15 12:28:27,975 - Epoch [65/1000], Step [4200/4367], Loss: 0.1208
2025-02-15 12:29:02,897 - Epoch [65/1000], Step [4300/4367], Loss: 0.1195
2025-02-15 12:29:35,990 - Epoch [65/1000], Validation Step [100/1090], Val Loss: 0.0001
2025-02-15 12:29:45,184 - Epoch [65/1000], Validation Step [200/1090], Val Loss: 0.0001
2025-02-15 12:29:54,528 - Epoch [65/1000], Validation Step [300/1090], Val Loss: 0.2309
2025-02-15 12:30:04,195 - Epoch [65/1000], Validation Step [400/1090], Val Loss: 0.0505
2025-02-15 12:30:13,303 - Epoch [65/1000], Validation Step [500/1090], Val Loss: 0.3977
2025-02-15 12:30:22,838 - Epoch [65/1000], Validation Step [600/1090], Val Loss: 0.1419
2025-02-15 12:30:32,411 - Epoch [65/1000], Validation Step [700/1090], Val Loss: 0.1418
2025-02-15 12:30:41,202 - Epoch [65/1000], Validation Step [800/1090], Val Loss: 0.0048
2025-02-15 12:30:49,772 - Epoch [65/1000], Validation Step [900/1090], Val Loss: 0.0046
2025-02-15 12:30:59,030 - Epoch [65/1000], Validation Step [1000/1090], Val Loss: 0.0000
2025-02-15 12:31:07,655 - Epoch 65/1000, Train Loss: 0.1336, Val Loss: 0.1411, Accuracy: 94.79%
2025-02-15 12:31:43,331 - Epoch [66/1000], Step [100/4367], Loss: 0.1598
2025-02-15 12:32:18,083 - Epoch [66/1000], Step [200/4367], Loss: 0.0597
2025-02-15 12:32:52,692 - Epoch [66/1000], Step [300/4367], Loss: 0.2220
2025-02-15 12:33:27,198 - Epoch [66/1000], Step [400/4367], Loss: 0.1899
2025-02-15 12:34:01,842 - Epoch [66/1000], Step [500/4367], Loss: 0.1919
2025-02-15 12:34:36,783 - Epoch [66/1000], Step [600/4367], Loss: 0.1965
2025-02-15 12:35:11,570 - Epoch [66/1000], Step [700/4367], Loss: 0.0913
2025-02-15 12:35:46,210 - Epoch [66/1000], Step [800/4367], Loss: 0.0680
2025-02-15 12:36:20,738 - Epoch [66/1000], Step [900/4367], Loss: 0.0254
2025-02-15 12:36:55,892 - Epoch [66/1000], Step [1000/4367], Loss: 0.4469
2025-02-15 12:37:30,955 - Epoch [66/1000], Step [1100/4367], Loss: 0.0408
2025-02-15 12:38:05,974 - Epoch [66/1000], Step [1200/4367], Loss: 0.3556
2025-02-15 12:38:40,499 - Epoch [66/1000], Step [1300/4367], Loss: 0.0677
2025-02-15 12:39:15,076 - Epoch [66/1000], Step [1400/4367], Loss: 0.0618
2025-02-15 12:39:48,962 - Epoch [66/1000], Step [1500/4367], Loss: 0.1311
2025-02-15 12:40:23,432 - Epoch [66/1000], Step [1600/4367], Loss: 0.1005
2025-02-15 12:40:57,493 - Epoch [66/1000], Step [1700/4367], Loss: 0.0430
2025-02-15 12:41:31,798 - Epoch [66/1000], Step [1800/4367], Loss: 0.0665
2025-02-15 12:42:06,210 - Epoch [66/1000], Step [1900/4367], Loss: 0.2744
2025-02-15 12:42:41,281 - Epoch [66/1000], Step [2000/4367], Loss: 0.2021
2025-02-15 12:43:16,123 - Epoch [66/1000], Step [2100/4367], Loss: 0.0154
2025-02-15 12:43:50,776 - Epoch [66/1000], Step [2200/4367], Loss: 0.1903
2025-02-15 12:44:25,822 - Epoch [66/1000], Step [2300/4367], Loss: 0.0850
2025-02-15 12:45:00,019 - Epoch [66/1000], Step [2400/4367], Loss: 0.2228
2025-02-15 12:45:34,550 - Epoch [66/1000], Step [2500/4367], Loss: 0.0295
2025-02-15 12:46:09,229 - Epoch [66/1000], Step [2600/4367], Loss: 0.1282
2025-02-15 12:46:43,636 - Epoch [66/1000], Step [2700/4367], Loss: 0.0234
2025-02-15 12:47:18,784 - Epoch [66/1000], Step [2800/4367], Loss: 0.2589
2025-02-15 12:47:53,581 - Epoch [66/1000], Step [2900/4367], Loss: 0.1212
2025-02-15 12:48:28,103 - Epoch [66/1000], Step [3000/4367], Loss: 0.0695
2025-02-15 12:49:02,410 - Epoch [66/1000], Step [3100/4367], Loss: 0.2099
2025-02-15 12:49:37,091 - Epoch [66/1000], Step [3200/4367], Loss: 0.0665
2025-02-15 12:50:11,436 - Epoch [66/1000], Step [3300/4367], Loss: 0.0558
2025-02-15 12:50:46,329 - Epoch [66/1000], Step [3400/4367], Loss: 0.0366
2025-02-15 12:51:21,239 - Epoch [66/1000], Step [3500/4367], Loss: 0.1092
2025-02-15 12:51:55,941 - Epoch [66/1000], Step [3600/4367], Loss: 0.0931
2025-02-15 12:52:30,208 - Epoch [66/1000], Step [3700/4367], Loss: 0.0346
2025-02-15 12:53:05,152 - Epoch [66/1000], Step [3800/4367], Loss: 0.2179
2025-02-15 12:53:40,294 - Epoch [66/1000], Step [3900/4367], Loss: 0.0652
2025-02-15 12:54:14,717 - Epoch [66/1000], Step [4000/4367], Loss: 0.3259
2025-02-15 12:54:49,340 - Epoch [66/1000], Step [4100/4367], Loss: 0.3589
2025-02-15 12:55:24,289 - Epoch [66/1000], Step [4200/4367], Loss: 0.2799
2025-02-15 12:55:59,118 - Epoch [66/1000], Step [4300/4367], Loss: 0.0684
2025-02-15 12:56:32,605 - Epoch [66/1000], Validation Step [100/1090], Val Loss: 0.0003
2025-02-15 12:56:41,796 - Epoch [66/1000], Validation Step [200/1090], Val Loss: 0.0016
2025-02-15 12:56:51,151 - Epoch [66/1000], Validation Step [300/1090], Val Loss: 0.2297
2025-02-15 12:57:00,815 - Epoch [66/1000], Validation Step [400/1090], Val Loss: 0.0515
2025-02-15 12:57:09,925 - Epoch [66/1000], Validation Step [500/1090], Val Loss: 0.4418
2025-02-15 12:57:19,460 - Epoch [66/1000], Validation Step [600/1090], Val Loss: 0.1570
2025-02-15 12:57:29,030 - Epoch [66/1000], Validation Step [700/1090], Val Loss: 0.1489
2025-02-15 12:57:37,818 - Epoch [66/1000], Validation Step [800/1090], Val Loss: 0.0035
2025-02-15 12:57:46,382 - Epoch [66/1000], Validation Step [900/1090], Val Loss: 0.0042
2025-02-15 12:57:55,643 - Epoch [66/1000], Validation Step [1000/1090], Val Loss: 0.0000
2025-02-15 12:58:04,269 - Epoch 66/1000, Train Loss: 0.1331, Val Loss: 0.1427, Accuracy: 94.76%
2025-02-15 12:58:04,696 - Model saved at /data2/personal/sungjin/korean_dialects/checkpoint_real/checkpoint_epoch_66.pth
2025-02-15 12:58:40,754 - Epoch [67/1000], Step [100/4367], Loss: 0.1230
2025-02-15 12:59:15,861 - Epoch [67/1000], Step [200/4367], Loss: 0.0387
2025-02-15 12:59:50,206 - Epoch [67/1000], Step [300/4367], Loss: 0.0798
2025-02-15 13:00:25,361 - Epoch [67/1000], Step [400/4367], Loss: 0.1425
2025-02-15 13:00:59,493 - Epoch [67/1000], Step [500/4367], Loss: 0.1494
2025-02-15 13:01:34,359 - Epoch [67/1000], Step [600/4367], Loss: 0.0952
2025-02-15 13:02:08,727 - Epoch [67/1000], Step [700/4367], Loss: 0.2100
2025-02-15 13:02:43,081 - Epoch [67/1000], Step [800/4367], Loss: 0.1646
2025-02-15 13:03:17,837 - Epoch [67/1000], Step [900/4367], Loss: 0.0546
2025-02-15 13:03:52,485 - Epoch [67/1000], Step [1000/4367], Loss: 0.0593
2025-02-15 13:04:26,851 - Epoch [67/1000], Step [1100/4367], Loss: 0.0956
2025-02-15 13:05:01,401 - Epoch [67/1000], Step [1200/4367], Loss: 0.1212
2025-02-15 13:05:36,154 - Epoch [67/1000], Step [1300/4367], Loss: 0.1442
2025-02-15 13:06:10,543 - Epoch [67/1000], Step [1400/4367], Loss: 0.0987
2025-02-15 13:06:45,731 - Epoch [67/1000], Step [1500/4367], Loss: 0.1455
2025-02-15 13:07:20,432 - Epoch [67/1000], Step [1600/4367], Loss: 0.1004
2025-02-15 13:07:55,267 - Epoch [67/1000], Step [1700/4367], Loss: 0.1278
2025-02-15 13:08:29,599 - Epoch [67/1000], Step [1800/4367], Loss: 0.0906
2025-02-15 13:09:04,346 - Epoch [67/1000], Step [1900/4367], Loss: 0.0435
2025-02-15 13:09:39,019 - Epoch [67/1000], Step [2000/4367], Loss: 0.1685
2025-02-15 13:10:13,987 - Epoch [67/1000], Step [2100/4367], Loss: 0.1911
2025-02-15 13:10:48,651 - Epoch [67/1000], Step [2200/4367], Loss: 0.1072
2025-02-15 13:11:22,926 - Epoch [67/1000], Step [2300/4367], Loss: 0.0982
2025-02-15 13:11:57,440 - Epoch [67/1000], Step [2400/4367], Loss: 0.1804
2025-02-15 13:12:32,355 - Epoch [67/1000], Step [2500/4367], Loss: 0.0839
2025-02-15 13:13:07,275 - Epoch [67/1000], Step [2600/4367], Loss: 0.2510
2025-02-15 13:13:42,003 - Epoch [67/1000], Step [2700/4367], Loss: 0.0461
2025-02-15 13:14:16,566 - Epoch [67/1000], Step [2800/4367], Loss: 0.1279
2025-02-15 13:14:51,121 - Epoch [67/1000], Step [2900/4367], Loss: 0.3239
2025-02-15 13:15:25,952 - Epoch [67/1000], Step [3000/4367], Loss: 0.2135
2025-02-15 13:16:00,641 - Epoch [67/1000], Step [3100/4367], Loss: 0.1401
2025-02-15 13:16:36,033 - Epoch [67/1000], Step [3200/4367], Loss: 0.3231
2025-02-15 13:17:10,367 - Epoch [67/1000], Step [3300/4367], Loss: 0.1545
2025-02-15 13:17:44,922 - Epoch [67/1000], Step [3400/4367], Loss: 0.1459
2025-02-15 13:18:18,916 - Epoch [67/1000], Step [3500/4367], Loss: 0.1644
2025-02-15 13:18:53,513 - Epoch [67/1000], Step [3600/4367], Loss: 0.2093
2025-02-15 13:19:28,097 - Epoch [67/1000], Step [3700/4367], Loss: 0.0880
2025-02-15 13:20:03,043 - Epoch [67/1000], Step [3800/4367], Loss: 0.1130
2025-02-15 13:20:37,825 - Epoch [67/1000], Step [3900/4367], Loss: 0.0975
2025-02-15 13:21:12,757 - Epoch [67/1000], Step [4000/4367], Loss: 0.1634
2025-02-15 13:21:47,701 - Epoch [67/1000], Step [4100/4367], Loss: 0.3516
2025-02-15 13:22:22,902 - Epoch [67/1000], Step [4200/4367], Loss: 0.0682
2025-02-15 13:22:57,352 - Epoch [67/1000], Step [4300/4367], Loss: 0.0923
2025-02-15 13:23:30,243 - Epoch [67/1000], Validation Step [100/1090], Val Loss: 0.0002
2025-02-15 13:23:39,461 - Epoch [67/1000], Validation Step [200/1090], Val Loss: 0.0004
2025-02-15 13:23:48,818 - Epoch [67/1000], Validation Step [300/1090], Val Loss: 0.2648
2025-02-15 13:23:58,488 - Epoch [67/1000], Validation Step [400/1090], Val Loss: 0.0728
2025-02-15 13:24:07,603 - Epoch [67/1000], Validation Step [500/1090], Val Loss: 0.5816
2025-02-15 13:24:17,142 - Epoch [67/1000], Validation Step [600/1090], Val Loss: 0.1269
2025-02-15 13:24:26,717 - Epoch [67/1000], Validation Step [700/1090], Val Loss: 0.1294
2025-02-15 13:24:35,515 - Epoch [67/1000], Validation Step [800/1090], Val Loss: 0.0023
2025-02-15 13:24:44,083 - Epoch [67/1000], Validation Step [900/1090], Val Loss: 0.0020
2025-02-15 13:24:53,356 - Epoch [67/1000], Validation Step [1000/1090], Val Loss: 0.0000
2025-02-15 13:25:01,990 - Epoch 67/1000, Train Loss: 0.1342, Val Loss: 0.1435, Accuracy: 94.77%
2025-02-15 13:25:38,211 - Epoch [68/1000], Step [100/4367], Loss: 0.0978
2025-02-15 13:26:13,343 - Epoch [68/1000], Step [200/4367], Loss: 0.2639
2025-02-15 13:26:48,300 - Epoch [68/1000], Step [300/4367], Loss: 0.1992
2025-02-15 13:27:23,382 - Epoch [68/1000], Step [400/4367], Loss: 0.0686
2025-02-15 13:27:58,551 - Epoch [68/1000], Step [500/4367], Loss: 0.2034
2025-02-15 13:28:33,417 - Epoch [68/1000], Step [600/4367], Loss: 0.0763
2025-02-15 13:29:08,166 - Epoch [68/1000], Step [700/4367], Loss: 0.1505
2025-02-15 13:29:42,505 - Epoch [68/1000], Step [800/4367], Loss: 0.1026
2025-02-15 13:30:17,424 - Epoch [68/1000], Step [900/4367], Loss: 0.1792
2025-02-15 13:30:52,220 - Epoch [68/1000], Step [1000/4367], Loss: 0.2555
2025-02-15 13:31:26,726 - Epoch [68/1000], Step [1100/4367], Loss: 0.2390
2025-02-15 13:32:01,556 - Epoch [68/1000], Step [1200/4367], Loss: 0.3185
2025-02-15 13:32:35,730 - Epoch [68/1000], Step [1300/4367], Loss: 0.1041
2025-02-15 13:33:10,544 - Epoch [68/1000], Step [1400/4367], Loss: 0.0968
2025-02-15 13:33:44,831 - Epoch [68/1000], Step [1500/4367], Loss: 0.0531
2025-02-15 13:34:19,724 - Epoch [68/1000], Step [1600/4367], Loss: 0.0822
2025-02-15 13:34:54,281 - Epoch [68/1000], Step [1700/4367], Loss: 0.0652
2025-02-15 13:35:28,846 - Epoch [68/1000], Step [1800/4367], Loss: 0.1988
2025-02-15 13:36:03,282 - Epoch [68/1000], Step [1900/4367], Loss: 0.0829
2025-02-15 13:36:37,987 - Epoch [68/1000], Step [2000/4367], Loss: 0.1680
2025-02-15 13:37:12,564 - Epoch [68/1000], Step [2100/4367], Loss: 0.1085
2025-02-15 13:37:47,403 - Epoch [68/1000], Step [2200/4367], Loss: 0.1448
2025-02-15 13:38:22,229 - Epoch [68/1000], Step [2300/4367], Loss: 0.0541
2025-02-15 13:38:56,893 - Epoch [68/1000], Step [2400/4367], Loss: 0.1159
2025-02-15 13:39:31,616 - Epoch [68/1000], Step [2500/4367], Loss: 0.4109
2025-02-15 13:40:06,401 - Epoch [68/1000], Step [2600/4367], Loss: 0.2446
2025-02-15 13:40:41,197 - Epoch [68/1000], Step [2700/4367], Loss: 0.1037
2025-02-15 13:41:15,998 - Epoch [68/1000], Step [2800/4367], Loss: 0.1756
2025-02-15 13:41:50,795 - Epoch [68/1000], Step [2900/4367], Loss: 0.0447
2025-02-15 13:42:25,626 - Epoch [68/1000], Step [3000/4367], Loss: 0.1800
2025-02-15 13:43:00,086 - Epoch [68/1000], Step [3100/4367], Loss: 0.1893
2025-02-15 13:43:35,063 - Epoch [68/1000], Step [3200/4367], Loss: 0.1958
2025-02-15 13:44:09,647 - Epoch [68/1000], Step [3300/4367], Loss: 0.0684
2025-02-15 13:44:43,926 - Epoch [68/1000], Step [3400/4367], Loss: 0.0975
2025-02-15 13:45:18,830 - Epoch [68/1000], Step [3500/4367], Loss: 0.0836
2025-02-15 13:45:53,676 - Epoch [68/1000], Step [3600/4367], Loss: 0.0968
2025-02-15 13:46:28,256 - Epoch [68/1000], Step [3700/4367], Loss: 0.0512
2025-02-15 13:47:03,251 - Epoch [68/1000], Step [3800/4367], Loss: 0.0988
2025-02-15 13:47:36,998 - Epoch [68/1000], Step [3900/4367], Loss: 0.0938
2025-02-15 13:48:11,765 - Epoch [68/1000], Step [4000/4367], Loss: 0.1009
2025-02-15 13:48:46,356 - Epoch [68/1000], Step [4100/4367], Loss: 0.3154
2025-02-15 13:49:21,703 - Epoch [68/1000], Step [4200/4367], Loss: 0.2582
2025-02-15 13:49:56,366 - Epoch [68/1000], Step [4300/4367], Loss: 0.2508
2025-02-15 13:50:29,225 - Epoch [68/1000], Validation Step [100/1090], Val Loss: 0.0001
2025-02-15 13:50:38,426 - Epoch [68/1000], Validation Step [200/1090], Val Loss: 0.0001
2025-02-15 13:50:47,774 - Epoch [68/1000], Validation Step [300/1090], Val Loss: 0.2504
2025-02-15 13:50:57,455 - Epoch [68/1000], Validation Step [400/1090], Val Loss: 0.0484
2025-02-15 13:51:06,569 - Epoch [68/1000], Validation Step [500/1090], Val Loss: 0.3888
2025-02-15 13:51:16,102 - Epoch [68/1000], Validation Step [600/1090], Val Loss: 0.1388
2025-02-15 13:51:25,677 - Epoch [68/1000], Validation Step [700/1090], Val Loss: 0.1218
2025-02-15 13:51:34,470 - Epoch [68/1000], Validation Step [800/1090], Val Loss: 0.0067
2025-02-15 13:51:43,031 - Epoch [68/1000], Validation Step [900/1090], Val Loss: 0.0053
2025-02-15 13:51:52,291 - Epoch [68/1000], Validation Step [1000/1090], Val Loss: 0.0001
2025-02-15 13:52:00,904 - Epoch 68/1000, Train Loss: 0.1335, Val Loss: 0.1410, Accuracy: 94.84%
2025-02-15 13:52:01,323 - Model saved at /data2/personal/sungjin/korean_dialects/checkpoint_real/checkpoint_epoch_68.pth
2025-02-15 13:52:37,519 - Epoch [69/1000], Step [100/4367], Loss: 0.1360
2025-02-15 13:53:11,648 - Epoch [69/1000], Step [200/4367], Loss: 0.1574
2025-02-15 13:53:46,213 - Epoch [69/1000], Step [300/4367], Loss: 0.0293
2025-02-15 13:54:20,087 - Epoch [69/1000], Step [400/4367], Loss: 0.1411
2025-02-15 13:54:54,958 - Epoch [69/1000], Step [500/4367], Loss: 0.0504
2025-02-15 13:55:29,435 - Epoch [69/1000], Step [600/4367], Loss: 0.2154
2025-02-15 13:56:03,993 - Epoch [69/1000], Step [700/4367], Loss: 0.1453
2025-02-15 13:56:38,541 - Epoch [69/1000], Step [800/4367], Loss: 0.1358
2025-02-15 13:57:13,243 - Epoch [69/1000], Step [900/4367], Loss: 0.1354
2025-02-15 13:57:48,167 - Epoch [69/1000], Step [1000/4367], Loss: 0.1651
2025-02-15 13:58:22,556 - Epoch [69/1000], Step [1100/4367], Loss: 0.1565
2025-02-15 13:58:57,394 - Epoch [69/1000], Step [1200/4367], Loss: 0.1024
2025-02-15 13:59:32,264 - Epoch [69/1000], Step [1300/4367], Loss: 0.0433
2025-02-15 14:00:06,917 - Epoch [69/1000], Step [1400/4367], Loss: 0.1316
2025-02-15 14:00:41,600 - Epoch [69/1000], Step [1500/4367], Loss: 0.3037
2025-02-15 14:01:16,087 - Epoch [69/1000], Step [1600/4367], Loss: 0.0228
2025-02-15 14:01:50,625 - Epoch [69/1000], Step [1700/4367], Loss: 0.1367
2025-02-15 14:02:25,664 - Epoch [69/1000], Step [1800/4367], Loss: 0.0988
2025-02-15 14:03:00,183 - Epoch [69/1000], Step [1900/4367], Loss: 0.4040
2025-02-15 14:03:34,933 - Epoch [69/1000], Step [2000/4367], Loss: 0.2228
2025-02-15 14:04:09,643 - Epoch [69/1000], Step [2100/4367], Loss: 0.0609
2025-02-15 14:04:44,448 - Epoch [69/1000], Step [2200/4367], Loss: 0.0943
2025-02-15 14:05:19,152 - Epoch [69/1000], Step [2300/4367], Loss: 0.0610
2025-02-15 14:05:53,829 - Epoch [69/1000], Step [2400/4367], Loss: 0.0827
2025-02-15 14:06:28,404 - Epoch [69/1000], Step [2500/4367], Loss: 0.1477
2025-02-15 14:07:03,117 - Epoch [69/1000], Step [2600/4367], Loss: 0.0561
2025-02-15 14:07:38,005 - Epoch [69/1000], Step [2700/4367], Loss: 0.1000
2025-02-15 14:08:12,667 - Epoch [69/1000], Step [2800/4367], Loss: 0.1147
2025-02-15 14:08:47,167 - Epoch [69/1000], Step [2900/4367], Loss: 0.2576
2025-02-15 14:09:22,366 - Epoch [69/1000], Step [3000/4367], Loss: 0.2008
2025-02-15 14:09:57,248 - Epoch [69/1000], Step [3100/4367], Loss: 0.0389
2025-02-15 14:10:31,884 - Epoch [69/1000], Step [3200/4367], Loss: 0.0370
2025-02-15 14:11:06,876 - Epoch [69/1000], Step [3300/4367], Loss: 0.1556
2025-02-15 14:11:41,397 - Epoch [69/1000], Step [3400/4367], Loss: 0.0823
2025-02-15 14:12:16,097 - Epoch [69/1000], Step [3500/4367], Loss: 0.0750
2025-02-15 14:12:50,624 - Epoch [69/1000], Step [3600/4367], Loss: 0.0592
2025-02-15 14:13:25,248 - Epoch [69/1000], Step [3700/4367], Loss: 0.3787
2025-02-15 14:14:00,149 - Epoch [69/1000], Step [3800/4367], Loss: 0.1866
2025-02-15 14:14:35,049 - Epoch [69/1000], Step [3900/4367], Loss: 0.0939
2025-02-15 14:15:09,629 - Epoch [69/1000], Step [4000/4367], Loss: 0.0861
2025-02-15 14:15:44,382 - Epoch [69/1000], Step [4100/4367], Loss: 0.0618
2025-02-15 14:16:19,046 - Epoch [69/1000], Step [4200/4367], Loss: 0.1316
2025-02-15 14:16:53,788 - Epoch [69/1000], Step [4300/4367], Loss: 0.1324
2025-02-15 14:17:27,157 - Epoch [69/1000], Validation Step [100/1090], Val Loss: 0.0001
2025-02-15 14:17:36,364 - Epoch [69/1000], Validation Step [200/1090], Val Loss: 0.0004
2025-02-15 14:17:45,713 - Epoch [69/1000], Validation Step [300/1090], Val Loss: 0.2868
2025-02-15 14:17:55,379 - Epoch [69/1000], Validation Step [400/1090], Val Loss: 0.0420
2025-02-15 14:18:04,485 - Epoch [69/1000], Validation Step [500/1090], Val Loss: 0.4315
2025-02-15 14:18:14,013 - Epoch [69/1000], Validation Step [600/1090], Val Loss: 0.1105
2025-02-15 14:18:23,578 - Epoch [69/1000], Validation Step [700/1090], Val Loss: 0.0926
2025-02-15 14:18:32,374 - Epoch [69/1000], Validation Step [800/1090], Val Loss: 0.0033
2025-02-15 14:18:40,940 - Epoch [69/1000], Validation Step [900/1090], Val Loss: 0.0032
2025-02-15 14:18:50,209 - Epoch [69/1000], Validation Step [1000/1090], Val Loss: 0.0001
2025-02-15 14:18:58,834 - Epoch 69/1000, Train Loss: 0.1335, Val Loss: 0.1408, Accuracy: 94.87%
2025-02-15 14:19:35,014 - Epoch [70/1000], Step [100/4367], Loss: 0.0762
2025-02-15 14:20:09,681 - Epoch [70/1000], Step [200/4367], Loss: 0.1393
2025-02-15 14:20:44,386 - Epoch [70/1000], Step [300/4367], Loss: 0.0508
2025-02-15 14:21:19,571 - Epoch [70/1000], Step [400/4367], Loss: 0.0640
2025-02-15 14:21:54,453 - Epoch [70/1000], Step [500/4367], Loss: 0.0939
2025-02-15 14:22:29,178 - Epoch [70/1000], Step [600/4367], Loss: 0.0550
2025-02-15 14:23:03,902 - Epoch [70/1000], Step [700/4367], Loss: 0.1329
2025-02-15 14:23:37,990 - Epoch [70/1000], Step [800/4367], Loss: 0.1528
2025-02-15 14:24:12,569 - Epoch [70/1000], Step [900/4367], Loss: 0.2529
2025-02-15 14:24:47,139 - Epoch [70/1000], Step [1000/4367], Loss: 0.1798
2025-02-15 14:25:21,636 - Epoch [70/1000], Step [1100/4367], Loss: 0.1245
2025-02-15 14:25:56,169 - Epoch [70/1000], Step [1200/4367], Loss: 0.0897
2025-02-15 14:26:30,987 - Epoch [70/1000], Step [1300/4367], Loss: 0.4941
2025-02-15 14:27:05,940 - Epoch [70/1000], Step [1400/4367], Loss: 0.0910
2025-02-15 14:27:40,406 - Epoch [70/1000], Step [1500/4367], Loss: 0.0608
2025-02-15 14:28:15,272 - Epoch [70/1000], Step [1600/4367], Loss: 0.2338
2025-02-15 14:28:50,271 - Epoch [70/1000], Step [1700/4367], Loss: 0.1768
2025-02-15 14:29:25,380 - Epoch [70/1000], Step [1800/4367], Loss: 0.1465
2025-02-15 14:29:59,879 - Epoch [70/1000], Step [1900/4367], Loss: 0.0111
2025-02-15 14:30:34,716 - Epoch [70/1000], Step [2000/4367], Loss: 0.0445
2025-02-15 14:31:09,183 - Epoch [70/1000], Step [2100/4367], Loss: 0.1159
2025-02-15 14:31:44,364 - Epoch [70/1000], Step [2200/4367], Loss: 0.0866
2025-02-15 14:32:18,812 - Epoch [70/1000], Step [2300/4367], Loss: 0.2872
2025-02-15 14:32:53,476 - Epoch [70/1000], Step [2400/4367], Loss: 0.0426
2025-02-15 14:33:27,624 - Epoch [70/1000], Step [2500/4367], Loss: 0.3674
2025-02-15 14:34:02,285 - Epoch [70/1000], Step [2600/4367], Loss: 0.0202
2025-02-15 14:34:36,767 - Epoch [70/1000], Step [2700/4367], Loss: 0.1582
2025-02-15 14:35:11,719 - Epoch [70/1000], Step [2800/4367], Loss: 0.0944
2025-02-15 14:35:46,572 - Epoch [70/1000], Step [2900/4367], Loss: 0.0379
2025-02-15 14:36:21,438 - Epoch [70/1000], Step [3000/4367], Loss: 0.1451
2025-02-15 14:36:55,851 - Epoch [70/1000], Step [3100/4367], Loss: 0.0474
2025-02-15 14:37:30,133 - Epoch [70/1000], Step [3200/4367], Loss: 0.0788
2025-02-15 14:38:04,632 - Epoch [70/1000], Step [3300/4367], Loss: 0.1412
2025-02-15 14:38:38,882 - Epoch [70/1000], Step [3400/4367], Loss: 0.1207
2025-02-15 14:39:13,572 - Epoch [70/1000], Step [3500/4367], Loss: 0.0629
2025-02-15 14:39:48,323 - Epoch [70/1000], Step [3600/4367], Loss: 0.1709
2025-02-15 14:40:23,239 - Epoch [70/1000], Step [3700/4367], Loss: 0.2241
2025-02-15 14:40:58,223 - Epoch [70/1000], Step [3800/4367], Loss: 0.1605
2025-02-15 14:41:33,241 - Epoch [70/1000], Step [3900/4367], Loss: 0.2895
2025-02-15 14:42:07,970 - Epoch [70/1000], Step [4000/4367], Loss: 0.2356
2025-02-15 14:42:43,236 - Epoch [70/1000], Step [4100/4367], Loss: 0.1097
2025-02-15 14:43:17,820 - Epoch [70/1000], Step [4200/4367], Loss: 0.1509
2025-02-15 14:43:52,973 - Epoch [70/1000], Step [4300/4367], Loss: 0.1550
2025-02-15 14:44:26,285 - Epoch [70/1000], Validation Step [100/1090], Val Loss: 0.0000
2025-02-15 14:44:35,498 - Epoch [70/1000], Validation Step [200/1090], Val Loss: 0.0001
2025-02-15 14:44:44,849 - Epoch [70/1000], Validation Step [300/1090], Val Loss: 0.2470
2025-02-15 14:44:54,526 - Epoch [70/1000], Validation Step [400/1090], Val Loss: 0.0476
2025-02-15 14:45:03,643 - Epoch [70/1000], Validation Step [500/1090], Val Loss: 0.4294
2025-02-15 14:45:13,185 - Epoch [70/1000], Validation Step [600/1090], Val Loss: 0.1446
2025-02-15 14:45:22,762 - Epoch [70/1000], Validation Step [700/1090], Val Loss: 0.1388
2025-02-15 14:45:31,564 - Epoch [70/1000], Validation Step [800/1090], Val Loss: 0.0036
2025-02-15 14:45:40,128 - Epoch [70/1000], Validation Step [900/1090], Val Loss: 0.0033
2025-02-15 14:45:49,392 - Epoch [70/1000], Validation Step [1000/1090], Val Loss: 0.0000
2025-02-15 14:45:58,027 - Epoch 70/1000, Train Loss: 0.1347, Val Loss: 0.1407, Accuracy: 94.86%
2025-02-15 14:45:58,511 - Model saved at /data2/personal/sungjin/korean_dialects/checkpoint_real/checkpoint_epoch_70.pth
2025-02-15 14:46:34,314 - Epoch [71/1000], Step [100/4367], Loss: 0.1136
2025-02-15 14:47:08,996 - Epoch [71/1000], Step [200/4367], Loss: 0.0827
2025-02-15 14:47:43,773 - Epoch [71/1000], Step [300/4367], Loss: 0.2305
2025-02-15 14:48:18,292 - Epoch [71/1000], Step [400/4367], Loss: 0.0946
2025-02-15 14:48:53,246 - Epoch [71/1000], Step [500/4367], Loss: 0.0811
2025-02-15 14:49:27,587 - Epoch [71/1000], Step [600/4367], Loss: 0.0995
2025-02-15 14:50:02,456 - Epoch [71/1000], Step [700/4367], Loss: 0.0730
2025-02-15 14:50:36,760 - Epoch [71/1000], Step [800/4367], Loss: 0.1129
2025-02-15 14:51:11,330 - Epoch [71/1000], Step [900/4367], Loss: 0.2264
2025-02-15 14:51:46,375 - Epoch [71/1000], Step [1000/4367], Loss: 0.0565
2025-02-15 14:52:21,055 - Epoch [71/1000], Step [1100/4367], Loss: 0.0693
2025-02-15 14:52:55,338 - Epoch [71/1000], Step [1200/4367], Loss: 0.1553
2025-02-15 14:53:29,868 - Epoch [71/1000], Step [1300/4367], Loss: 0.1103
2025-02-15 14:54:04,427 - Epoch [71/1000], Step [1400/4367], Loss: 0.2768
2025-02-15 14:54:39,138 - Epoch [71/1000], Step [1500/4367], Loss: 0.0319
2025-02-15 14:55:13,621 - Epoch [71/1000], Step [1600/4367], Loss: 0.2723
2025-02-15 14:55:48,334 - Epoch [71/1000], Step [1700/4367], Loss: 0.1477
2025-02-15 14:56:22,701 - Epoch [71/1000], Step [1800/4367], Loss: 0.1516
2025-02-15 14:56:57,948 - Epoch [71/1000], Step [1900/4367], Loss: 0.2967
2025-02-15 14:57:32,235 - Epoch [71/1000], Step [2000/4367], Loss: 0.4506
2025-02-15 14:58:07,057 - Epoch [71/1000], Step [2100/4367], Loss: 0.1000
2025-02-15 14:58:41,201 - Epoch [71/1000], Step [2200/4367], Loss: 0.0499
2025-02-15 14:59:15,589 - Epoch [71/1000], Step [2300/4367], Loss: 0.0749
2025-02-15 14:59:50,268 - Epoch [71/1000], Step [2400/4367], Loss: 0.0994
2025-02-15 15:00:24,577 - Epoch [71/1000], Step [2500/4367], Loss: 0.2393
2025-02-15 15:00:59,138 - Epoch [71/1000], Step [2600/4367], Loss: 0.1555
2025-02-15 15:01:33,952 - Epoch [71/1000], Step [2700/4367], Loss: 0.0481
2025-02-15 15:02:08,278 - Epoch [71/1000], Step [2800/4367], Loss: 0.2258
2025-02-15 15:02:42,877 - Epoch [71/1000], Step [2900/4367], Loss: 0.3185
2025-02-15 15:03:17,350 - Epoch [71/1000], Step [3000/4367], Loss: 0.0759
2025-02-15 15:03:52,093 - Epoch [71/1000], Step [3100/4367], Loss: 0.4401
2025-02-15 15:04:27,045 - Epoch [71/1000], Step [3200/4367], Loss: 0.1804
2025-02-15 15:05:01,835 - Epoch [71/1000], Step [3300/4367], Loss: 0.0594
2025-02-15 15:05:36,729 - Epoch [71/1000], Step [3400/4367], Loss: 0.1004
2025-02-15 15:06:11,630 - Epoch [71/1000], Step [3500/4367], Loss: 0.1443
2025-02-15 15:06:46,225 - Epoch [71/1000], Step [3600/4367], Loss: 0.1256
2025-02-15 15:07:21,213 - Epoch [71/1000], Step [3700/4367], Loss: 0.0613
2025-02-15 15:07:56,008 - Epoch [71/1000], Step [3800/4367], Loss: 0.0988
2025-02-15 15:08:31,218 - Epoch [71/1000], Step [3900/4367], Loss: 0.2624
2025-02-15 15:09:06,128 - Epoch [71/1000], Step [4000/4367], Loss: 0.1826
2025-02-15 15:09:40,818 - Epoch [71/1000], Step [4100/4367], Loss: 0.1186
2025-02-15 15:10:15,497 - Epoch [71/1000], Step [4200/4367], Loss: 0.2354
2025-02-15 15:10:50,589 - Epoch [71/1000], Step [4300/4367], Loss: 0.2909
2025-02-15 15:11:24,094 - Epoch [71/1000], Validation Step [100/1090], Val Loss: 0.0001
2025-02-15 15:11:33,291 - Epoch [71/1000], Validation Step [200/1090], Val Loss: 0.0002
2025-02-15 15:11:42,646 - Epoch [71/1000], Validation Step [300/1090], Val Loss: 0.3040
2025-02-15 15:11:52,323 - Epoch [71/1000], Validation Step [400/1090], Val Loss: 0.0629
2025-02-15 15:12:01,438 - Epoch [71/1000], Validation Step [500/1090], Val Loss: 0.4960
2025-02-15 15:12:10,982 - Epoch [71/1000], Validation Step [600/1090], Val Loss: 0.0909
2025-02-15 15:12:20,542 - Epoch [71/1000], Validation Step [700/1090], Val Loss: 0.1021
2025-02-15 15:12:29,325 - Epoch [71/1000], Validation Step [800/1090], Val Loss: 0.0032
2025-02-15 15:12:37,902 - Epoch [71/1000], Validation Step [900/1090], Val Loss: 0.0027
2025-02-15 15:12:47,163 - Epoch [71/1000], Validation Step [1000/1090], Val Loss: 0.0000
2025-02-15 15:12:55,799 - Epoch 71/1000, Train Loss: 0.1326, Val Loss: 0.1409, Accuracy: 94.80%
2025-02-15 15:13:31,470 - Epoch [72/1000], Step [100/4367], Loss: 0.1373
2025-02-15 15:14:06,212 - Epoch [72/1000], Step [200/4367], Loss: 0.0659
2025-02-15 15:14:40,739 - Epoch [72/1000], Step [300/4367], Loss: 0.1986
2025-02-15 15:15:15,113 - Epoch [72/1000], Step [400/4367], Loss: 0.0683
2025-02-15 15:15:49,971 - Epoch [72/1000], Step [500/4367], Loss: 0.1929
2025-02-15 15:16:24,813 - Epoch [72/1000], Step [600/4367], Loss: 0.2288
2025-02-15 15:17:00,025 - Epoch [72/1000], Step [700/4367], Loss: 0.0506
2025-02-15 15:17:34,241 - Epoch [72/1000], Step [800/4367], Loss: 0.2392
2025-02-15 15:18:08,988 - Epoch [72/1000], Step [900/4367], Loss: 0.1566
2025-02-15 15:18:43,647 - Epoch [72/1000], Step [1000/4367], Loss: 0.1486
2025-02-15 15:19:18,437 - Epoch [72/1000], Step [1100/4367], Loss: 0.0875
2025-02-15 15:19:53,265 - Epoch [72/1000], Step [1200/4367], Loss: 0.0463
2025-02-15 15:20:27,617 - Epoch [72/1000], Step [1300/4367], Loss: 0.1370
2025-02-15 15:21:02,227 - Epoch [72/1000], Step [1400/4367], Loss: 0.2965
2025-02-15 15:21:37,102 - Epoch [72/1000], Step [1500/4367], Loss: 0.0387
2025-02-15 15:22:12,068 - Epoch [72/1000], Step [1600/4367], Loss: 0.3067
2025-02-15 15:22:46,693 - Epoch [72/1000], Step [1700/4367], Loss: 0.1703
2025-02-15 15:23:20,930 - Epoch [72/1000], Step [1800/4367], Loss: 0.0484
2025-02-15 15:23:55,465 - Epoch [72/1000], Step [1900/4367], Loss: 0.0667
2025-02-15 15:24:30,656 - Epoch [72/1000], Step [2000/4367], Loss: 0.1130
2025-02-15 15:25:05,293 - Epoch [72/1000], Step [2100/4367], Loss: 0.1592
2025-02-15 15:25:40,110 - Epoch [72/1000], Step [2200/4367], Loss: 0.1544
2025-02-15 15:26:15,145 - Epoch [72/1000], Step [2300/4367], Loss: 0.0399
2025-02-15 15:26:50,197 - Epoch [72/1000], Step [2400/4367], Loss: 0.0900
2025-02-15 15:27:25,084 - Epoch [72/1000], Step [2500/4367], Loss: 0.0857
2025-02-15 15:28:00,187 - Epoch [72/1000], Step [2600/4367], Loss: 0.2788
2025-02-15 15:28:34,800 - Epoch [72/1000], Step [2700/4367], Loss: 0.1438
2025-02-15 15:29:09,405 - Epoch [72/1000], Step [2800/4367], Loss: 0.4184
2025-02-15 15:29:44,431 - Epoch [72/1000], Step [2900/4367], Loss: 0.1219
2025-02-15 15:30:19,341 - Epoch [72/1000], Step [3000/4367], Loss: 0.0341
2025-02-15 15:30:54,367 - Epoch [72/1000], Step [3100/4367], Loss: 0.1388
2025-02-15 15:31:28,799 - Epoch [72/1000], Step [3200/4367], Loss: 0.0636
2025-02-15 15:32:03,317 - Epoch [72/1000], Step [3300/4367], Loss: 0.2681
2025-02-15 15:32:38,180 - Epoch [72/1000], Step [3400/4367], Loss: 0.0644
2025-02-15 15:33:13,035 - Epoch [72/1000], Step [3500/4367], Loss: 0.0409
2025-02-15 15:33:47,800 - Epoch [72/1000], Step [3600/4367], Loss: 0.1192
2025-02-15 15:34:22,784 - Epoch [72/1000], Step [3700/4367], Loss: 0.1381
2025-02-15 15:34:57,376 - Epoch [72/1000], Step [3800/4367], Loss: 0.1328
2025-02-15 15:35:31,726 - Epoch [72/1000], Step [3900/4367], Loss: 0.0906
2025-02-15 15:36:06,250 - Epoch [72/1000], Step [4000/4367], Loss: 0.2120
2025-02-15 15:36:40,814 - Epoch [72/1000], Step [4100/4367], Loss: 0.0569
2025-02-15 15:37:15,979 - Epoch [72/1000], Step [4200/4367], Loss: 0.1116
2025-02-15 15:37:50,314 - Epoch [72/1000], Step [4300/4367], Loss: 0.0334
2025-02-15 15:38:23,377 - Epoch [72/1000], Validation Step [100/1090], Val Loss: 0.0002
2025-02-15 15:38:32,514 - Epoch [72/1000], Validation Step [200/1090], Val Loss: 0.0003
2025-02-15 15:38:41,800 - Epoch [72/1000], Validation Step [300/1090], Val Loss: 0.2672
2025-02-15 15:38:51,416 - Epoch [72/1000], Validation Step [400/1090], Val Loss: 0.0475
2025-02-15 15:39:00,469 - Epoch [72/1000], Validation Step [500/1090], Val Loss: 0.3841
2025-02-15 15:39:09,952 - Epoch [72/1000], Validation Step [600/1090], Val Loss: 0.1366
2025-02-15 15:39:19,475 - Epoch [72/1000], Validation Step [700/1090], Val Loss: 0.1322
2025-02-15 15:39:28,213 - Epoch [72/1000], Validation Step [800/1090], Val Loss: 0.0060
2025-02-15 15:39:36,729 - Epoch [72/1000], Validation Step [900/1090], Val Loss: 0.0052
2025-02-15 15:39:45,928 - Epoch [72/1000], Validation Step [1000/1090], Val Loss: 0.0001
2025-02-15 15:39:54,486 - Epoch 72/1000, Train Loss: 0.1325, Val Loss: 0.1405, Accuracy: 94.91%
2025-02-15 15:39:54,929 - Model saved at /data2/personal/sungjin/korean_dialects/checkpoint_real/checkpoint_epoch_72.pth
2025-02-15 15:40:30,340 - Epoch [73/1000], Step [100/4367], Loss: 0.1528
2025-02-15 15:41:05,723 - Epoch [73/1000], Step [200/4367], Loss: 0.0927
2025-02-15 15:41:40,162 - Epoch [73/1000], Step [300/4367], Loss: 0.1443
2025-02-15 15:42:15,000 - Epoch [73/1000], Step [400/4367], Loss: 0.0232
2025-02-15 15:42:49,670 - Epoch [73/1000], Step [500/4367], Loss: 0.0660
2025-02-15 15:43:23,761 - Epoch [73/1000], Step [600/4367], Loss: 0.2082
2025-02-15 15:43:58,530 - Epoch [73/1000], Step [700/4367], Loss: 0.1825
2025-02-15 15:44:32,680 - Epoch [73/1000], Step [800/4367], Loss: 0.2943
2025-02-15 15:45:07,230 - Epoch [73/1000], Step [900/4367], Loss: 0.0434
2025-02-15 15:45:41,686 - Epoch [73/1000], Step [1000/4367], Loss: 0.1399
2025-02-15 15:46:16,117 - Epoch [73/1000], Step [1100/4367], Loss: 0.0278
2025-02-15 15:46:51,065 - Epoch [73/1000], Step [1200/4367], Loss: 0.2206
2025-02-15 15:47:25,523 - Epoch [73/1000], Step [1300/4367], Loss: 0.3547
2025-02-15 15:48:00,411 - Epoch [73/1000], Step [1400/4367], Loss: 0.0929
2025-02-15 15:48:35,284 - Epoch [73/1000], Step [1500/4367], Loss: 0.2452
2025-02-15 15:49:09,767 - Epoch [73/1000], Step [1600/4367], Loss: 0.1956
2025-02-15 15:49:44,392 - Epoch [73/1000], Step [1700/4367], Loss: 0.2244
2025-02-15 15:50:19,254 - Epoch [73/1000], Step [1800/4367], Loss: 0.0462
2025-02-15 15:50:53,910 - Epoch [73/1000], Step [1900/4367], Loss: 0.1670
2025-02-15 15:51:28,808 - Epoch [73/1000], Step [2000/4367], Loss: 0.0679
2025-02-15 15:52:03,613 - Epoch [73/1000], Step [2100/4367], Loss: 0.0483
2025-02-15 15:52:38,256 - Epoch [73/1000], Step [2200/4367], Loss: 0.1388
2025-02-15 15:53:12,666 - Epoch [73/1000], Step [2300/4367], Loss: 0.0930
2025-02-15 15:53:47,261 - Epoch [73/1000], Step [2400/4367], Loss: 0.0961
2025-02-15 15:54:22,131 - Epoch [73/1000], Step [2500/4367], Loss: 0.1811
2025-02-15 15:54:56,885 - Epoch [73/1000], Step [2600/4367], Loss: 0.2073
2025-02-15 15:55:31,466 - Epoch [73/1000], Step [2700/4367], Loss: 0.2403
2025-02-15 15:56:06,128 - Epoch [73/1000], Step [2800/4367], Loss: 0.1753
2025-02-15 15:56:41,055 - Epoch [73/1000], Step [2900/4367], Loss: 0.0224
2025-02-15 15:57:16,177 - Epoch [73/1000], Step [3000/4367], Loss: 0.1142
2025-02-15 15:57:50,426 - Epoch [73/1000], Step [3100/4367], Loss: 0.1888
2025-02-15 15:58:24,822 - Epoch [73/1000], Step [3200/4367], Loss: 0.0547
2025-02-15 15:58:59,454 - Epoch [73/1000], Step [3300/4367], Loss: 0.0318
2025-02-15 15:59:34,025 - Epoch [73/1000], Step [3400/4367], Loss: 0.1228
2025-02-15 16:00:08,929 - Epoch [73/1000], Step [3500/4367], Loss: 0.1254
2025-02-15 16:00:43,367 - Epoch [73/1000], Step [3600/4367], Loss: 0.4323
2025-02-15 16:01:18,071 - Epoch [73/1000], Step [3700/4367], Loss: 0.1341
2025-02-15 16:01:53,312 - Epoch [73/1000], Step [3800/4367], Loss: 0.1252
2025-02-15 16:02:27,622 - Epoch [73/1000], Step [3900/4367], Loss: 0.2382
2025-02-15 16:03:02,221 - Epoch [73/1000], Step [4000/4367], Loss: 0.0361
2025-02-15 16:03:37,039 - Epoch [73/1000], Step [4100/4367], Loss: 0.0497
2025-02-15 16:04:11,447 - Epoch [73/1000], Step [4200/4367], Loss: 0.0952
2025-02-15 16:04:46,146 - Epoch [73/1000], Step [4300/4367], Loss: 0.1784
2025-02-15 16:05:19,329 - Epoch [73/1000], Validation Step [100/1090], Val Loss: 0.0002
2025-02-15 16:05:28,530 - Epoch [73/1000], Validation Step [200/1090], Val Loss: 0.0005
2025-02-15 16:05:37,882 - Epoch [73/1000], Validation Step [300/1090], Val Loss: 0.2672
2025-02-15 16:05:47,542 - Epoch [73/1000], Validation Step [400/1090], Val Loss: 0.0570
2025-02-15 16:05:56,656 - Epoch [73/1000], Validation Step [500/1090], Val Loss: 0.4087
2025-02-15 16:06:06,199 - Epoch [73/1000], Validation Step [600/1090], Val Loss: 0.1117
2025-02-15 16:06:15,778 - Epoch [73/1000], Validation Step [700/1090], Val Loss: 0.1166
2025-02-15 16:06:24,570 - Epoch [73/1000], Validation Step [800/1090], Val Loss: 0.0058
2025-02-15 16:06:33,134 - Epoch [73/1000], Validation Step [900/1090], Val Loss: 0.0046
2025-02-15 16:06:42,394 - Epoch [73/1000], Validation Step [1000/1090], Val Loss: 0.0000
2025-02-15 16:06:51,036 - Epoch 73/1000, Train Loss: 0.1329, Val Loss: 0.1398, Accuracy: 94.93%
2025-02-15 16:07:26,647 - Epoch [74/1000], Step [100/4367], Loss: 0.0838
2025-02-15 16:08:01,455 - Epoch [74/1000], Step [200/4367], Loss: 0.0644
2025-02-15 16:08:36,248 - Epoch [74/1000], Step [300/4367], Loss: 0.2083
2025-02-15 16:09:10,894 - Epoch [74/1000], Step [400/4367], Loss: 0.0909
2025-02-15 16:09:45,880 - Epoch [74/1000], Step [500/4367], Loss: 0.1690
2025-02-15 16:10:20,465 - Epoch [74/1000], Step [600/4367], Loss: 0.0548
2025-02-15 16:10:55,152 - Epoch [74/1000], Step [700/4367], Loss: 0.1369
2025-02-15 16:11:29,716 - Epoch [74/1000], Step [800/4367], Loss: 0.1621
2025-02-15 16:12:04,632 - Epoch [74/1000], Step [900/4367], Loss: 0.2492
2025-02-15 16:12:39,587 - Epoch [74/1000], Step [1000/4367], Loss: 0.1545
2025-02-15 16:13:14,598 - Epoch [74/1000], Step [1100/4367], Loss: 0.1705
2025-02-15 16:13:49,427 - Epoch [74/1000], Step [1200/4367], Loss: 0.0721
2025-02-15 16:14:24,343 - Epoch [74/1000], Step [1300/4367], Loss: 0.0539
2025-02-15 16:14:58,848 - Epoch [74/1000], Step [1400/4367], Loss: 0.1180
2025-02-15 16:15:33,469 - Epoch [74/1000], Step [1500/4367], Loss: 0.0826
2025-02-15 16:16:08,042 - Epoch [74/1000], Step [1600/4367], Loss: 0.0761
2025-02-15 16:16:42,374 - Epoch [74/1000], Step [1700/4367], Loss: 0.1542
2025-02-15 16:17:17,331 - Epoch [74/1000], Step [1800/4367], Loss: 0.1232
2025-02-15 16:17:52,010 - Epoch [74/1000], Step [1900/4367], Loss: 0.1064
2025-02-15 16:18:26,612 - Epoch [74/1000], Step [2000/4367], Loss: 0.0666
2025-02-15 16:19:01,196 - Epoch [74/1000], Step [2100/4367], Loss: 0.0772
2025-02-15 16:19:35,975 - Epoch [74/1000], Step [2200/4367], Loss: 0.0671
2025-02-15 16:20:11,180 - Epoch [74/1000], Step [2300/4367], Loss: 0.1004
2025-02-15 16:20:45,638 - Epoch [74/1000], Step [2400/4367], Loss: 0.0653
2025-02-15 16:21:20,517 - Epoch [74/1000], Step [2500/4367], Loss: 0.1239
2025-02-15 16:21:55,087 - Epoch [74/1000], Step [2600/4367], Loss: 0.3167
2025-02-15 16:22:30,007 - Epoch [74/1000], Step [2700/4367], Loss: 0.2133
2025-02-15 16:23:04,744 - Epoch [74/1000], Step [2800/4367], Loss: 0.0325
2025-02-15 16:23:39,989 - Epoch [74/1000], Step [2900/4367], Loss: 0.2220
2025-02-15 16:24:14,714 - Epoch [74/1000], Step [3000/4367], Loss: 0.0436
2025-02-15 16:24:49,343 - Epoch [74/1000], Step [3100/4367], Loss: 0.0446
2025-02-15 16:25:24,254 - Epoch [74/1000], Step [3200/4367], Loss: 0.0349
2025-02-15 16:25:59,065 - Epoch [74/1000], Step [3300/4367], Loss: 0.1812
2025-02-15 16:26:33,569 - Epoch [74/1000], Step [3400/4367], Loss: 0.1906
2025-02-15 16:27:08,308 - Epoch [74/1000], Step [3500/4367], Loss: 0.1000
2025-02-15 16:27:42,517 - Epoch [74/1000], Step [3600/4367], Loss: 0.0432
2025-02-15 16:28:17,139 - Epoch [74/1000], Step [3700/4367], Loss: 0.2138
2025-02-15 16:28:52,181 - Epoch [74/1000], Step [3800/4367], Loss: 0.1781
2025-02-15 16:29:26,805 - Epoch [74/1000], Step [3900/4367], Loss: 0.0682
2025-02-15 16:30:01,370 - Epoch [74/1000], Step [4000/4367], Loss: 0.0955
2025-02-15 16:30:35,907 - Epoch [74/1000], Step [4100/4367], Loss: 0.1981
2025-02-15 16:31:10,027 - Epoch [74/1000], Step [4200/4367], Loss: 0.2455
2025-02-15 16:31:44,241 - Epoch [74/1000], Step [4300/4367], Loss: 0.1867
2025-02-15 16:32:17,676 - Epoch [74/1000], Validation Step [100/1090], Val Loss: 0.0001
2025-02-15 16:32:26,884 - Epoch [74/1000], Validation Step [200/1090], Val Loss: 0.0006
2025-02-15 16:32:36,241 - Epoch [74/1000], Validation Step [300/1090], Val Loss: 0.2370
2025-02-15 16:32:45,918 - Epoch [74/1000], Validation Step [400/1090], Val Loss: 0.0460
2025-02-15 16:32:55,042 - Epoch [74/1000], Validation Step [500/1090], Val Loss: 0.4178
2025-02-15 16:33:04,588 - Epoch [74/1000], Validation Step [600/1090], Val Loss: 0.1379
2025-02-15 16:33:14,169 - Epoch [74/1000], Validation Step [700/1090], Val Loss: 0.1459
2025-02-15 16:33:22,971 - Epoch [74/1000], Validation Step [800/1090], Val Loss: 0.0050
2025-02-15 16:33:31,544 - Epoch [74/1000], Validation Step [900/1090], Val Loss: 0.0042
2025-02-15 16:33:40,817 - Epoch [74/1000], Validation Step [1000/1090], Val Loss: 0.0000
2025-02-15 16:33:49,451 - Epoch 74/1000, Train Loss: 0.1318, Val Loss: 0.1383, Accuracy: 94.88%
2025-02-15 16:33:49,886 - Model saved at /data2/personal/sungjin/korean_dialects/checkpoint_real/checkpoint_epoch_74.pth
2025-02-15 16:34:25,977 - Epoch [75/1000], Step [100/4367], Loss: 0.0502
2025-02-15 16:35:00,708 - Epoch [75/1000], Step [200/4367], Loss: 0.1259
2025-02-15 16:35:35,953 - Epoch [75/1000], Step [300/4367], Loss: 0.0169
2025-02-15 16:36:11,106 - Epoch [75/1000], Step [400/4367], Loss: 0.1838
2025-02-15 16:36:45,442 - Epoch [75/1000], Step [500/4367], Loss: 0.2075
2025-02-15 16:37:20,257 - Epoch [75/1000], Step [600/4367], Loss: 0.2072
2025-02-15 16:37:54,619 - Epoch [75/1000], Step [700/4367], Loss: 0.1156
2025-02-15 16:38:29,502 - Epoch [75/1000], Step [800/4367], Loss: 0.1102
2025-02-15 16:39:04,125 - Epoch [75/1000], Step [900/4367], Loss: 0.0459
2025-02-15 16:39:38,805 - Epoch [75/1000], Step [1000/4367], Loss: 0.0432
2025-02-15 16:40:13,516 - Epoch [75/1000], Step [1100/4367], Loss: 0.0714
2025-02-15 16:40:47,575 - Epoch [75/1000], Step [1200/4367], Loss: 0.0431
2025-02-15 16:41:22,661 - Epoch [75/1000], Step [1300/4367], Loss: 0.0441
2025-02-15 16:41:57,284 - Epoch [75/1000], Step [1400/4367], Loss: 0.2449
2025-02-15 16:42:32,039 - Epoch [75/1000], Step [1500/4367], Loss: 0.1275
2025-02-15 16:43:06,427 - Epoch [75/1000], Step [1600/4367], Loss: 0.0399
2025-02-15 16:43:41,301 - Epoch [75/1000], Step [1700/4367], Loss: 0.1924
2025-02-15 16:44:15,819 - Epoch [75/1000], Step [1800/4367], Loss: 0.0252
2025-02-15 16:44:50,293 - Epoch [75/1000], Step [1900/4367], Loss: 0.0985
2025-02-15 16:45:25,089 - Epoch [75/1000], Step [2000/4367], Loss: 0.1359
2025-02-15 16:45:59,540 - Epoch [75/1000], Step [2100/4367], Loss: 0.1051
2025-02-15 16:46:34,224 - Epoch [75/1000], Step [2200/4367], Loss: 0.2223
2025-02-15 16:47:09,301 - Epoch [75/1000], Step [2300/4367], Loss: 0.0426
2025-02-15 16:47:43,720 - Epoch [75/1000], Step [2400/4367], Loss: 0.1670
2025-02-15 16:48:18,959 - Epoch [75/1000], Step [2500/4367], Loss: 0.0523
2025-02-15 16:48:53,884 - Epoch [75/1000], Step [2600/4367], Loss: 0.0861
2025-02-15 16:49:28,390 - Epoch [75/1000], Step [2700/4367], Loss: 0.0806
2025-02-15 16:50:03,142 - Epoch [75/1000], Step [2800/4367], Loss: 0.2638
2025-02-15 16:50:37,930 - Epoch [75/1000], Step [2900/4367], Loss: 0.0240
2025-02-15 16:51:12,654 - Epoch [75/1000], Step [3000/4367], Loss: 0.1374
2025-02-15 16:51:47,169 - Epoch [75/1000], Step [3100/4367], Loss: 0.0925
2025-02-15 16:52:21,517 - Epoch [75/1000], Step [3200/4367], Loss: 0.0507
2025-02-15 16:52:56,244 - Epoch [75/1000], Step [3300/4367], Loss: 0.1686
2025-02-15 16:53:31,020 - Epoch [75/1000], Step [3400/4367], Loss: 0.0918
2025-02-15 16:54:05,775 - Epoch [75/1000], Step [3500/4367], Loss: 0.1387
2025-02-15 16:54:40,257 - Epoch [75/1000], Step [3600/4367], Loss: 0.1992
2025-02-15 16:55:14,988 - Epoch [75/1000], Step [3700/4367], Loss: 0.1588
2025-02-15 16:55:50,073 - Epoch [75/1000], Step [3800/4367], Loss: 0.0796
2025-02-15 16:56:25,053 - Epoch [75/1000], Step [3900/4367], Loss: 0.2946
2025-02-15 16:56:59,768 - Epoch [75/1000], Step [4000/4367], Loss: 0.3486
2025-02-15 16:57:34,256 - Epoch [75/1000], Step [4100/4367], Loss: 0.0672
2025-02-15 16:58:09,099 - Epoch [75/1000], Step [4200/4367], Loss: 0.0160
2025-02-15 16:58:43,452 - Epoch [75/1000], Step [4300/4367], Loss: 0.0200
2025-02-15 16:59:16,063 - Epoch [75/1000], Validation Step [100/1090], Val Loss: 0.0001
2025-02-15 16:59:25,266 - Epoch [75/1000], Validation Step [200/1090], Val Loss: 0.0002
2025-02-15 16:59:34,617 - Epoch [75/1000], Validation Step [300/1090], Val Loss: 0.2723
2025-02-15 16:59:44,294 - Epoch [75/1000], Validation Step [400/1090], Val Loss: 0.0811
2025-02-15 16:59:53,398 - Epoch [75/1000], Validation Step [500/1090], Val Loss: 0.5389
2025-02-15 17:00:02,932 - Epoch [75/1000], Validation Step [600/1090], Val Loss: 0.1145
2025-02-15 17:00:12,501 - Epoch [75/1000], Validation Step [700/1090], Val Loss: 0.1244
2025-02-15 17:00:21,291 - Epoch [75/1000], Validation Step [800/1090], Val Loss: 0.0025
2025-02-15 17:00:29,856 - Epoch [75/1000], Validation Step [900/1090], Val Loss: 0.0018
2025-02-15 17:00:39,106 - Epoch [75/1000], Validation Step [1000/1090], Val Loss: 0.0000
2025-02-15 17:00:47,733 - Epoch 75/1000, Train Loss: 0.1327, Val Loss: 0.1437, Accuracy: 94.79%
2025-02-15 17:01:23,436 - Epoch [76/1000], Step [100/4367], Loss: 0.1692
2025-02-15 17:01:58,244 - Epoch [76/1000], Step [200/4367], Loss: 0.0912
2025-02-15 17:02:32,843 - Epoch [76/1000], Step [300/4367], Loss: 0.0489
2025-02-15 17:03:07,750 - Epoch [76/1000], Step [400/4367], Loss: 0.0678
2025-02-15 17:03:42,158 - Epoch [76/1000], Step [500/4367], Loss: 0.0919
2025-02-15 17:04:16,311 - Epoch [76/1000], Step [600/4367], Loss: 0.0988
2025-02-15 17:04:51,068 - Epoch [76/1000], Step [700/4367], Loss: 0.2263
2025-02-15 17:05:25,642 - Epoch [76/1000], Step [800/4367], Loss: 0.1266
2025-02-15 17:06:00,440 - Epoch [76/1000], Step [900/4367], Loss: 0.0836
2025-02-15 17:06:35,391 - Epoch [76/1000], Step [1000/4367], Loss: 0.1350
2025-02-15 17:07:09,753 - Epoch [76/1000], Step [1100/4367], Loss: 0.1437
2025-02-15 17:07:44,675 - Epoch [76/1000], Step [1200/4367], Loss: 0.1456
2025-02-15 17:08:19,289 - Epoch [76/1000], Step [1300/4367], Loss: 0.1138
2025-02-15 17:08:53,809 - Epoch [76/1000], Step [1400/4367], Loss: 0.2385
2025-02-15 17:09:28,567 - Epoch [76/1000], Step [1500/4367], Loss: 0.1642
2025-02-15 17:10:03,390 - Epoch [76/1000], Step [1600/4367], Loss: 0.1302
2025-02-15 17:10:37,637 - Epoch [76/1000], Step [1700/4367], Loss: 0.1759
2025-02-15 17:11:12,459 - Epoch [76/1000], Step [1800/4367], Loss: 0.0452
2025-02-15 17:11:47,231 - Epoch [76/1000], Step [1900/4367], Loss: 0.0401
2025-02-15 17:12:22,277 - Epoch [76/1000], Step [2000/4367], Loss: 0.0623
2025-02-15 17:12:56,785 - Epoch [76/1000], Step [2100/4367], Loss: 0.0750
2025-02-15 17:13:31,521 - Epoch [76/1000], Step [2200/4367], Loss: 0.0153
2025-02-15 17:14:06,054 - Epoch [76/1000], Step [2300/4367], Loss: 0.3518
2025-02-15 17:14:40,904 - Epoch [76/1000], Step [2400/4367], Loss: 0.1459
2025-02-15 17:15:15,699 - Epoch [76/1000], Step [2500/4367], Loss: 0.1161
2025-02-15 17:15:50,427 - Epoch [76/1000], Step [2600/4367], Loss: 0.1461
2025-02-15 17:16:24,385 - Epoch [76/1000], Step [2700/4367], Loss: 0.0571
2025-02-15 17:16:59,296 - Epoch [76/1000], Step [2800/4367], Loss: 0.2334
2025-02-15 17:17:34,340 - Epoch [76/1000], Step [2900/4367], Loss: 0.1310
2025-02-15 17:18:09,557 - Epoch [76/1000], Step [3000/4367], Loss: 0.2189
2025-02-15 17:18:44,215 - Epoch [76/1000], Step [3100/4367], Loss: 0.1874
2025-02-15 17:19:19,221 - Epoch [76/1000], Step [3200/4367], Loss: 0.1462
2025-02-15 17:19:53,947 - Epoch [76/1000], Step [3300/4367], Loss: 0.1058
2025-02-15 17:20:28,838 - Epoch [76/1000], Step [3400/4367], Loss: 0.0531
2025-02-15 17:21:03,232 - Epoch [76/1000], Step [3500/4367], Loss: 0.1477
2025-02-15 17:21:37,576 - Epoch [76/1000], Step [3600/4367], Loss: 0.0325
2025-02-15 17:22:12,028 - Epoch [76/1000], Step [3700/4367], Loss: 0.1963
2025-02-15 17:22:46,757 - Epoch [76/1000], Step [3800/4367], Loss: 0.1036
2025-02-15 17:23:21,182 - Epoch [76/1000], Step [3900/4367], Loss: 0.0422
2025-02-15 17:23:56,173 - Epoch [76/1000], Step [4000/4367], Loss: 0.3137
2025-02-15 17:24:30,972 - Epoch [76/1000], Step [4100/4367], Loss: 0.0417
2025-02-15 17:25:05,435 - Epoch [76/1000], Step [4200/4367], Loss: 0.1467
2025-02-15 17:25:40,220 - Epoch [76/1000], Step [4300/4367], Loss: 0.1509
2025-02-15 17:26:13,430 - Epoch [76/1000], Validation Step [100/1090], Val Loss: 0.0001
2025-02-15 17:26:22,629 - Epoch [76/1000], Validation Step [200/1090], Val Loss: 0.0003
2025-02-15 17:26:31,978 - Epoch [76/1000], Validation Step [300/1090], Val Loss: 0.2596
2025-02-15 17:26:41,655 - Epoch [76/1000], Validation Step [400/1090], Val Loss: 0.0603
2025-02-15 17:26:50,766 - Epoch [76/1000], Validation Step [500/1090], Val Loss: 0.4340
2025-02-15 17:27:00,304 - Epoch [76/1000], Validation Step [600/1090], Val Loss: 0.1443
2025-02-15 17:27:09,879 - Epoch [76/1000], Validation Step [700/1090], Val Loss: 0.1497
2025-02-15 17:27:18,668 - Epoch [76/1000], Validation Step [800/1090], Val Loss: 0.0035
2025-02-15 17:27:27,236 - Epoch [76/1000], Validation Step [900/1090], Val Loss: 0.0032
2025-02-15 17:27:36,509 - Epoch [76/1000], Validation Step [1000/1090], Val Loss: 0.0000
2025-02-15 17:27:45,149 - Epoch 76/1000, Train Loss: 0.1321, Val Loss: 0.1418, Accuracy: 94.87%
2025-02-15 17:27:45,564 - Model saved at /data2/personal/sungjin/korean_dialects/checkpoint_real/checkpoint_epoch_76.pth
2025-02-15 17:28:21,330 - Epoch [77/1000], Step [100/4367], Loss: 0.2147
2025-02-15 17:28:56,285 - Epoch [77/1000], Step [200/4367], Loss: 0.2046
2025-02-15 17:29:31,103 - Epoch [77/1000], Step [300/4367], Loss: 0.2185
2025-02-15 17:30:05,949 - Epoch [77/1000], Step [400/4367], Loss: 0.1220
2025-02-15 17:30:40,567 - Epoch [77/1000], Step [500/4367], Loss: 0.1227
2025-02-15 17:31:15,148 - Epoch [77/1000], Step [600/4367], Loss: 0.1359
2025-02-15 17:31:49,812 - Epoch [77/1000], Step [700/4367], Loss: 0.0665
2025-02-15 17:32:24,407 - Epoch [77/1000], Step [800/4367], Loss: 0.0817
2025-02-15 17:32:58,927 - Epoch [77/1000], Step [900/4367], Loss: 0.3883
2025-02-15 17:33:33,672 - Epoch [77/1000], Step [1000/4367], Loss: 0.2078
2025-02-15 17:34:08,483 - Epoch [77/1000], Step [1100/4367], Loss: 0.1294
2025-02-15 17:34:42,730 - Epoch [77/1000], Step [1200/4367], Loss: 0.1610
2025-02-15 17:35:17,077 - Epoch [77/1000], Step [1300/4367], Loss: 0.0439
2025-02-15 17:35:51,539 - Epoch [77/1000], Step [1400/4367], Loss: 0.0505
2025-02-15 17:36:26,829 - Epoch [77/1000], Step [1500/4367], Loss: 0.1732
2025-02-15 17:37:01,360 - Epoch [77/1000], Step [1600/4367], Loss: 0.1639
2025-02-15 17:37:35,850 - Epoch [77/1000], Step [1700/4367], Loss: 0.1012
2025-02-15 17:38:10,752 - Epoch [77/1000], Step [1800/4367], Loss: 0.1174
2025-02-15 17:38:45,461 - Epoch [77/1000], Step [1900/4367], Loss: 0.2430
2025-02-15 17:39:20,174 - Epoch [77/1000], Step [2000/4367], Loss: 0.0377
2025-02-15 17:39:54,979 - Epoch [77/1000], Step [2100/4367], Loss: 0.1031
2025-02-15 17:40:29,542 - Epoch [77/1000], Step [2200/4367], Loss: 0.1068
2025-02-15 17:41:03,952 - Epoch [77/1000], Step [2300/4367], Loss: 0.0248
2025-02-15 17:41:38,922 - Epoch [77/1000], Step [2400/4367], Loss: 0.0611
2025-02-15 17:42:13,737 - Epoch [77/1000], Step [2500/4367], Loss: 0.1367
2025-02-15 17:42:48,494 - Epoch [77/1000], Step [2600/4367], Loss: 0.1453
2025-02-15 17:43:23,355 - Epoch [77/1000], Step [2700/4367], Loss: 0.1795
2025-02-15 17:43:57,863 - Epoch [77/1000], Step [2800/4367], Loss: 0.0522
2025-02-15 17:44:32,709 - Epoch [77/1000], Step [2900/4367], Loss: 0.1134
2025-02-15 17:45:07,615 - Epoch [77/1000], Step [3000/4367], Loss: 0.2944
2025-02-15 17:45:42,565 - Epoch [77/1000], Step [3100/4367], Loss: 0.0877
2025-02-15 17:46:16,993 - Epoch [77/1000], Step [3200/4367], Loss: 0.0492
2025-02-15 17:46:52,075 - Epoch [77/1000], Step [3300/4367], Loss: 0.1892
2025-02-15 17:47:26,870 - Epoch [77/1000], Step [3400/4367], Loss: 0.2220
2025-02-15 17:48:01,816 - Epoch [77/1000], Step [3500/4367], Loss: 0.1857
2025-02-15 17:48:35,875 - Epoch [77/1000], Step [3600/4367], Loss: 0.0891
2025-02-15 17:49:10,649 - Epoch [77/1000], Step [3700/4367], Loss: 0.1009
2025-02-15 17:49:45,624 - Epoch [77/1000], Step [3800/4367], Loss: 0.0507
2025-02-15 17:50:20,238 - Epoch [77/1000], Step [3900/4367], Loss: 0.2146
2025-02-15 17:50:54,659 - Epoch [77/1000], Step [4000/4367], Loss: 0.2035
2025-02-15 17:51:29,258 - Epoch [77/1000], Step [4100/4367], Loss: 0.0746
2025-02-15 17:52:04,239 - Epoch [77/1000], Step [4200/4367], Loss: 0.0539
2025-02-15 17:52:39,038 - Epoch [77/1000], Step [4300/4367], Loss: 0.1373
2025-02-15 17:53:12,017 - Epoch [77/1000], Validation Step [100/1090], Val Loss: 0.0001
2025-02-15 17:53:21,225 - Epoch [77/1000], Validation Step [200/1090], Val Loss: 0.0004
2025-02-15 17:53:30,576 - Epoch [77/1000], Validation Step [300/1090], Val Loss: 0.2346
2025-02-15 17:53:40,240 - Epoch [77/1000], Validation Step [400/1090], Val Loss: 0.0473
2025-02-15 17:53:49,353 - Epoch [77/1000], Validation Step [500/1090], Val Loss: 0.3987
2025-02-15 17:53:58,891 - Epoch [77/1000], Validation Step [600/1090], Val Loss: 0.1430
2025-02-15 17:54:08,464 - Epoch [77/1000], Validation Step [700/1090], Val Loss: 0.1428
2025-02-15 17:54:17,260 - Epoch [77/1000], Validation Step [800/1090], Val Loss: 0.0056
2025-02-15 17:54:25,830 - Epoch [77/1000], Validation Step [900/1090], Val Loss: 0.0049
2025-02-15 17:54:35,094 - Epoch [77/1000], Validation Step [1000/1090], Val Loss: 0.0000
2025-02-15 17:54:43,717 - Epoch 77/1000, Train Loss: 0.1320, Val Loss: 0.1383, Accuracy: 94.87%
2025-02-15 17:55:19,744 - Epoch [78/1000], Step [100/4367], Loss: 0.2832
2025-02-15 17:55:54,602 - Epoch [78/1000], Step [200/4367], Loss: 0.1704
2025-02-15 17:56:29,265 - Epoch [78/1000], Step [300/4367], Loss: 0.1379
2025-02-15 17:57:04,163 - Epoch [78/1000], Step [400/4367], Loss: 0.0251
2025-02-15 17:57:38,918 - Epoch [78/1000], Step [500/4367], Loss: 0.2856
2025-02-15 17:58:13,956 - Epoch [78/1000], Step [600/4367], Loss: 0.0201
2025-02-15 17:58:48,858 - Epoch [78/1000], Step [700/4367], Loss: 0.1733
2025-02-15 17:59:23,239 - Epoch [78/1000], Step [800/4367], Loss: 0.0585
2025-02-15 17:59:57,858 - Epoch [78/1000], Step [900/4367], Loss: 0.1956
2025-02-15 18:00:32,483 - Epoch [78/1000], Step [1000/4367], Loss: 0.1717
2025-02-15 18:01:07,503 - Epoch [78/1000], Step [1100/4367], Loss: 0.2253
2025-02-15 18:01:42,234 - Epoch [78/1000], Step [1200/4367], Loss: 0.0185
2025-02-15 18:02:16,745 - Epoch [78/1000], Step [1300/4367], Loss: 0.2252
2025-02-15 18:02:51,095 - Epoch [78/1000], Step [1400/4367], Loss: 0.0832
2025-02-15 18:03:26,155 - Epoch [78/1000], Step [1500/4367], Loss: 0.0664
2025-02-15 18:04:01,165 - Epoch [78/1000], Step [1600/4367], Loss: 0.0653
2025-02-15 18:04:35,701 - Epoch [78/1000], Step [1700/4367], Loss: 0.1576
2025-02-15 18:05:10,173 - Epoch [78/1000], Step [1800/4367], Loss: 0.0620
2025-02-15 18:05:44,860 - Epoch [78/1000], Step [1900/4367], Loss: 0.0333
2025-02-15 18:06:19,598 - Epoch [78/1000], Step [2000/4367], Loss: 0.0703
2025-02-15 18:06:53,970 - Epoch [78/1000], Step [2100/4367], Loss: 0.0999
2025-02-15 18:07:28,838 - Epoch [78/1000], Step [2200/4367], Loss: 0.1740
2025-02-15 18:08:03,715 - Epoch [78/1000], Step [2300/4367], Loss: 0.1696
2025-02-15 18:08:38,740 - Epoch [78/1000], Step [2400/4367], Loss: 0.1643
2025-02-15 18:09:13,186 - Epoch [78/1000], Step [2500/4367], Loss: 0.0979
2025-02-15 18:09:48,098 - Epoch [78/1000], Step [2600/4367], Loss: 0.0823
2025-02-15 18:10:23,302 - Epoch [78/1000], Step [2700/4367], Loss: 0.2136
2025-02-15 18:10:57,652 - Epoch [78/1000], Step [2800/4367], Loss: 0.1603
2025-02-15 18:11:32,912 - Epoch [78/1000], Step [2900/4367], Loss: 0.1115
2025-02-15 18:12:07,998 - Epoch [78/1000], Step [3000/4367], Loss: 0.1272
2025-02-15 18:12:42,931 - Epoch [78/1000], Step [3100/4367], Loss: 0.0613
2025-02-15 18:13:18,120 - Epoch [78/1000], Step [3200/4367], Loss: 0.0654
2025-02-15 18:13:52,673 - Epoch [78/1000], Step [3300/4367], Loss: 0.1684
2025-02-15 18:14:27,242 - Epoch [78/1000], Step [3400/4367], Loss: 0.1993
2025-02-15 18:15:02,267 - Epoch [78/1000], Step [3500/4367], Loss: 0.2014
2025-02-15 18:15:36,918 - Epoch [78/1000], Step [3600/4367], Loss: 0.1635
2025-02-15 18:16:11,651 - Epoch [78/1000], Step [3700/4367], Loss: 0.1053
2025-02-15 18:16:46,418 - Epoch [78/1000], Step [3800/4367], Loss: 0.0538
2025-02-15 18:17:21,662 - Epoch [78/1000], Step [3900/4367], Loss: 0.2416
2025-02-15 18:17:56,525 - Epoch [78/1000], Step [4000/4367], Loss: 0.0760
2025-02-15 18:18:31,031 - Epoch [78/1000], Step [4100/4367], Loss: 0.0279
2025-02-15 18:19:05,629 - Epoch [78/1000], Step [4200/4367], Loss: 0.1502
2025-02-15 18:19:40,385 - Epoch [78/1000], Step [4300/4367], Loss: 0.1186
2025-02-15 18:20:13,215 - Epoch [78/1000], Validation Step [100/1090], Val Loss: 0.0001
2025-02-15 18:20:22,417 - Epoch [78/1000], Validation Step [200/1090], Val Loss: 0.0003
2025-02-15 18:20:31,752 - Epoch [78/1000], Validation Step [300/1090], Val Loss: 0.2753
2025-02-15 18:20:41,423 - Epoch [78/1000], Validation Step [400/1090], Val Loss: 0.0523
2025-02-15 18:20:50,532 - Epoch [78/1000], Validation Step [500/1090], Val Loss: 0.4443
2025-02-15 18:21:00,064 - Epoch [78/1000], Validation Step [600/1090], Val Loss: 0.0957
2025-02-15 18:21:09,630 - Epoch [78/1000], Validation Step [700/1090], Val Loss: 0.1031
2025-02-15 18:21:18,421 - Epoch [78/1000], Validation Step [800/1090], Val Loss: 0.0035
2025-02-15 18:21:26,982 - Epoch [78/1000], Validation Step [900/1090], Val Loss: 0.0035
2025-02-15 18:21:36,238 - Epoch [78/1000], Validation Step [1000/1090], Val Loss: 0.0000
2025-02-15 18:21:44,869 - Epoch 78/1000, Train Loss: 0.1317, Val Loss: 0.1414, Accuracy: 94.84%
2025-02-15 18:21:45,265 - Model saved at /data2/personal/sungjin/korean_dialects/checkpoint_real/checkpoint_epoch_78.pth
2025-02-15 18:22:21,214 - Epoch [79/1000], Step [100/4367], Loss: 0.2052
2025-02-15 18:22:56,035 - Epoch [79/1000], Step [200/4367], Loss: 0.0670
2025-02-15 18:23:30,572 - Epoch [79/1000], Step [300/4367], Loss: 0.1280
2025-02-15 18:24:05,859 - Epoch [79/1000], Step [400/4367], Loss: 0.1999
2025-02-15 18:24:40,375 - Epoch [79/1000], Step [500/4367], Loss: 0.1363
2025-02-15 18:25:15,372 - Epoch [79/1000], Step [600/4367], Loss: 0.1756
2025-02-15 18:25:49,935 - Epoch [79/1000], Step [700/4367], Loss: 0.1970
2025-02-15 18:26:24,598 - Epoch [79/1000], Step [800/4367], Loss: 0.1010
2025-02-15 18:26:59,414 - Epoch [79/1000], Step [900/4367], Loss: 0.4183
2025-02-15 18:27:34,362 - Epoch [79/1000], Step [1000/4367], Loss: 0.0196
2025-02-15 18:28:08,895 - Epoch [79/1000], Step [1100/4367], Loss: 0.2145
2025-02-15 18:28:43,750 - Epoch [79/1000], Step [1200/4367], Loss: 0.0607
2025-02-15 18:29:17,856 - Epoch [79/1000], Step [1300/4367], Loss: 0.1468
2025-02-15 18:29:52,526 - Epoch [79/1000], Step [1400/4367], Loss: 0.0611
2025-02-15 18:30:27,790 - Epoch [79/1000], Step [1500/4367], Loss: 0.1925
2025-02-15 18:31:02,655 - Epoch [79/1000], Step [1600/4367], Loss: 0.0715
2025-02-15 18:31:37,863 - Epoch [79/1000], Step [1700/4367], Loss: 0.1786
2025-02-15 18:32:12,258 - Epoch [79/1000], Step [1800/4367], Loss: 0.2023
2025-02-15 18:32:46,842 - Epoch [79/1000], Step [1900/4367], Loss: 0.1753
2025-02-15 18:33:21,331 - Epoch [79/1000], Step [2000/4367], Loss: 0.0803
2025-02-15 18:33:55,458 - Epoch [79/1000], Step [2100/4367], Loss: 0.1699
2025-02-15 18:34:29,861 - Epoch [79/1000], Step [2200/4367], Loss: 0.0193
2025-02-15 18:35:04,057 - Epoch [79/1000], Step [2300/4367], Loss: 0.1345
2025-02-15 18:35:38,558 - Epoch [79/1000], Step [2400/4367], Loss: 0.0753
2025-02-15 18:36:12,787 - Epoch [79/1000], Step [2500/4367], Loss: 0.0971
2025-02-15 18:36:47,389 - Epoch [79/1000], Step [2600/4367], Loss: 0.2062
2025-02-15 18:37:22,207 - Epoch [79/1000], Step [2700/4367], Loss: 0.1845
2025-02-15 18:37:56,605 - Epoch [79/1000], Step [2800/4367], Loss: 0.3815
2025-02-15 18:38:31,667 - Epoch [79/1000], Step [2900/4367], Loss: 0.1231
2025-02-15 18:39:06,481 - Epoch [79/1000], Step [3000/4367], Loss: 0.1582
2025-02-15 18:39:41,207 - Epoch [79/1000], Step [3100/4367], Loss: 0.1275
2025-02-15 18:40:15,964 - Epoch [79/1000], Step [3200/4367], Loss: 0.0689
2025-02-15 18:40:50,782 - Epoch [79/1000], Step [3300/4367], Loss: 0.0407
2025-02-15 18:41:25,845 - Epoch [79/1000], Step [3400/4367], Loss: 0.0758
2025-02-15 18:42:00,282 - Epoch [79/1000], Step [3500/4367], Loss: 0.2014
2025-02-15 18:42:34,862 - Epoch [79/1000], Step [3600/4367], Loss: 0.0528
2025-02-15 18:43:09,259 - Epoch [79/1000], Step [3700/4367], Loss: 0.0703
2025-02-15 18:43:44,176 - Epoch [79/1000], Step [3800/4367], Loss: 0.1289
2025-02-15 18:44:19,373 - Epoch [79/1000], Step [3900/4367], Loss: 0.0356
2025-02-15 18:44:54,094 - Epoch [79/1000], Step [4000/4367], Loss: 0.2862
2025-02-15 18:45:29,352 - Epoch [79/1000], Step [4100/4367], Loss: 0.1321
2025-02-15 18:46:03,752 - Epoch [79/1000], Step [4200/4367], Loss: 0.0870
2025-02-15 18:46:38,608 - Epoch [79/1000], Step [4300/4367], Loss: 0.2133
2025-02-15 18:47:11,947 - Epoch [79/1000], Validation Step [100/1090], Val Loss: 0.0002
2025-02-15 18:47:21,148 - Epoch [79/1000], Validation Step [200/1090], Val Loss: 0.0004
2025-02-15 18:47:30,491 - Epoch [79/1000], Validation Step [300/1090], Val Loss: 0.2387
2025-02-15 18:47:40,153 - Epoch [79/1000], Validation Step [400/1090], Val Loss: 0.0544
2025-02-15 18:47:49,256 - Epoch [79/1000], Validation Step [500/1090], Val Loss: 0.4293
2025-02-15 18:47:58,786 - Epoch [79/1000], Validation Step [600/1090], Val Loss: 0.1193
2025-02-15 18:48:08,350 - Epoch [79/1000], Validation Step [700/1090], Val Loss: 0.1109
2025-02-15 18:48:17,136 - Epoch [79/1000], Validation Step [800/1090], Val Loss: 0.0049
2025-02-15 18:48:25,696 - Epoch [79/1000], Validation Step [900/1090], Val Loss: 0.0041
2025-02-15 18:48:34,956 - Epoch [79/1000], Validation Step [1000/1090], Val Loss: 0.0000
2025-02-15 18:48:43,573 - Epoch 79/1000, Train Loss: 0.1321, Val Loss: 0.1400, Accuracy: 94.83%
2025-02-15 18:49:19,679 - Epoch [80/1000], Step [100/4367], Loss: 0.1435
2025-02-15 18:49:54,603 - Epoch [80/1000], Step [200/4367], Loss: 0.1561
2025-02-15 18:50:29,260 - Epoch [80/1000], Step [300/4367], Loss: 0.0769
2025-02-15 18:51:04,121 - Epoch [80/1000], Step [400/4367], Loss: 0.1912
2025-02-15 18:51:38,653 - Epoch [80/1000], Step [500/4367], Loss: 0.2798
2025-02-15 18:52:13,314 - Epoch [80/1000], Step [600/4367], Loss: 0.0533
2025-02-15 18:52:47,871 - Epoch [80/1000], Step [700/4367], Loss: 0.0921
2025-02-15 18:53:22,575 - Epoch [80/1000], Step [800/4367], Loss: 0.1398
2025-02-15 18:53:56,995 - Epoch [80/1000], Step [900/4367], Loss: 0.0459
2025-02-15 18:54:32,018 - Epoch [80/1000], Step [1000/4367], Loss: 0.0710
2025-02-15 18:55:06,436 - Epoch [80/1000], Step [1100/4367], Loss: 0.0354
2025-02-15 18:55:41,142 - Epoch [80/1000], Step [1200/4367], Loss: 0.0795
2025-02-15 18:56:15,922 - Epoch [80/1000], Step [1300/4367], Loss: 0.0680
2025-02-15 18:56:50,893 - Epoch [80/1000], Step [1400/4367], Loss: 0.1688
2025-02-15 18:57:25,288 - Epoch [80/1000], Step [1500/4367], Loss: 0.0333
2025-02-15 18:58:00,390 - Epoch [80/1000], Step [1600/4367], Loss: 0.1185
2025-02-15 18:58:35,396 - Epoch [80/1000], Step [1700/4367], Loss: 0.0805
2025-02-15 18:59:09,780 - Epoch [80/1000], Step [1800/4367], Loss: 0.1515
2025-02-15 18:59:44,700 - Epoch [80/1000], Step [1900/4367], Loss: 0.0664
2025-02-15 19:00:19,594 - Epoch [80/1000], Step [2000/4367], Loss: 0.0557
2025-02-15 19:00:54,362 - Epoch [80/1000], Step [2100/4367], Loss: 0.0657
2025-02-15 19:01:28,899 - Epoch [80/1000], Step [2200/4367], Loss: 0.3695
2025-02-15 19:02:03,392 - Epoch [80/1000], Step [2300/4367], Loss: 0.1110
2025-02-15 19:02:38,092 - Epoch [80/1000], Step [2400/4367], Loss: 0.2126
2025-02-15 19:03:13,169 - Epoch [80/1000], Step [2500/4367], Loss: 0.2406
2025-02-15 19:03:48,394 - Epoch [80/1000], Step [2600/4367], Loss: 0.1450
2025-02-15 19:04:23,128 - Epoch [80/1000], Step [2700/4367], Loss: 0.2020
2025-02-15 19:04:57,909 - Epoch [80/1000], Step [2800/4367], Loss: 0.1889
2025-02-15 19:05:32,180 - Epoch [80/1000], Step [2900/4367], Loss: 0.2429
2025-02-15 19:06:07,206 - Epoch [80/1000], Step [3000/4367], Loss: 0.0658
2025-02-15 19:06:41,991 - Epoch [80/1000], Step [3100/4367], Loss: 0.0981
2025-02-15 19:07:16,656 - Epoch [80/1000], Step [3200/4367], Loss: 0.1534
2025-02-15 19:07:51,423 - Epoch [80/1000], Step [3300/4367], Loss: 0.0521
2025-02-15 19:08:25,786 - Epoch [80/1000], Step [3400/4367], Loss: 0.0495
2025-02-15 19:09:00,223 - Epoch [80/1000], Step [3500/4367], Loss: 0.0853
2025-02-15 19:09:34,767 - Epoch [80/1000], Step [3600/4367], Loss: 0.0465
2025-02-15 19:10:09,562 - Epoch [80/1000], Step [3700/4367], Loss: 0.0816
2025-02-15 19:10:44,600 - Epoch [80/1000], Step [3800/4367], Loss: 0.2963
2025-02-15 19:11:19,186 - Epoch [80/1000], Step [3900/4367], Loss: 0.2126
2025-02-15 19:11:53,712 - Epoch [80/1000], Step [4000/4367], Loss: 0.0973
2025-02-15 19:12:28,573 - Epoch [80/1000], Step [4100/4367], Loss: 0.0728
2025-02-15 19:13:03,141 - Epoch [80/1000], Step [4200/4367], Loss: 0.0344
2025-02-15 19:13:37,960 - Epoch [80/1000], Step [4300/4367], Loss: 0.2364
2025-02-15 19:14:10,833 - Epoch [80/1000], Validation Step [100/1090], Val Loss: 0.0004
2025-02-15 19:14:20,055 - Epoch [80/1000], Validation Step [200/1090], Val Loss: 0.0010
2025-02-15 19:14:29,411 - Epoch [80/1000], Validation Step [300/1090], Val Loss: 0.2515
2025-02-15 19:14:39,085 - Epoch [80/1000], Validation Step [400/1090], Val Loss: 0.0392
2025-02-15 19:14:48,211 - Epoch [80/1000], Validation Step [500/1090], Val Loss: 0.4000
2025-02-15 19:14:57,755 - Epoch [80/1000], Validation Step [600/1090], Val Loss: 0.1285
2025-02-15 19:15:07,333 - Epoch [80/1000], Validation Step [700/1090], Val Loss: 0.1229
2025-02-15 19:15:16,126 - Epoch [80/1000], Validation Step [800/1090], Val Loss: 0.0044
2025-02-15 19:15:24,699 - Epoch [80/1000], Validation Step [900/1090], Val Loss: 0.0041
2025-02-15 19:15:33,964 - Epoch [80/1000], Validation Step [1000/1090], Val Loss: 0.0001
2025-02-15 19:15:42,596 - Epoch 80/1000, Train Loss: 0.1325, Val Loss: 0.1400, Accuracy: 94.89%
2025-02-15 19:15:43,026 - Model saved at /data2/personal/sungjin/korean_dialects/checkpoint_real/checkpoint_epoch_80.pth
2025-02-15 19:16:18,468 - Epoch [81/1000], Step [100/4367], Loss: 0.1872
2025-02-15 19:16:53,151 - Epoch [81/1000], Step [200/4367], Loss: 0.0375
2025-02-15 19:17:28,031 - Epoch [81/1000], Step [300/4367], Loss: 0.0532
2025-02-15 19:18:02,956 - Epoch [81/1000], Step [400/4367], Loss: 0.1355
2025-02-15 19:18:37,644 - Epoch [81/1000], Step [500/4367], Loss: 0.2531
2025-02-15 19:19:12,403 - Epoch [81/1000], Step [600/4367], Loss: 0.0923
2025-02-15 19:19:47,071 - Epoch [81/1000], Step [700/4367], Loss: 0.0395
2025-02-15 19:20:21,528 - Epoch [81/1000], Step [800/4367], Loss: 0.2851
2025-02-15 19:20:56,725 - Epoch [81/1000], Step [900/4367], Loss: 0.0215
2025-02-15 19:21:31,090 - Epoch [81/1000], Step [1000/4367], Loss: 0.3008
2025-02-15 19:22:05,944 - Epoch [81/1000], Step [1100/4367], Loss: 0.1100
2025-02-15 19:22:40,752 - Epoch [81/1000], Step [1200/4367], Loss: 0.0992
2025-02-15 19:23:15,468 - Epoch [81/1000], Step [1300/4367], Loss: 0.2516
2025-02-15 19:23:50,319 - Epoch [81/1000], Step [1400/4367], Loss: 0.0486
2025-02-15 19:24:24,847 - Epoch [81/1000], Step [1500/4367], Loss: 0.0440
2025-02-15 19:24:59,556 - Epoch [81/1000], Step [1600/4367], Loss: 0.1463
2025-02-15 19:25:34,347 - Epoch [81/1000], Step [1700/4367], Loss: 0.0940
2025-02-15 19:26:08,886 - Epoch [81/1000], Step [1800/4367], Loss: 0.1408
2025-02-15 19:26:43,569 - Epoch [81/1000], Step [1900/4367], Loss: 0.1115
2025-02-15 19:27:18,474 - Epoch [81/1000], Step [2000/4367], Loss: 0.2336
2025-02-15 19:27:53,337 - Epoch [81/1000], Step [2100/4367], Loss: 0.2569
2025-02-15 19:28:28,125 - Epoch [81/1000], Step [2200/4367], Loss: 0.1036
2025-02-15 19:29:02,672 - Epoch [81/1000], Step [2300/4367], Loss: 0.1671
2025-02-15 19:29:37,117 - Epoch [81/1000], Step [2400/4367], Loss: 0.0810
2025-02-15 19:30:12,145 - Epoch [81/1000], Step [2500/4367], Loss: 0.4479
2025-02-15 19:30:46,330 - Epoch [81/1000], Step [2600/4367], Loss: 0.3460
2025-02-15 19:31:21,128 - Epoch [81/1000], Step [2700/4367], Loss: 0.1648
2025-02-15 19:31:55,977 - Epoch [81/1000], Step [2800/4367], Loss: 0.0418
2025-02-15 19:32:31,020 - Epoch [81/1000], Step [2900/4367], Loss: 0.1361
2025-02-15 19:33:05,735 - Epoch [81/1000], Step [3000/4367], Loss: 0.0536
2025-02-15 19:33:40,363 - Epoch [81/1000], Step [3100/4367], Loss: 0.1352
2025-02-15 19:34:14,901 - Epoch [81/1000], Step [3200/4367], Loss: 0.3055
2025-02-15 19:34:49,361 - Epoch [81/1000], Step [3300/4367], Loss: 0.0137
2025-02-15 19:35:23,961 - Epoch [81/1000], Step [3400/4367], Loss: 0.0719
2025-02-15 19:35:58,334 - Epoch [81/1000], Step [3500/4367], Loss: 0.1422
2025-02-15 19:36:33,407 - Epoch [81/1000], Step [3600/4367], Loss: 0.2443
2025-02-15 19:37:08,625 - Epoch [81/1000], Step [3700/4367], Loss: 0.1366
2025-02-15 19:37:43,502 - Epoch [81/1000], Step [3800/4367], Loss: 0.0939
2025-02-15 19:38:18,452 - Epoch [81/1000], Step [3900/4367], Loss: 0.2415
2025-02-15 19:38:52,914 - Epoch [81/1000], Step [4000/4367], Loss: 0.5087
2025-02-15 19:39:27,565 - Epoch [81/1000], Step [4100/4367], Loss: 0.1353
2025-02-15 19:40:02,414 - Epoch [81/1000], Step [4200/4367], Loss: 0.0952
2025-02-15 19:40:37,168 - Epoch [81/1000], Step [4300/4367], Loss: 0.0941
2025-02-15 19:41:10,374 - Epoch [81/1000], Validation Step [100/1090], Val Loss: 0.0002
2025-02-15 19:41:19,567 - Epoch [81/1000], Validation Step [200/1090], Val Loss: 0.0005
2025-02-15 19:41:28,925 - Epoch [81/1000], Validation Step [300/1090], Val Loss: 0.2563
2025-02-15 19:41:38,596 - Epoch [81/1000], Validation Step [400/1090], Val Loss: 0.0398
2025-02-15 19:41:47,711 - Epoch [81/1000], Validation Step [500/1090], Val Loss: 0.3670
2025-02-15 19:41:57,249 - Epoch [81/1000], Validation Step [600/1090], Val Loss: 0.1189
2025-02-15 19:42:06,826 - Epoch [81/1000], Validation Step [700/1090], Val Loss: 0.1136
2025-02-15 19:42:15,618 - Epoch [81/1000], Validation Step [800/1090], Val Loss: 0.0081
2025-02-15 19:42:24,189 - Epoch [81/1000], Validation Step [900/1090], Val Loss: 0.0072
2025-02-15 19:42:33,449 - Epoch [81/1000], Validation Step [1000/1090], Val Loss: 0.0001
2025-02-15 19:42:42,080 - Epoch 81/1000, Train Loss: 0.1323, Val Loss: 0.1387, Accuracy: 94.95%
2025-02-15 19:43:18,174 - Epoch [82/1000], Step [100/4367], Loss: 0.0522
2025-02-15 19:43:52,816 - Epoch [82/1000], Step [200/4367], Loss: 0.1572
2025-02-15 19:44:27,442 - Epoch [82/1000], Step [300/4367], Loss: 0.1599
2025-02-15 19:45:02,564 - Epoch [82/1000], Step [400/4367], Loss: 0.1000
2025-02-15 19:45:37,285 - Epoch [82/1000], Step [500/4367], Loss: 0.0367
2025-02-15 19:46:11,922 - Epoch [82/1000], Step [600/4367], Loss: 0.1063
2025-02-15 19:46:46,326 - Epoch [82/1000], Step [700/4367], Loss: 0.0375
2025-02-15 19:47:21,176 - Epoch [82/1000], Step [800/4367], Loss: 0.0347
2025-02-15 19:47:56,173 - Epoch [82/1000], Step [900/4367], Loss: 0.2077
2025-02-15 19:48:30,444 - Epoch [82/1000], Step [1000/4367], Loss: 0.2369
2025-02-15 19:49:04,750 - Epoch [82/1000], Step [1100/4367], Loss: 0.0889
2025-02-15 19:49:39,586 - Epoch [82/1000], Step [1200/4367], Loss: 0.1179
2025-02-15 19:50:13,948 - Epoch [82/1000], Step [1300/4367], Loss: 0.0286
2025-02-15 19:50:48,900 - Epoch [82/1000], Step [1400/4367], Loss: 0.1829
2025-02-15 19:51:23,221 - Epoch [82/1000], Step [1500/4367], Loss: 0.3454
2025-02-15 19:51:57,868 - Epoch [82/1000], Step [1600/4367], Loss: 0.0777
2025-02-15 19:52:32,520 - Epoch [82/1000], Step [1700/4367], Loss: 0.1515
2025-02-15 19:53:07,247 - Epoch [82/1000], Step [1800/4367], Loss: 0.1791
2025-02-15 19:53:41,982 - Epoch [82/1000], Step [1900/4367], Loss: 0.0672
2025-02-15 19:54:16,580 - Epoch [82/1000], Step [2000/4367], Loss: 0.1123
2025-02-15 19:54:51,669 - Epoch [82/1000], Step [2100/4367], Loss: 0.2061
2025-02-15 19:55:26,271 - Epoch [82/1000], Step [2200/4367], Loss: 0.1371
2025-02-15 19:56:00,816 - Epoch [82/1000], Step [2300/4367], Loss: 0.0675
2025-02-15 19:56:35,964 - Epoch [82/1000], Step [2400/4367], Loss: 0.2208
2025-02-15 19:57:11,029 - Epoch [82/1000], Step [2500/4367], Loss: 0.1997
2025-02-15 19:57:45,925 - Epoch [82/1000], Step [2600/4367], Loss: 0.2943
2025-02-15 19:58:20,828 - Epoch [82/1000], Step [2700/4367], Loss: 0.2289
2025-02-15 19:58:55,645 - Epoch [82/1000], Step [2800/4367], Loss: 0.1926
2025-02-15 19:59:30,726 - Epoch [82/1000], Step [2900/4367], Loss: 0.0971
2025-02-15 20:00:05,489 - Epoch [82/1000], Step [3000/4367], Loss: 0.0895
2025-02-15 20:00:40,576 - Epoch [82/1000], Step [3100/4367], Loss: 0.1021
2025-02-15 20:01:15,233 - Epoch [82/1000], Step [3200/4367], Loss: 0.0571
2025-02-15 20:01:49,762 - Epoch [82/1000], Step [3300/4367], Loss: 0.2172
2025-02-15 20:02:24,494 - Epoch [82/1000], Step [3400/4367], Loss: 0.0732
2025-02-15 20:02:58,807 - Epoch [82/1000], Step [3500/4367], Loss: 0.0264
2025-02-15 20:03:33,434 - Epoch [82/1000], Step [3600/4367], Loss: 0.2360
2025-02-15 20:04:08,226 - Epoch [82/1000], Step [3700/4367], Loss: 0.0580
2025-02-15 20:04:42,721 - Epoch [82/1000], Step [3800/4367], Loss: 0.1204
2025-02-15 20:05:17,572 - Epoch [82/1000], Step [3900/4367], Loss: 0.0886
2025-02-15 20:05:52,011 - Epoch [82/1000], Step [4000/4367], Loss: 0.0889
2025-02-15 20:06:26,758 - Epoch [82/1000], Step [4100/4367], Loss: 0.0630
2025-02-15 20:07:01,195 - Epoch [82/1000], Step [4200/4367], Loss: 0.3462
2025-02-15 20:07:35,628 - Epoch [82/1000], Step [4300/4367], Loss: 0.0813
2025-02-15 20:08:08,910 - Epoch [82/1000], Validation Step [100/1090], Val Loss: 0.0005
2025-02-15 20:08:18,112 - Epoch [82/1000], Validation Step [200/1090], Val Loss: 0.0015
2025-02-15 20:08:27,465 - Epoch [82/1000], Validation Step [300/1090], Val Loss: 0.2505
2025-02-15 20:08:37,126 - Epoch [82/1000], Validation Step [400/1090], Val Loss: 0.0257
2025-02-15 20:08:46,240 - Epoch [82/1000], Validation Step [500/1090], Val Loss: 0.3732
2025-02-15 20:08:55,771 - Epoch [82/1000], Validation Step [600/1090], Val Loss: 0.1280
2025-02-15 20:09:05,333 - Epoch [82/1000], Validation Step [700/1090], Val Loss: 0.1372
2025-02-15 20:09:14,119 - Epoch [82/1000], Validation Step [800/1090], Val Loss: 0.0060
2025-02-15 20:09:22,674 - Epoch [82/1000], Validation Step [900/1090], Val Loss: 0.0071
2025-02-15 20:09:31,933 - Epoch [82/1000], Validation Step [1000/1090], Val Loss: 0.0001
2025-02-15 20:09:40,552 - Epoch 82/1000, Train Loss: 0.1321, Val Loss: 0.1430, Accuracy: 94.84%
2025-02-15 20:09:40,973 - Model saved at /data2/personal/sungjin/korean_dialects/checkpoint_real/checkpoint_epoch_82.pth
2025-02-15 20:10:16,350 - Epoch [83/1000], Step [100/4367], Loss: 0.1786
2025-02-15 20:10:51,367 - Epoch [83/1000], Step [200/4367], Loss: 0.3920
2025-02-15 20:11:25,928 - Epoch [83/1000], Step [300/4367], Loss: 0.0836
2025-02-15 20:12:00,925 - Epoch [83/1000], Step [400/4367], Loss: 0.2717
2025-02-15 20:12:35,512 - Epoch [83/1000], Step [500/4367], Loss: 0.3987
2025-02-15 20:13:09,881 - Epoch [83/1000], Step [600/4367], Loss: 0.3553
2025-02-15 20:13:44,449 - Epoch [83/1000], Step [700/4367], Loss: 0.1792
2025-02-15 20:14:19,238 - Epoch [83/1000], Step [800/4367], Loss: 0.2293
2025-02-15 20:14:54,428 - Epoch [83/1000], Step [900/4367], Loss: 0.0566
2025-02-15 20:15:28,976 - Epoch [83/1000], Step [1000/4367], Loss: 0.0604
2025-02-15 20:16:03,614 - Epoch [83/1000], Step [1100/4367], Loss: 0.0968
2025-02-15 20:16:38,248 - Epoch [83/1000], Step [1200/4367], Loss: 0.0924
2025-02-15 20:17:13,390 - Epoch [83/1000], Step [1300/4367], Loss: 0.2656
2025-02-15 20:17:48,249 - Epoch [83/1000], Step [1400/4367], Loss: 0.0968
2025-02-15 20:18:22,576 - Epoch [83/1000], Step [1500/4367], Loss: 0.0524
2025-02-15 20:18:56,758 - Epoch [83/1000], Step [1600/4367], Loss: 0.1568
2025-02-15 20:19:31,249 - Epoch [83/1000], Step [1700/4367], Loss: 0.1513
2025-02-15 20:20:05,887 - Epoch [83/1000], Step [1800/4367], Loss: 0.2784
2025-02-15 20:20:40,517 - Epoch [83/1000], Step [1900/4367], Loss: 0.2693
2025-02-15 20:21:15,325 - Epoch [83/1000], Step [2000/4367], Loss: 0.0541
2025-02-15 20:21:49,773 - Epoch [83/1000], Step [2100/4367], Loss: 0.3782
2025-02-15 20:22:24,556 - Epoch [83/1000], Step [2200/4367], Loss: 0.0967
2025-02-15 20:22:59,502 - Epoch [83/1000], Step [2300/4367], Loss: 0.0592
2025-02-15 20:23:34,026 - Epoch [83/1000], Step [2400/4367], Loss: 0.1172
2025-02-15 20:24:08,479 - Epoch [83/1000], Step [2500/4367], Loss: 0.1757
2025-02-15 20:24:42,643 - Epoch [83/1000], Step [2600/4367], Loss: 0.1557
2025-02-15 20:25:17,333 - Epoch [83/1000], Step [2700/4367], Loss: 0.1624
2025-02-15 20:25:52,096 - Epoch [83/1000], Step [2800/4367], Loss: 0.0769
2025-02-15 20:26:26,844 - Epoch [83/1000], Step [2900/4367], Loss: 0.0979
2025-02-15 20:27:01,629 - Epoch [83/1000], Step [3000/4367], Loss: 0.1194
2025-02-15 20:27:36,241 - Epoch [83/1000], Step [3100/4367], Loss: 0.1865
2025-02-15 20:28:10,838 - Epoch [83/1000], Step [3200/4367], Loss: 0.1169
2025-02-15 20:28:45,652 - Epoch [83/1000], Step [3300/4367], Loss: 0.0922
2025-02-15 20:29:19,711 - Epoch [83/1000], Step [3400/4367], Loss: 0.1471
2025-02-15 20:29:53,828 - Epoch [83/1000], Step [3500/4367], Loss: 0.1049
2025-02-15 20:30:27,920 - Epoch [83/1000], Step [3600/4367], Loss: 0.1613
2025-02-15 20:31:02,572 - Epoch [83/1000], Step [3700/4367], Loss: 0.0509
2025-02-15 20:31:37,358 - Epoch [83/1000], Step [3800/4367], Loss: 0.0698
2025-02-15 20:32:12,213 - Epoch [83/1000], Step [3900/4367], Loss: 0.0327
2025-02-15 20:32:46,827 - Epoch [83/1000], Step [4000/4367], Loss: 0.1494
2025-02-15 20:33:21,832 - Epoch [83/1000], Step [4100/4367], Loss: 0.1221
2025-02-15 20:33:56,401 - Epoch [83/1000], Step [4200/4367], Loss: 0.2577
2025-02-15 20:34:30,688 - Epoch [83/1000], Step [4300/4367], Loss: 0.1144
2025-02-15 20:35:03,755 - Epoch [83/1000], Validation Step [100/1090], Val Loss: 0.0001
2025-02-15 20:35:12,969 - Epoch [83/1000], Validation Step [200/1090], Val Loss: 0.0003
2025-02-15 20:35:22,312 - Epoch [83/1000], Validation Step [300/1090], Val Loss: 0.2611
2025-02-15 20:35:31,983 - Epoch [83/1000], Validation Step [400/1090], Val Loss: 0.0532
2025-02-15 20:35:41,096 - Epoch [83/1000], Validation Step [500/1090], Val Loss: 0.4171
2025-02-15 20:35:50,631 - Epoch [83/1000], Validation Step [600/1090], Val Loss: 0.1196
2025-02-15 20:36:00,201 - Epoch [83/1000], Validation Step [700/1090], Val Loss: 0.1132
2025-02-15 20:36:08,990 - Epoch [83/1000], Validation Step [800/1090], Val Loss: 0.0055
2025-02-15 20:36:17,558 - Epoch [83/1000], Validation Step [900/1090], Val Loss: 0.0044
2025-02-15 20:36:26,823 - Epoch [83/1000], Validation Step [1000/1090], Val Loss: 0.0001
2025-02-15 20:36:35,456 - Epoch 83/1000, Train Loss: 0.1326, Val Loss: 0.1405, Accuracy: 94.86%
2025-02-15 20:37:10,787 - Epoch [84/1000], Step [100/4367], Loss: 0.1773
2025-02-15 20:37:45,586 - Epoch [84/1000], Step [200/4367], Loss: 0.1492
2025-02-15 20:38:20,046 - Epoch [84/1000], Step [300/4367], Loss: 0.0171
2025-02-15 20:38:54,833 - Epoch [84/1000], Step [400/4367], Loss: 0.0108
2025-02-15 20:39:29,204 - Epoch [84/1000], Step [500/4367], Loss: 0.1912
2025-02-15 20:40:03,812 - Epoch [84/1000], Step [600/4367], Loss: 0.0410
2025-02-15 20:40:38,741 - Epoch [84/1000], Step [700/4367], Loss: 0.2482
2025-02-15 20:41:13,679 - Epoch [84/1000], Step [800/4367], Loss: 0.1265
2025-02-15 20:41:48,241 - Epoch [84/1000], Step [900/4367], Loss: 0.1133
2025-02-15 20:42:23,233 - Epoch [84/1000], Step [1000/4367], Loss: 0.1974
2025-02-15 20:42:57,904 - Epoch [84/1000], Step [1100/4367], Loss: 0.0607
2025-02-15 20:43:32,321 - Epoch [84/1000], Step [1200/4367], Loss: 0.0405
2025-02-15 20:44:06,802 - Epoch [84/1000], Step [1300/4367], Loss: 0.0659
2025-02-15 20:44:41,645 - Epoch [84/1000], Step [1400/4367], Loss: 0.1019
2025-02-15 20:45:16,501 - Epoch [84/1000], Step [1500/4367], Loss: 0.1758
2025-02-15 20:45:51,356 - Epoch [84/1000], Step [1600/4367], Loss: 0.0554
2025-02-15 20:46:26,304 - Epoch [84/1000], Step [1700/4367], Loss: 0.2654
2025-02-15 20:47:01,024 - Epoch [84/1000], Step [1800/4367], Loss: 0.0836
2025-02-15 20:47:36,009 - Epoch [84/1000], Step [1900/4367], Loss: 0.0242
2025-02-15 20:48:10,500 - Epoch [84/1000], Step [2000/4367], Loss: 0.0893
2025-02-15 20:48:45,230 - Epoch [84/1000], Step [2100/4367], Loss: 0.1677
2025-02-15 20:49:19,784 - Epoch [84/1000], Step [2200/4367], Loss: 0.0500
2025-02-15 20:49:54,588 - Epoch [84/1000], Step [2300/4367], Loss: 0.0743
2025-02-15 20:50:29,299 - Epoch [84/1000], Step [2400/4367], Loss: 0.0615
2025-02-15 20:51:04,163 - Epoch [84/1000], Step [2500/4367], Loss: 0.1918
2025-02-15 20:51:38,806 - Epoch [84/1000], Step [2600/4367], Loss: 0.1915
2025-02-15 20:52:13,582 - Epoch [84/1000], Step [2700/4367], Loss: 0.1486
2025-02-15 20:52:48,740 - Epoch [84/1000], Step [2800/4367], Loss: 0.1142
2025-02-15 20:53:23,077 - Epoch [84/1000], Step [2900/4367], Loss: 0.0251
2025-02-15 20:53:57,470 - Epoch [84/1000], Step [3000/4367], Loss: 0.2516
2025-02-15 20:54:32,125 - Epoch [84/1000], Step [3100/4367], Loss: 0.0898
2025-02-15 20:55:07,032 - Epoch [84/1000], Step [3200/4367], Loss: 0.2636
2025-02-15 20:55:41,846 - Epoch [84/1000], Step [3300/4367], Loss: 0.1350
2025-02-15 20:56:16,868 - Epoch [84/1000], Step [3400/4367], Loss: 0.1436
2025-02-15 20:56:51,615 - Epoch [84/1000], Step [3500/4367], Loss: 0.2488
2025-02-15 20:57:26,516 - Epoch [84/1000], Step [3600/4367], Loss: 0.0296
2025-02-15 20:58:01,425 - Epoch [84/1000], Step [3700/4367], Loss: 0.1745
2025-02-15 20:58:36,312 - Epoch [84/1000], Step [3800/4367], Loss: 0.3310
2025-02-15 20:59:10,660 - Epoch [84/1000], Step [3900/4367], Loss: 0.0868
2025-02-15 20:59:45,632 - Epoch [84/1000], Step [4000/4367], Loss: 0.1677
2025-02-15 21:00:20,387 - Epoch [84/1000], Step [4100/4367], Loss: 0.2613
2025-02-15 21:00:55,193 - Epoch [84/1000], Step [4200/4367], Loss: 0.0344
2025-02-15 21:01:30,021 - Epoch [84/1000], Step [4300/4367], Loss: 0.2679
2025-02-15 21:02:02,757 - Epoch [84/1000], Validation Step [100/1090], Val Loss: 0.0000
2025-02-15 21:02:11,961 - Epoch [84/1000], Validation Step [200/1090], Val Loss: 0.0001
2025-02-15 21:02:21,289 - Epoch [84/1000], Validation Step [300/1090], Val Loss: 0.2678
2025-02-15 21:02:30,949 - Epoch [84/1000], Validation Step [400/1090], Val Loss: 0.0348
2025-02-15 21:02:40,056 - Epoch [84/1000], Validation Step [500/1090], Val Loss: 0.4208
2025-02-15 21:02:49,579 - Epoch [84/1000], Validation Step [600/1090], Val Loss: 0.1170
2025-02-15 21:02:59,148 - Epoch [84/1000], Validation Step [700/1090], Val Loss: 0.1379
2025-02-15 21:03:07,931 - Epoch [84/1000], Validation Step [800/1090], Val Loss: 0.0037
2025-02-15 21:03:16,497 - Epoch [84/1000], Validation Step [900/1090], Val Loss: 0.0041
2025-02-15 21:03:25,751 - Epoch [84/1000], Validation Step [1000/1090], Val Loss: 0.0000
2025-02-15 21:03:34,370 - Epoch 84/1000, Train Loss: 0.1325, Val Loss: 0.1389, Accuracy: 94.94%
2025-02-15 21:03:34,798 - Model saved at /data2/personal/sungjin/korean_dialects/checkpoint_real/checkpoint_epoch_84.pth
2025-02-15 21:04:10,514 - Epoch [85/1000], Step [100/4367], Loss: 0.1894
2025-02-15 21:04:45,163 - Epoch [85/1000], Step [200/4367], Loss: 0.0978
2025-02-15 21:05:19,457 - Epoch [85/1000], Step [300/4367], Loss: 0.1945
2025-02-15 21:05:54,088 - Epoch [85/1000], Step [400/4367], Loss: 0.2516
2025-02-15 21:06:28,550 - Epoch [85/1000], Step [500/4367], Loss: 0.0640
2025-02-15 21:07:03,415 - Epoch [85/1000], Step [600/4367], Loss: 0.2106
2025-02-15 21:07:37,984 - Epoch [85/1000], Step [700/4367], Loss: 0.2665
2025-02-15 21:08:12,616 - Epoch [85/1000], Step [800/4367], Loss: 0.1175
2025-02-15 21:08:47,651 - Epoch [85/1000], Step [900/4367], Loss: 0.0923
2025-02-15 21:09:22,823 - Epoch [85/1000], Step [1000/4367], Loss: 0.3660
2025-02-15 21:09:57,748 - Epoch [85/1000], Step [1100/4367], Loss: 0.2055
2025-02-15 21:10:32,212 - Epoch [85/1000], Step [1200/4367], Loss: 0.1424
2025-02-15 21:11:07,143 - Epoch [85/1000], Step [1300/4367], Loss: 0.0296
2025-02-15 21:11:41,954 - Epoch [85/1000], Step [1400/4367], Loss: 0.1757
2025-02-15 21:12:16,554 - Epoch [85/1000], Step [1500/4367], Loss: 0.0806
2025-02-15 21:12:51,064 - Epoch [85/1000], Step [1600/4367], Loss: 0.2077
2025-02-15 21:13:25,675 - Epoch [85/1000], Step [1700/4367], Loss: 0.2819
2025-02-15 21:14:00,374 - Epoch [85/1000], Step [1800/4367], Loss: 0.2558
2025-02-15 21:14:35,168 - Epoch [85/1000], Step [1900/4367], Loss: 0.1031
2025-02-15 21:15:09,378 - Epoch [85/1000], Step [2000/4367], Loss: 0.1194
2025-02-15 21:15:44,208 - Epoch [85/1000], Step [2100/4367], Loss: 0.1526
2025-02-15 21:16:18,848 - Epoch [85/1000], Step [2200/4367], Loss: 0.1841
2025-02-15 21:16:53,785 - Epoch [85/1000], Step [2300/4367], Loss: 0.0805
2025-02-15 21:17:28,262 - Epoch [85/1000], Step [2400/4367], Loss: 0.2576
2025-02-15 21:18:03,046 - Epoch [85/1000], Step [2500/4367], Loss: 0.0842
2025-02-15 21:18:37,949 - Epoch [85/1000], Step [2600/4367], Loss: 0.1527
2025-02-15 21:19:12,660 - Epoch [85/1000], Step [2700/4367], Loss: 0.0711
2025-02-15 21:19:47,320 - Epoch [85/1000], Step [2800/4367], Loss: 0.2226
2025-02-15 21:20:22,247 - Epoch [85/1000], Step [2900/4367], Loss: 0.1154
2025-02-15 21:20:56,726 - Epoch [85/1000], Step [3000/4367], Loss: 0.4494
2025-02-15 21:21:31,369 - Epoch [85/1000], Step [3100/4367], Loss: 0.0359
2025-02-15 21:22:06,444 - Epoch [85/1000], Step [3200/4367], Loss: 0.3426
2025-02-15 21:22:41,099 - Epoch [85/1000], Step [3300/4367], Loss: 0.2847
2025-02-15 21:23:16,234 - Epoch [85/1000], Step [3400/4367], Loss: 0.0433
2025-02-15 21:23:50,975 - Epoch [85/1000], Step [3500/4367], Loss: 0.1998
2025-02-15 21:24:25,682 - Epoch [85/1000], Step [3600/4367], Loss: 0.0528
2025-02-15 21:25:00,584 - Epoch [85/1000], Step [3700/4367], Loss: 0.0984
2025-02-15 21:25:35,653 - Epoch [85/1000], Step [3800/4367], Loss: 0.2234
2025-02-15 21:26:10,175 - Epoch [85/1000], Step [3900/4367], Loss: 0.1833
2025-02-15 21:26:44,671 - Epoch [85/1000], Step [4000/4367], Loss: 0.1686
2025-02-15 21:27:19,438 - Epoch [85/1000], Step [4100/4367], Loss: 0.0836
2025-02-15 21:27:54,441 - Epoch [85/1000], Step [4200/4367], Loss: 0.0727
2025-02-15 21:28:28,972 - Epoch [85/1000], Step [4300/4367], Loss: 0.1874
2025-02-15 21:29:02,005 - Epoch [85/1000], Validation Step [100/1090], Val Loss: 0.0001
2025-02-15 21:29:11,208 - Epoch [85/1000], Validation Step [200/1090], Val Loss: 0.0001
2025-02-15 21:29:20,558 - Epoch [85/1000], Validation Step [300/1090], Val Loss: 0.2518
2025-02-15 21:29:30,227 - Epoch [85/1000], Validation Step [400/1090], Val Loss: 0.0841
2025-02-15 21:29:39,337 - Epoch [85/1000], Validation Step [500/1090], Val Loss: 0.5289
2025-02-15 21:29:48,871 - Epoch [85/1000], Validation Step [600/1090], Val Loss: 0.1206
2025-02-15 21:29:58,449 - Epoch [85/1000], Validation Step [700/1090], Val Loss: 0.1204
2025-02-15 21:30:07,252 - Epoch [85/1000], Validation Step [800/1090], Val Loss: 0.0049
2025-02-15 21:30:15,817 - Epoch [85/1000], Validation Step [900/1090], Val Loss: 0.0038
2025-02-15 21:30:25,082 - Epoch [85/1000], Validation Step [1000/1090], Val Loss: 0.0000
2025-02-15 21:30:33,694 - Epoch 85/1000, Train Loss: 0.1327, Val Loss: 0.1439, Accuracy: 94.78%
2025-02-15 21:31:09,311 - Epoch [86/1000], Step [100/4367], Loss: 0.0611
2025-02-15 21:31:44,441 - Epoch [86/1000], Step [200/4367], Loss: 0.2203
2025-02-15 21:32:19,134 - Epoch [86/1000], Step [300/4367], Loss: 0.0593
2025-02-15 21:32:53,833 - Epoch [86/1000], Step [400/4367], Loss: 0.3071
2025-02-15 21:33:28,418 - Epoch [86/1000], Step [500/4367], Loss: 0.1425
2025-02-15 21:34:02,983 - Epoch [86/1000], Step [600/4367], Loss: 0.2434
2025-02-15 21:34:37,632 - Epoch [86/1000], Step [700/4367], Loss: 0.1307
2025-02-15 21:35:11,823 - Epoch [86/1000], Step [800/4367], Loss: 0.1417
2025-02-15 21:35:46,380 - Epoch [86/1000], Step [900/4367], Loss: 0.1277
2025-02-15 21:36:20,884 - Epoch [86/1000], Step [1000/4367], Loss: 0.2895
2025-02-15 21:36:55,339 - Epoch [86/1000], Step [1100/4367], Loss: 0.1298
2025-02-15 21:37:29,818 - Epoch [86/1000], Step [1200/4367], Loss: 0.1456
2025-02-15 21:38:04,666 - Epoch [86/1000], Step [1300/4367], Loss: 0.2070
2025-02-15 21:38:39,411 - Epoch [86/1000], Step [1400/4367], Loss: 0.0485
2025-02-15 21:39:14,059 - Epoch [86/1000], Step [1500/4367], Loss: 0.3111
2025-02-15 21:39:48,849 - Epoch [86/1000], Step [1600/4367], Loss: 0.1171
2025-02-15 21:40:24,026 - Epoch [86/1000], Step [1700/4367], Loss: 0.0874
2025-02-15 21:40:58,769 - Epoch [86/1000], Step [1800/4367], Loss: 0.0460
2025-02-15 21:41:32,869 - Epoch [86/1000], Step [1900/4367], Loss: 0.1199
2025-02-15 21:42:07,354 - Epoch [86/1000], Step [2000/4367], Loss: 0.2287
2025-02-15 21:42:41,765 - Epoch [86/1000], Step [2100/4367], Loss: 0.0725
2025-02-15 21:43:16,144 - Epoch [86/1000], Step [2200/4367], Loss: 0.1612
2025-02-15 21:43:50,597 - Epoch [86/1000], Step [2300/4367], Loss: 0.1081
2025-02-15 21:44:25,684 - Epoch [86/1000], Step [2400/4367], Loss: 0.1761
2025-02-15 21:45:00,247 - Epoch [86/1000], Step [2500/4367], Loss: 0.3191
2025-02-15 21:45:35,061 - Epoch [86/1000], Step [2600/4367], Loss: 0.1653
2025-02-15 21:46:09,733 - Epoch [86/1000], Step [2700/4367], Loss: 0.0357
2025-02-15 21:46:44,334 - Epoch [86/1000], Step [2800/4367], Loss: 0.2595
2025-02-15 21:47:19,040 - Epoch [86/1000], Step [2900/4367], Loss: 0.1914
2025-02-15 21:47:53,764 - Epoch [86/1000], Step [3000/4367], Loss: 0.2374
2025-02-15 21:48:28,513 - Epoch [86/1000], Step [3100/4367], Loss: 0.0394
2025-02-15 21:49:03,039 - Epoch [86/1000], Step [3200/4367], Loss: 0.0758
2025-02-15 21:49:37,931 - Epoch [86/1000], Step [3300/4367], Loss: 0.0547
2025-02-15 21:50:11,932 - Epoch [86/1000], Step [3400/4367], Loss: 0.2087
2025-02-15 21:50:46,787 - Epoch [86/1000], Step [3500/4367], Loss: 0.1121
2025-02-15 21:51:21,097 - Epoch [86/1000], Step [3600/4367], Loss: 0.2165
2025-02-15 21:51:56,134 - Epoch [86/1000], Step [3700/4367], Loss: 0.0568
2025-02-15 21:52:30,848 - Epoch [86/1000], Step [3800/4367], Loss: 0.0601
2025-02-15 21:53:06,074 - Epoch [86/1000], Step [3900/4367], Loss: 0.0380
2025-02-15 21:53:40,612 - Epoch [86/1000], Step [4000/4367], Loss: 0.0793
2025-02-15 21:54:15,400 - Epoch [86/1000], Step [4100/4367], Loss: 0.1485
2025-02-15 21:54:49,842 - Epoch [86/1000], Step [4200/4367], Loss: 0.2128
2025-02-15 21:55:24,329 - Epoch [86/1000], Step [4300/4367], Loss: 0.0186
2025-02-15 21:55:57,379 - Epoch [86/1000], Validation Step [100/1090], Val Loss: 0.0000
2025-02-15 21:56:06,578 - Epoch [86/1000], Validation Step [200/1090], Val Loss: 0.0001
2025-02-15 21:56:15,918 - Epoch [86/1000], Validation Step [300/1090], Val Loss: 0.2432
2025-02-15 21:56:25,585 - Epoch [86/1000], Validation Step [400/1090], Val Loss: 0.0334
2025-02-15 21:56:34,689 - Epoch [86/1000], Validation Step [500/1090], Val Loss: 0.3811
2025-02-15 21:56:44,220 - Epoch [86/1000], Validation Step [600/1090], Val Loss: 0.1493
2025-02-15 21:56:53,787 - Epoch [86/1000], Validation Step [700/1090], Val Loss: 0.1504
2025-02-15 21:57:02,570 - Epoch [86/1000], Validation Step [800/1090], Val Loss: 0.0060
2025-02-15 21:57:11,127 - Epoch [86/1000], Validation Step [900/1090], Val Loss: 0.0054
2025-02-15 21:57:20,377 - Epoch [86/1000], Validation Step [1000/1090], Val Loss: 0.0001
2025-02-15 21:57:28,990 - Epoch 86/1000, Train Loss: 0.1319, Val Loss: 0.1416, Accuracy: 94.83%
2025-02-15 21:57:29,392 - Model saved at /data2/personal/sungjin/korean_dialects/checkpoint_real/checkpoint_epoch_86.pth
2025-02-15 21:58:04,933 - Epoch [87/1000], Step [100/4367], Loss: 0.0924
2025-02-15 21:58:39,191 - Epoch [87/1000], Step [200/4367], Loss: 0.0648
2025-02-15 21:59:14,102 - Epoch [87/1000], Step [300/4367], Loss: 0.0823
2025-02-15 21:59:48,323 - Epoch [87/1000], Step [400/4367], Loss: 0.2866
2025-02-15 22:00:22,644 - Epoch [87/1000], Step [500/4367], Loss: 0.0877
2025-02-15 22:00:57,887 - Epoch [87/1000], Step [600/4367], Loss: 0.1971
2025-02-15 22:01:32,964 - Epoch [87/1000], Step [700/4367], Loss: 0.1069
2025-02-15 22:02:08,062 - Epoch [87/1000], Step [800/4367], Loss: 0.0801
2025-02-15 22:02:42,771 - Epoch [87/1000], Step [900/4367], Loss: 0.2150
2025-02-15 22:03:17,609 - Epoch [87/1000], Step [1000/4367], Loss: 0.1911
2025-02-15 22:03:52,130 - Epoch [87/1000], Step [1100/4367], Loss: 0.2197
2025-02-15 22:04:27,090 - Epoch [87/1000], Step [1200/4367], Loss: 0.1637
2025-02-15 22:05:01,565 - Epoch [87/1000], Step [1300/4367], Loss: 0.0463
2025-02-15 22:05:36,435 - Epoch [87/1000], Step [1400/4367], Loss: 0.1075
2025-02-15 22:06:10,780 - Epoch [87/1000], Step [1500/4367], Loss: 0.1286
2025-02-15 22:06:45,564 - Epoch [87/1000], Step [1600/4367], Loss: 0.1123
2025-02-15 22:07:20,116 - Epoch [87/1000], Step [1700/4367], Loss: 0.0418
2025-02-15 22:07:54,795 - Epoch [87/1000], Step [1800/4367], Loss: 0.1366
2025-02-15 22:08:29,930 - Epoch [87/1000], Step [1900/4367], Loss: 0.1867
2025-02-15 22:09:04,234 - Epoch [87/1000], Step [2000/4367], Loss: 0.0839
2025-02-15 22:09:38,992 - Epoch [87/1000], Step [2100/4367], Loss: 0.1857
2025-02-15 22:10:13,913 - Epoch [87/1000], Step [2200/4367], Loss: 0.1054
2025-02-15 22:10:48,316 - Epoch [87/1000], Step [2300/4367], Loss: 0.1202
2025-02-15 22:11:22,705 - Epoch [87/1000], Step [2400/4367], Loss: 0.1354
2025-02-15 22:11:57,206 - Epoch [87/1000], Step [2500/4367], Loss: 0.1645
2025-02-15 22:12:32,170 - Epoch [87/1000], Step [2600/4367], Loss: 0.1470
2025-02-15 22:13:06,841 - Epoch [87/1000], Step [2700/4367], Loss: 0.3263
2025-02-15 22:13:41,187 - Epoch [87/1000], Step [2800/4367], Loss: 0.0609
2025-02-15 22:14:15,802 - Epoch [87/1000], Step [2900/4367], Loss: 0.0526
2025-02-15 22:14:50,607 - Epoch [87/1000], Step [3000/4367], Loss: 0.2115
2025-02-15 22:15:25,450 - Epoch [87/1000], Step [3100/4367], Loss: 0.1989
2025-02-15 22:16:00,076 - Epoch [87/1000], Step [3200/4367], Loss: 0.1071
2025-02-15 22:16:34,681 - Epoch [87/1000], Step [3300/4367], Loss: 0.1624
2025-02-15 22:17:09,587 - Epoch [87/1000], Step [3400/4367], Loss: 0.2444
2025-02-15 22:17:44,248 - Epoch [87/1000], Step [3500/4367], Loss: 0.0450
2025-02-15 22:18:19,050 - Epoch [87/1000], Step [3600/4367], Loss: 0.0404
2025-02-15 22:18:53,646 - Epoch [87/1000], Step [3700/4367], Loss: 0.1403
2025-02-15 22:19:28,029 - Epoch [87/1000], Step [3800/4367], Loss: 0.1573
2025-02-15 22:20:02,839 - Epoch [87/1000], Step [3900/4367], Loss: 0.0655
2025-02-15 22:20:37,381 - Epoch [87/1000], Step [4000/4367], Loss: 0.2882
2025-02-15 22:21:11,883 - Epoch [87/1000], Step [4100/4367], Loss: 0.0388
2025-02-15 22:21:46,515 - Epoch [87/1000], Step [4200/4367], Loss: 0.0364
2025-02-15 22:22:21,114 - Epoch [87/1000], Step [4300/4367], Loss: 0.1165
2025-02-15 22:22:54,371 - Epoch [87/1000], Validation Step [100/1090], Val Loss: 0.0001
2025-02-15 22:23:03,571 - Epoch [87/1000], Validation Step [200/1090], Val Loss: 0.0005
2025-02-15 22:23:12,923 - Epoch [87/1000], Validation Step [300/1090], Val Loss: 0.2815
2025-02-15 22:23:22,596 - Epoch [87/1000], Validation Step [400/1090], Val Loss: 0.0889
2025-02-15 22:23:31,710 - Epoch [87/1000], Validation Step [500/1090], Val Loss: 0.4277
2025-02-15 22:23:41,252 - Epoch [87/1000], Validation Step [600/1090], Val Loss: 0.0966
2025-02-15 22:23:50,826 - Epoch [87/1000], Validation Step [700/1090], Val Loss: 0.0956
2025-02-15 22:23:59,620 - Epoch [87/1000], Validation Step [800/1090], Val Loss: 0.0059
2025-02-15 22:24:08,196 - Epoch [87/1000], Validation Step [900/1090], Val Loss: 0.0056
2025-02-15 22:24:17,466 - Epoch [87/1000], Validation Step [1000/1090], Val Loss: 0.0004
2025-02-15 22:24:26,095 - Epoch 87/1000, Train Loss: 0.1334, Val Loss: 0.1488, Accuracy: 94.65%
2025-02-15 22:25:01,615 - Epoch [88/1000], Step [100/4367], Loss: 0.1296
2025-02-15 22:25:36,557 - Epoch [88/1000], Step [200/4367], Loss: 0.1647
2025-02-15 22:26:11,422 - Epoch [88/1000], Step [300/4367], Loss: 0.1602
2025-02-15 22:26:45,825 - Epoch [88/1000], Step [400/4367], Loss: 0.2299
2025-02-15 22:27:20,629 - Epoch [88/1000], Step [500/4367], Loss: 0.1295
2025-02-15 22:27:55,542 - Epoch [88/1000], Step [600/4367], Loss: 0.1141
2025-02-15 22:28:30,190 - Epoch [88/1000], Step [700/4367], Loss: 0.0628
2025-02-15 22:29:04,632 - Epoch [88/1000], Step [800/4367], Loss: 0.1320
2025-02-15 22:29:39,326 - Epoch [88/1000], Step [900/4367], Loss: 0.2090
2025-02-15 22:30:14,292 - Epoch [88/1000], Step [1000/4367], Loss: 0.1930
2025-02-15 22:30:49,209 - Epoch [88/1000], Step [1100/4367], Loss: 0.0818
2025-02-15 22:31:23,910 - Epoch [88/1000], Step [1200/4367], Loss: 0.0616
2025-02-15 22:31:58,218 - Epoch [88/1000], Step [1300/4367], Loss: 0.1627
2025-02-15 22:32:33,255 - Epoch [88/1000], Step [1400/4367], Loss: 0.0400
2025-02-15 22:33:08,156 - Epoch [88/1000], Step [1500/4367], Loss: 0.1122
2025-02-15 22:33:42,909 - Epoch [88/1000], Step [1600/4367], Loss: 0.0811
2025-02-15 22:34:17,323 - Epoch [88/1000], Step [1700/4367], Loss: 0.1693
2025-02-15 22:34:52,229 - Epoch [88/1000], Step [1800/4367], Loss: 0.0788
2025-02-15 22:35:26,704 - Epoch [88/1000], Step [1900/4367], Loss: 0.0321
2025-02-15 22:36:01,814 - Epoch [88/1000], Step [2000/4367], Loss: 0.1957
2025-02-15 22:36:36,935 - Epoch [88/1000], Step [2100/4367], Loss: 0.0649
2025-02-15 22:37:11,233 - Epoch [88/1000], Step [2200/4367], Loss: 0.1155
2025-02-15 22:37:45,881 - Epoch [88/1000], Step [2300/4367], Loss: 0.0588
2025-02-15 22:38:20,872 - Epoch [88/1000], Step [2400/4367], Loss: 0.0910
2025-02-15 22:38:55,416 - Epoch [88/1000], Step [2500/4367], Loss: 0.1116
2025-02-15 22:39:30,280 - Epoch [88/1000], Step [2600/4367], Loss: 0.0809
2025-02-15 22:40:04,659 - Epoch [88/1000], Step [2700/4367], Loss: 0.0946
2025-02-15 22:40:38,915 - Epoch [88/1000], Step [2800/4367], Loss: 0.2891
2025-02-15 22:41:13,281 - Epoch [88/1000], Step [2900/4367], Loss: 0.0655
2025-02-15 22:41:47,713 - Epoch [88/1000], Step [3000/4367], Loss: 0.0890
2025-02-15 22:42:21,905 - Epoch [88/1000], Step [3100/4367], Loss: 0.0543
2025-02-15 22:42:56,658 - Epoch [88/1000], Step [3200/4367], Loss: 0.2904
2025-02-15 22:43:31,557 - Epoch [88/1000], Step [3300/4367], Loss: 0.3280
2025-02-15 22:44:06,493 - Epoch [88/1000], Step [3400/4367], Loss: 0.0754
2025-02-15 22:44:41,116 - Epoch [88/1000], Step [3500/4367], Loss: 0.1320
2025-02-15 22:45:15,985 - Epoch [88/1000], Step [3600/4367], Loss: 0.3905
2025-02-15 22:45:50,530 - Epoch [88/1000], Step [3700/4367], Loss: 0.1700
2025-02-15 22:46:25,179 - Epoch [88/1000], Step [3800/4367], Loss: 0.1730
2025-02-15 22:46:59,791 - Epoch [88/1000], Step [3900/4367], Loss: 0.1422
2025-02-15 22:47:34,451 - Epoch [88/1000], Step [4000/4367], Loss: 0.0324
2025-02-15 22:48:08,718 - Epoch [88/1000], Step [4100/4367], Loss: 0.0198
2025-02-15 22:48:43,637 - Epoch [88/1000], Step [4200/4367], Loss: 0.1289
2025-02-15 22:49:18,770 - Epoch [88/1000], Step [4300/4367], Loss: 0.0347
2025-02-15 22:49:52,073 - Epoch [88/1000], Validation Step [100/1090], Val Loss: 0.0001
2025-02-15 22:50:01,254 - Epoch [88/1000], Validation Step [200/1090], Val Loss: 0.0002
2025-02-15 22:50:10,581 - Epoch [88/1000], Validation Step [300/1090], Val Loss: 0.2552
2025-02-15 22:50:20,239 - Epoch [88/1000], Validation Step [400/1090], Val Loss: 0.0411
2025-02-15 22:50:29,346 - Epoch [88/1000], Validation Step [500/1090], Val Loss: 0.4156
2025-02-15 22:50:38,872 - Epoch [88/1000], Validation Step [600/1090], Val Loss: 0.1209
2025-02-15 22:50:48,428 - Epoch [88/1000], Validation Step [700/1090], Val Loss: 0.1109
2025-02-15 22:50:57,209 - Epoch [88/1000], Validation Step [800/1090], Val Loss: 0.0045
2025-02-15 22:51:05,762 - Epoch [88/1000], Validation Step [900/1090], Val Loss: 0.0046
2025-02-15 22:51:15,000 - Epoch [88/1000], Validation Step [1000/1090], Val Loss: 0.0000
2025-02-15 22:51:23,616 - Epoch 88/1000, Train Loss: 0.1324, Val Loss: 0.1391, Accuracy: 94.94%
2025-02-15 22:51:24,010 - Model saved at /data2/personal/sungjin/korean_dialects/checkpoint_real/checkpoint_epoch_88.pth
2025-02-15 22:51:59,878 - Epoch [89/1000], Step [100/4367], Loss: 0.3624
2025-02-15 22:52:34,711 - Epoch [89/1000], Step [200/4367], Loss: 0.0774
2025-02-15 22:53:09,156 - Epoch [89/1000], Step [300/4367], Loss: 0.0726
2025-02-15 22:53:43,761 - Epoch [89/1000], Step [400/4367], Loss: 0.1836
2025-02-15 22:54:18,025 - Epoch [89/1000], Step [500/4367], Loss: 0.1652
2025-02-15 22:54:52,972 - Epoch [89/1000], Step [600/4367], Loss: 0.4105
2025-02-15 22:55:27,333 - Epoch [89/1000], Step [700/4367], Loss: 0.2420
2025-02-15 22:56:01,898 - Epoch [89/1000], Step [800/4367], Loss: 0.1876
2025-02-15 22:56:37,218 - Epoch [89/1000], Step [900/4367], Loss: 0.2067
2025-02-15 22:57:11,729 - Epoch [89/1000], Step [1000/4367], Loss: 0.1060
2025-02-15 22:57:46,699 - Epoch [89/1000], Step [1100/4367], Loss: 0.1603
2025-02-15 22:58:21,485 - Epoch [89/1000], Step [1200/4367], Loss: 0.0214
2025-02-15 22:58:56,198 - Epoch [89/1000], Step [1300/4367], Loss: 0.1082
2025-02-15 22:59:30,751 - Epoch [89/1000], Step [1400/4367], Loss: 0.1122
2025-02-15 23:00:05,203 - Epoch [89/1000], Step [1500/4367], Loss: 0.0946
2025-02-15 23:00:40,210 - Epoch [89/1000], Step [1600/4367], Loss: 0.0890
2025-02-15 23:01:15,316 - Epoch [89/1000], Step [1700/4367], Loss: 0.1187
2025-02-15 23:01:49,996 - Epoch [89/1000], Step [1800/4367], Loss: 0.0487
2025-02-15 23:02:24,245 - Epoch [89/1000], Step [1900/4367], Loss: 0.2081
2025-02-15 23:02:58,997 - Epoch [89/1000], Step [2000/4367], Loss: 0.2029
2025-02-15 23:03:33,695 - Epoch [89/1000], Step [2100/4367], Loss: 0.1545
2025-02-15 23:04:08,060 - Epoch [89/1000], Step [2200/4367], Loss: 0.0277
2025-02-15 23:04:42,808 - Epoch [89/1000], Step [2300/4367], Loss: 0.0890
2025-02-15 23:05:17,377 - Epoch [89/1000], Step [2400/4367], Loss: 0.1898
2025-02-15 23:05:52,447 - Epoch [89/1000], Step [2500/4367], Loss: 0.3175
2025-02-15 23:06:27,341 - Epoch [89/1000], Step [2600/4367], Loss: 0.2111
2025-02-15 23:07:02,316 - Epoch [89/1000], Step [2700/4367], Loss: 0.1594
2025-02-15 23:07:36,985 - Epoch [89/1000], Step [2800/4367], Loss: 0.0622
2025-02-15 23:08:11,616 - Epoch [89/1000], Step [2900/4367], Loss: 0.1459
2025-02-15 23:08:46,036 - Epoch [89/1000], Step [3000/4367], Loss: 0.1538
2025-02-15 23:09:20,894 - Epoch [89/1000], Step [3100/4367], Loss: 0.0822
2025-02-15 23:09:55,841 - Epoch [89/1000], Step [3200/4367], Loss: 0.1196
2025-02-15 23:10:30,615 - Epoch [89/1000], Step [3300/4367], Loss: 0.0963
2025-02-15 23:11:05,492 - Epoch [89/1000], Step [3400/4367], Loss: 0.1016
2025-02-15 23:11:40,206 - Epoch [89/1000], Step [3500/4367], Loss: 0.2018
2025-02-15 23:12:15,133 - Epoch [89/1000], Step [3600/4367], Loss: 0.0783
2025-02-15 23:12:49,653 - Epoch [89/1000], Step [3700/4367], Loss: 0.0460
2025-02-15 23:13:24,482 - Epoch [89/1000], Step [3800/4367], Loss: 0.0300
2025-02-15 23:13:59,013 - Epoch [89/1000], Step [3900/4367], Loss: 0.2236
2025-02-15 23:14:33,479 - Epoch [89/1000], Step [4000/4367], Loss: 0.2322
2025-02-15 23:15:07,946 - Epoch [89/1000], Step [4100/4367], Loss: 0.0989
2025-02-15 23:15:42,371 - Epoch [89/1000], Step [4200/4367], Loss: 0.3477
2025-02-15 23:16:17,240 - Epoch [89/1000], Step [4300/4367], Loss: 0.1588
2025-02-15 23:16:50,314 - Epoch [89/1000], Validation Step [100/1090], Val Loss: 0.0001
2025-02-15 23:16:59,505 - Epoch [89/1000], Validation Step [200/1090], Val Loss: 0.0001
2025-02-15 23:17:08,839 - Epoch [89/1000], Validation Step [300/1090], Val Loss: 0.3080
2025-02-15 23:17:18,502 - Epoch [89/1000], Validation Step [400/1090], Val Loss: 0.0635
2025-02-15 23:17:27,608 - Epoch [89/1000], Validation Step [500/1090], Val Loss: 0.4800
2025-02-15 23:17:37,137 - Epoch [89/1000], Validation Step [600/1090], Val Loss: 0.0719
2025-02-15 23:17:46,711 - Epoch [89/1000], Validation Step [700/1090], Val Loss: 0.0747
2025-02-15 23:17:55,498 - Epoch [89/1000], Validation Step [800/1090], Val Loss: 0.0029
2025-02-15 23:18:04,062 - Epoch [89/1000], Validation Step [900/1090], Val Loss: 0.0021
2025-02-15 23:18:13,311 - Epoch [89/1000], Validation Step [1000/1090], Val Loss: 0.0000
2025-02-15 23:18:21,942 - Epoch 89/1000, Train Loss: 0.1330, Val Loss: 0.1459, Accuracy: 94.68%
2025-02-15 23:18:57,591 - Epoch [90/1000], Step [100/4367], Loss: 0.1685
2025-02-15 23:19:32,270 - Epoch [90/1000], Step [200/4367], Loss: 0.0542
2025-02-15 23:20:06,892 - Epoch [90/1000], Step [300/4367], Loss: 0.1529
2025-02-15 23:20:41,694 - Epoch [90/1000], Step [400/4367], Loss: 0.2824
2025-02-15 23:21:16,543 - Epoch [90/1000], Step [500/4367], Loss: 0.1798
2025-02-15 23:21:51,054 - Epoch [90/1000], Step [600/4367], Loss: 0.2357
2025-02-15 23:22:25,959 - Epoch [90/1000], Step [700/4367], Loss: 0.0670
2025-02-15 23:22:59,935 - Epoch [90/1000], Step [800/4367], Loss: 0.1900
2025-02-15 23:23:34,337 - Epoch [90/1000], Step [900/4367], Loss: 0.1644
2025-02-15 23:24:08,836 - Epoch [90/1000], Step [1000/4367], Loss: 0.3007
2025-02-15 23:24:43,516 - Epoch [90/1000], Step [1100/4367], Loss: 0.0890
2025-02-15 23:25:18,532 - Epoch [90/1000], Step [1200/4367], Loss: 0.0910
2025-02-15 23:25:53,410 - Epoch [90/1000], Step [1300/4367], Loss: 0.0851
2025-02-15 23:26:27,940 - Epoch [90/1000], Step [1400/4367], Loss: 0.1120
2025-02-15 23:27:02,345 - Epoch [90/1000], Step [1500/4367], Loss: 0.1599
2025-02-15 23:27:37,016 - Epoch [90/1000], Step [1600/4367], Loss: 0.1497
2025-02-15 23:28:12,119 - Epoch [90/1000], Step [1700/4367], Loss: 0.1261
2025-02-15 23:28:46,936 - Epoch [90/1000], Step [1800/4367], Loss: 0.1204
2025-02-15 23:29:21,323 - Epoch [90/1000], Step [1900/4367], Loss: 0.3205
2025-02-15 23:29:55,750 - Epoch [90/1000], Step [2000/4367], Loss: 0.1049
2025-02-15 23:30:30,764 - Epoch [90/1000], Step [2100/4367], Loss: 0.1421
2025-02-15 23:31:05,857 - Epoch [90/1000], Step [2200/4367], Loss: 0.0451
2025-02-15 23:31:40,421 - Epoch [90/1000], Step [2300/4367], Loss: 0.1578
2025-02-15 23:32:15,332 - Epoch [90/1000], Step [2400/4367], Loss: 0.0224
2025-02-15 23:32:49,682 - Epoch [90/1000], Step [2500/4367], Loss: 0.2738
2025-02-15 23:33:24,029 - Epoch [90/1000], Step [2600/4367], Loss: 0.1094
2025-02-15 23:33:58,643 - Epoch [90/1000], Step [2700/4367], Loss: 0.0572
2025-02-15 23:34:33,299 - Epoch [90/1000], Step [2800/4367], Loss: 0.0976
2025-02-15 23:35:08,053 - Epoch [90/1000], Step [2900/4367], Loss: 0.2848
2025-02-15 23:35:42,959 - Epoch [90/1000], Step [3000/4367], Loss: 0.0809
2025-02-15 23:36:17,668 - Epoch [90/1000], Step [3100/4367], Loss: 0.1017
2025-02-15 23:36:52,298 - Epoch [90/1000], Step [3200/4367], Loss: 0.2932
2025-02-15 23:37:27,280 - Epoch [90/1000], Step [3300/4367], Loss: 0.2533
2025-02-15 23:38:02,469 - Epoch [90/1000], Step [3400/4367], Loss: 0.1411
2025-02-15 23:38:37,247 - Epoch [90/1000], Step [3500/4367], Loss: 0.2221
2025-02-15 23:39:11,716 - Epoch [90/1000], Step [3600/4367], Loss: 0.2270
2025-02-15 23:39:46,435 - Epoch [90/1000], Step [3700/4367], Loss: 0.2366
2025-02-15 23:40:21,482 - Epoch [90/1000], Step [3800/4367], Loss: 0.3229
2025-02-15 23:40:56,047 - Epoch [90/1000], Step [3900/4367], Loss: 0.1027
2025-02-15 23:41:31,050 - Epoch [90/1000], Step [4000/4367], Loss: 0.0888
2025-02-15 23:42:06,053 - Epoch [90/1000], Step [4100/4367], Loss: 0.1160
2025-02-15 23:42:40,359 - Epoch [90/1000], Step [4200/4367], Loss: 0.0850
2025-02-15 23:43:14,954 - Epoch [90/1000], Step [4300/4367], Loss: 0.0241
2025-02-15 23:43:48,041 - Epoch [90/1000], Validation Step [100/1090], Val Loss: 0.0001
2025-02-15 23:43:57,229 - Epoch [90/1000], Validation Step [200/1090], Val Loss: 0.0002
2025-02-15 23:44:06,568 - Epoch [90/1000], Validation Step [300/1090], Val Loss: 0.2518
2025-02-15 23:44:16,228 - Epoch [90/1000], Validation Step [400/1090], Val Loss: 0.0431
2025-02-15 23:44:25,338 - Epoch [90/1000], Validation Step [500/1090], Val Loss: 0.4168
2025-02-15 23:44:34,866 - Epoch [90/1000], Validation Step [600/1090], Val Loss: 0.1179
2025-02-15 23:44:44,436 - Epoch [90/1000], Validation Step [700/1090], Val Loss: 0.1298
2025-02-15 23:44:53,228 - Epoch [90/1000], Validation Step [800/1090], Val Loss: 0.0051
2025-02-15 23:45:01,793 - Epoch [90/1000], Validation Step [900/1090], Val Loss: 0.0040
2025-02-15 23:45:11,054 - Epoch [90/1000], Validation Step [1000/1090], Val Loss: 0.0001
2025-02-15 23:45:19,696 - Epoch 90/1000, Train Loss: 0.1324, Val Loss: 0.1392, Accuracy: 94.88%
2025-02-15 23:45:20,142 - Model saved at /data2/personal/sungjin/korean_dialects/checkpoint_real/checkpoint_epoch_90.pth
2025-02-15 23:45:55,660 - Epoch [91/1000], Step [100/4367], Loss: 0.1348
2025-02-15 23:46:30,368 - Epoch [91/1000], Step [200/4367], Loss: 0.2093
2025-02-15 23:47:05,233 - Epoch [91/1000], Step [300/4367], Loss: 0.1789
2025-02-15 23:47:40,706 - Epoch [91/1000], Step [400/4367], Loss: 0.1587
2025-02-15 23:48:15,235 - Epoch [91/1000], Step [500/4367], Loss: 0.1434
2025-02-15 23:48:49,902 - Epoch [91/1000], Step [600/4367], Loss: 0.0818
2025-02-15 23:49:24,724 - Epoch [91/1000], Step [700/4367], Loss: 0.1418
2025-02-15 23:49:59,372 - Epoch [91/1000], Step [800/4367], Loss: 0.0566
2025-02-15 23:50:33,918 - Epoch [91/1000], Step [900/4367], Loss: 0.1052
2025-02-15 23:51:08,867 - Epoch [91/1000], Step [1000/4367], Loss: 0.0781
2025-02-15 23:51:43,756 - Epoch [91/1000], Step [1100/4367], Loss: 0.2146
2025-02-15 23:52:18,718 - Epoch [91/1000], Step [1200/4367], Loss: 0.0459
2025-02-15 23:52:53,577 - Epoch [91/1000], Step [1300/4367], Loss: 0.0742
2025-02-15 23:53:28,042 - Epoch [91/1000], Step [1400/4367], Loss: 0.3390
2025-02-15 23:54:02,974 - Epoch [91/1000], Step [1500/4367], Loss: 0.2505
2025-02-15 23:54:37,771 - Epoch [91/1000], Step [1600/4367], Loss: 0.0963
2025-02-15 23:55:12,200 - Epoch [91/1000], Step [1700/4367], Loss: 0.2161
2025-02-15 23:55:47,221 - Epoch [91/1000], Step [1800/4367], Loss: 0.0822
2025-02-15 23:56:21,766 - Epoch [91/1000], Step [1900/4367], Loss: 0.1528
2025-02-15 23:56:56,047 - Epoch [91/1000], Step [2000/4367], Loss: 0.2730
2025-02-15 23:57:30,227 - Epoch [91/1000], Step [2100/4367], Loss: 0.2402
2025-02-15 23:58:04,852 - Epoch [91/1000], Step [2200/4367], Loss: 0.0615
2025-02-15 23:58:39,319 - Epoch [91/1000], Step [2300/4367], Loss: 0.0569
2025-02-15 23:59:14,115 - Epoch [91/1000], Step [2400/4367], Loss: 0.3175
2025-02-15 23:59:49,121 - Epoch [91/1000], Step [2500/4367], Loss: 0.3345
2025-02-16 00:00:23,651 - Epoch [91/1000], Step [2600/4367], Loss: 0.0988
2025-02-16 00:00:58,254 - Epoch [91/1000], Step [2700/4367], Loss: 0.1757
2025-02-16 00:01:32,765 - Epoch [91/1000], Step [2800/4367], Loss: 0.0684
2025-02-16 00:02:07,044 - Epoch [91/1000], Step [2900/4367], Loss: 0.1420
2025-02-16 00:02:41,561 - Epoch [91/1000], Step [3000/4367], Loss: 0.0257
2025-02-16 00:03:16,330 - Epoch [91/1000], Step [3100/4367], Loss: 0.2003
2025-02-16 00:03:50,573 - Epoch [91/1000], Step [3200/4367], Loss: 0.0475
2025-02-16 00:04:25,258 - Epoch [91/1000], Step [3300/4367], Loss: 0.2093
2025-02-16 00:04:59,979 - Epoch [91/1000], Step [3400/4367], Loss: 0.0571
2025-02-16 00:05:34,877 - Epoch [91/1000], Step [3500/4367], Loss: 0.0416
2025-02-16 00:06:09,678 - Epoch [91/1000], Step [3600/4367], Loss: 0.0939
2025-02-16 00:06:44,333 - Epoch [91/1000], Step [3700/4367], Loss: 0.0874
2025-02-16 00:07:18,637 - Epoch [91/1000], Step [3800/4367], Loss: 0.2212
2025-02-16 00:07:53,597 - Epoch [91/1000], Step [3900/4367], Loss: 0.0941
2025-02-16 00:08:28,335 - Epoch [91/1000], Step [4000/4367], Loss: 0.1206
2025-02-16 00:09:03,103 - Epoch [91/1000], Step [4100/4367], Loss: 0.2305
2025-02-16 00:09:37,607 - Epoch [91/1000], Step [4200/4367], Loss: 0.0893
2025-02-16 00:10:12,141 - Epoch [91/1000], Step [4300/4367], Loss: 0.2147
2025-02-16 00:10:45,419 - Epoch [91/1000], Validation Step [100/1090], Val Loss: 0.0000
2025-02-16 00:10:54,624 - Epoch [91/1000], Validation Step [200/1090], Val Loss: 0.0000
2025-02-16 00:11:03,967 - Epoch [91/1000], Validation Step [300/1090], Val Loss: 0.2900
2025-02-16 00:11:13,630 - Epoch [91/1000], Validation Step [400/1090], Val Loss: 0.0610
2025-02-16 00:11:22,750 - Epoch [91/1000], Validation Step [500/1090], Val Loss: 0.4928
2025-02-16 00:11:32,278 - Epoch [91/1000], Validation Step [600/1090], Val Loss: 0.0919
2025-02-16 00:11:41,857 - Epoch [91/1000], Validation Step [700/1090], Val Loss: 0.0863
2025-02-16 00:11:50,643 - Epoch [91/1000], Validation Step [800/1090], Val Loss: 0.0040
2025-02-16 00:11:59,212 - Epoch [91/1000], Validation Step [900/1090], Val Loss: 0.0034
2025-02-16 00:12:08,491 - Epoch [91/1000], Validation Step [1000/1090], Val Loss: 0.0002
2025-02-16 00:12:17,108 - Epoch 91/1000, Train Loss: 0.1324, Val Loss: 0.1394, Accuracy: 94.89%
2025-02-16 00:12:53,208 - Epoch [92/1000], Step [100/4367], Loss: 0.1663
2025-02-16 00:13:28,019 - Epoch [92/1000], Step [200/4367], Loss: 0.1466
2025-02-16 00:14:03,247 - Epoch [92/1000], Step [300/4367], Loss: 0.0501
2025-02-16 00:14:38,090 - Epoch [92/1000], Step [400/4367], Loss: 0.1151
2025-02-16 00:15:12,995 - Epoch [92/1000], Step [500/4367], Loss: 0.2173
2025-02-16 00:15:47,500 - Epoch [92/1000], Step [600/4367], Loss: 0.2082
2025-02-16 00:16:22,484 - Epoch [92/1000], Step [700/4367], Loss: 0.0748
2025-02-16 00:16:57,327 - Epoch [92/1000], Step [800/4367], Loss: 0.0509
2025-02-16 00:17:32,410 - Epoch [92/1000], Step [900/4367], Loss: 0.0548
2025-02-16 00:18:06,191 - Epoch [92/1000], Step [1000/4367], Loss: 0.2897
2025-02-16 00:18:40,721 - Epoch [92/1000], Step [1100/4367], Loss: 0.0622
2025-02-16 00:19:14,700 - Epoch [92/1000], Step [1200/4367], Loss: 0.0770
2025-02-16 00:19:49,732 - Epoch [92/1000], Step [1300/4367], Loss: 0.1631
2025-02-16 00:20:24,479 - Epoch [92/1000], Step [1400/4367], Loss: 0.0837
2025-02-16 00:20:59,020 - Epoch [92/1000], Step [1500/4367], Loss: 0.0182
2025-02-16 00:21:33,842 - Epoch [92/1000], Step [1600/4367], Loss: 0.1852
2025-02-16 00:22:08,065 - Epoch [92/1000], Step [1700/4367], Loss: 0.2483
2025-02-16 00:22:42,991 - Epoch [92/1000], Step [1800/4367], Loss: 0.0870
2025-02-16 00:23:17,731 - Epoch [92/1000], Step [1900/4367], Loss: 0.1515
2025-02-16 00:23:52,996 - Epoch [92/1000], Step [2000/4367], Loss: 0.1429
2025-02-16 00:24:27,850 - Epoch [92/1000], Step [2100/4367], Loss: 0.1660
2025-02-16 00:25:02,566 - Epoch [92/1000], Step [2200/4367], Loss: 0.3038
2025-02-16 00:25:36,853 - Epoch [92/1000], Step [2300/4367], Loss: 0.1477
2025-02-16 00:26:11,340 - Epoch [92/1000], Step [2400/4367], Loss: 0.1432
2025-02-16 00:26:46,153 - Epoch [92/1000], Step [2500/4367], Loss: 0.1234
2025-02-16 00:27:20,526 - Epoch [92/1000], Step [2600/4367], Loss: 0.0707
2025-02-16 00:27:55,522 - Epoch [92/1000], Step [2700/4367], Loss: 0.0386
2025-02-16 00:28:30,582 - Epoch [92/1000], Step [2800/4367], Loss: 0.1756
2025-02-16 00:29:04,921 - Epoch [92/1000], Step [2900/4367], Loss: 0.0550
2025-02-16 00:29:39,663 - Epoch [92/1000], Step [3000/4367], Loss: 0.1132
2025-02-16 00:30:14,137 - Epoch [92/1000], Step [3100/4367], Loss: 0.1950
2025-02-16 00:30:49,150 - Epoch [92/1000], Step [3200/4367], Loss: 0.1551
2025-02-16 00:31:24,024 - Epoch [92/1000], Step [3300/4367], Loss: 0.2027
2025-02-16 00:31:58,695 - Epoch [92/1000], Step [3400/4367], Loss: 0.0635
2025-02-16 00:32:33,372 - Epoch [92/1000], Step [3500/4367], Loss: 0.1320
2025-02-16 00:33:07,487 - Epoch [92/1000], Step [3600/4367], Loss: 0.0996
2025-02-16 00:33:41,519 - Epoch [92/1000], Step [3700/4367], Loss: 0.4491
2025-02-16 00:34:16,091 - Epoch [92/1000], Step [3800/4367], Loss: 0.0986
2025-02-16 00:34:51,232 - Epoch [92/1000], Step [3900/4367], Loss: 0.1959
2025-02-16 00:35:26,123 - Epoch [92/1000], Step [4000/4367], Loss: 0.0646
2025-02-16 00:36:00,988 - Epoch [92/1000], Step [4100/4367], Loss: 0.0892
2025-02-16 00:36:35,898 - Epoch [92/1000], Step [4200/4367], Loss: 0.1398
2025-02-16 00:37:10,929 - Epoch [92/1000], Step [4300/4367], Loss: 0.1355
2025-02-16 00:37:44,220 - Epoch [92/1000], Validation Step [100/1090], Val Loss: 0.0002
2025-02-16 00:37:53,400 - Epoch [92/1000], Validation Step [200/1090], Val Loss: 0.0004
2025-02-16 00:38:02,743 - Epoch [92/1000], Validation Step [300/1090], Val Loss: 0.2444
2025-02-16 00:38:12,398 - Epoch [92/1000], Validation Step [400/1090], Val Loss: 0.0538
2025-02-16 00:38:21,499 - Epoch [92/1000], Validation Step [500/1090], Val Loss: 0.4717
2025-02-16 00:38:31,023 - Epoch [92/1000], Validation Step [600/1090], Val Loss: 0.1437
2025-02-16 00:38:40,582 - Epoch [92/1000], Validation Step [700/1090], Val Loss: 0.1480
2025-02-16 00:38:49,366 - Epoch [92/1000], Validation Step [800/1090], Val Loss: 0.0027
2025-02-16 00:38:57,920 - Epoch [92/1000], Validation Step [900/1090], Val Loss: 0.0029
2025-02-16 00:39:07,172 - Epoch [92/1000], Validation Step [1000/1090], Val Loss: 0.0000
2025-02-16 00:39:15,786 - Epoch 92/1000, Train Loss: 0.1316, Val Loss: 0.1413, Accuracy: 94.79%
2025-02-16 00:39:16,205 - Model saved at /data2/personal/sungjin/korean_dialects/checkpoint_real/checkpoint_epoch_92.pth
2025-02-16 00:39:51,948 - Epoch [93/1000], Step [100/4367], Loss: 0.0501
2025-02-16 00:40:26,464 - Epoch [93/1000], Step [200/4367], Loss: 0.2408
2025-02-16 00:41:00,886 - Epoch [93/1000], Step [300/4367], Loss: 0.1237
2025-02-16 00:41:35,889 - Epoch [93/1000], Step [400/4367], Loss: 0.1490
2025-02-16 00:42:10,591 - Epoch [93/1000], Step [500/4367], Loss: 0.0604
2025-02-16 00:42:45,977 - Epoch [93/1000], Step [600/4367], Loss: 0.1238
2025-02-16 00:43:20,527 - Epoch [93/1000], Step [700/4367], Loss: 0.0751
2025-02-16 00:43:55,881 - Epoch [93/1000], Step [800/4367], Loss: 0.1183
2025-02-16 00:44:30,681 - Epoch [93/1000], Step [900/4367], Loss: 0.1503
2025-02-16 00:45:04,984 - Epoch [93/1000], Step [1000/4367], Loss: 0.1825
2025-02-16 00:45:39,221 - Epoch [93/1000], Step [1100/4367], Loss: 0.0785
2025-02-16 00:46:13,978 - Epoch [93/1000], Step [1200/4367], Loss: 0.1638
2025-02-16 00:46:48,364 - Epoch [93/1000], Step [1300/4367], Loss: 0.0280
2025-02-16 00:47:22,704 - Epoch [93/1000], Step [1400/4367], Loss: 0.0875
2025-02-16 00:47:57,025 - Epoch [93/1000], Step [1500/4367], Loss: 0.1398
2025-02-16 00:48:31,538 - Epoch [93/1000], Step [1600/4367], Loss: 0.0781
2025-02-16 00:49:06,328 - Epoch [93/1000], Step [1700/4367], Loss: 0.0684
2025-02-16 00:49:41,112 - Epoch [93/1000], Step [1800/4367], Loss: 0.0498
2025-02-16 00:50:16,212 - Epoch [93/1000], Step [1900/4367], Loss: 0.1256
2025-02-16 00:50:51,001 - Epoch [93/1000], Step [2000/4367], Loss: 0.0413
2025-02-16 00:51:25,698 - Epoch [93/1000], Step [2100/4367], Loss: 0.1479
2025-02-16 00:52:00,914 - Epoch [93/1000], Step [2200/4367], Loss: 0.0795
2025-02-16 00:52:35,800 - Epoch [93/1000], Step [2300/4367], Loss: 0.1820
2025-02-16 00:53:10,259 - Epoch [93/1000], Step [2400/4367], Loss: 0.1993
2025-02-16 00:53:44,759 - Epoch [93/1000], Step [2500/4367], Loss: 0.0798
2025-02-16 00:54:19,556 - Epoch [93/1000], Step [2600/4367], Loss: 0.1966
2025-02-16 00:54:53,790 - Epoch [93/1000], Step [2700/4367], Loss: 0.1878
2025-02-16 00:55:28,926 - Epoch [93/1000], Step [2800/4367], Loss: 0.0591
2025-02-16 00:56:03,873 - Epoch [93/1000], Step [2900/4367], Loss: 0.0830
2025-02-16 00:56:38,562 - Epoch [93/1000], Step [3000/4367], Loss: 0.0608
2025-02-16 00:57:13,558 - Epoch [93/1000], Step [3100/4367], Loss: 0.0345
2025-02-16 00:57:48,460 - Epoch [93/1000], Step [3200/4367], Loss: 0.0878
2025-02-16 00:58:22,941 - Epoch [93/1000], Step [3300/4367], Loss: 0.1081
2025-02-16 00:58:57,284 - Epoch [93/1000], Step [3400/4367], Loss: 0.1077
2025-02-16 00:59:32,125 - Epoch [93/1000], Step [3500/4367], Loss: 0.0854
2025-02-16 01:00:07,227 - Epoch [93/1000], Step [3600/4367], Loss: 0.1504
2025-02-16 01:00:42,127 - Epoch [93/1000], Step [3700/4367], Loss: 0.0282
2025-02-16 01:01:16,783 - Epoch [93/1000], Step [3800/4367], Loss: 0.0555
2025-02-16 01:01:51,321 - Epoch [93/1000], Step [3900/4367], Loss: 0.0453
2025-02-16 01:02:25,753 - Epoch [93/1000], Step [4000/4367], Loss: 0.1109
2025-02-16 01:03:00,647 - Epoch [93/1000], Step [4100/4367], Loss: 0.0929
2025-02-16 01:03:35,860 - Epoch [93/1000], Step [4200/4367], Loss: 0.1297
2025-02-16 01:04:10,556 - Epoch [93/1000], Step [4300/4367], Loss: 0.2891
2025-02-16 01:04:43,335 - Epoch [93/1000], Validation Step [100/1090], Val Loss: 0.0010
2025-02-16 01:04:52,537 - Epoch [93/1000], Validation Step [200/1090], Val Loss: 0.0023
2025-02-16 01:05:01,885 - Epoch [93/1000], Validation Step [300/1090], Val Loss: 0.2252
2025-02-16 01:05:11,558 - Epoch [93/1000], Validation Step [400/1090], Val Loss: 0.0436
2025-02-16 01:05:20,666 - Epoch [93/1000], Validation Step [500/1090], Val Loss: 0.3851
2025-02-16 01:05:30,208 - Epoch [93/1000], Validation Step [600/1090], Val Loss: 0.1495
2025-02-16 01:05:39,784 - Epoch [93/1000], Validation Step [700/1090], Val Loss: 0.1536
2025-02-16 01:05:48,578 - Epoch [93/1000], Validation Step [800/1090], Val Loss: 0.0079
2025-02-16 01:05:57,140 - Epoch [93/1000], Validation Step [900/1090], Val Loss: 0.0078
2025-02-16 01:06:06,406 - Epoch [93/1000], Validation Step [1000/1090], Val Loss: 0.0001
2025-02-16 01:06:15,029 - Epoch 93/1000, Train Loss: 0.1313, Val Loss: 0.1428, Accuracy: 94.73%
2025-02-16 01:06:50,559 - Epoch [94/1000], Step [100/4367], Loss: 0.2885
2025-02-16 01:07:24,961 - Epoch [94/1000], Step [200/4367], Loss: 0.1726
2025-02-16 01:07:59,265 - Epoch [94/1000], Step [300/4367], Loss: 0.1539
2025-02-16 01:08:34,210 - Epoch [94/1000], Step [400/4367], Loss: 0.0487
2025-02-16 01:09:09,322 - Epoch [94/1000], Step [500/4367], Loss: 0.0516
2025-02-16 01:09:44,047 - Epoch [94/1000], Step [600/4367], Loss: 0.0809
2025-02-16 01:10:19,028 - Epoch [94/1000], Step [700/4367], Loss: 0.0772
2025-02-16 01:10:54,100 - Epoch [94/1000], Step [800/4367], Loss: 0.2714
2025-02-16 01:11:29,023 - Epoch [94/1000], Step [900/4367], Loss: 0.2498
2025-02-16 01:12:03,578 - Epoch [94/1000], Step [1000/4367], Loss: 0.1007
2025-02-16 01:12:38,333 - Epoch [94/1000], Step [1100/4367], Loss: 0.1554
2025-02-16 01:13:12,415 - Epoch [94/1000], Step [1200/4367], Loss: 0.2027
2025-02-16 01:13:47,179 - Epoch [94/1000], Step [1300/4367], Loss: 0.2031
2025-02-16 01:14:22,179 - Epoch [94/1000], Step [1400/4367], Loss: 0.0440
2025-02-16 01:14:56,843 - Epoch [94/1000], Step [1500/4367], Loss: 0.1515
2025-02-16 01:15:31,346 - Epoch [94/1000], Step [1600/4367], Loss: 0.3430
2025-02-16 01:16:05,512 - Epoch [94/1000], Step [1700/4367], Loss: 0.0265
2025-02-16 01:16:39,685 - Epoch [94/1000], Step [1800/4367], Loss: 0.1439
2025-02-16 01:17:14,037 - Epoch [94/1000], Step [1900/4367], Loss: 0.0870
2025-02-16 01:17:48,751 - Epoch [94/1000], Step [2000/4367], Loss: 0.2193
2025-02-16 01:18:23,326 - Epoch [94/1000], Step [2100/4367], Loss: 0.1063
2025-02-16 01:18:58,376 - Epoch [94/1000], Step [2200/4367], Loss: 0.1013
2025-02-16 01:19:32,606 - Epoch [94/1000], Step [2300/4367], Loss: 0.0195
2025-02-16 01:20:07,555 - Epoch [94/1000], Step [2400/4367], Loss: 0.1445
2025-02-16 01:20:42,295 - Epoch [94/1000], Step [2500/4367], Loss: 0.1050
2025-02-16 01:21:17,104 - Epoch [94/1000], Step [2600/4367], Loss: 0.0625
2025-02-16 01:21:51,623 - Epoch [94/1000], Step [2700/4367], Loss: 0.2596
2025-02-16 01:22:26,380 - Epoch [94/1000], Step [2800/4367], Loss: 0.0407
2025-02-16 01:23:01,618 - Epoch [94/1000], Step [2900/4367], Loss: 0.1543
2025-02-16 01:23:36,292 - Epoch [94/1000], Step [3000/4367], Loss: 0.0583
2025-02-16 01:24:11,176 - Epoch [94/1000], Step [3100/4367], Loss: 0.1423
2025-02-16 01:24:45,993 - Epoch [94/1000], Step [3200/4367], Loss: 0.0660
2025-02-16 01:25:20,514 - Epoch [94/1000], Step [3300/4367], Loss: 0.2042
2025-02-16 01:25:55,796 - Epoch [94/1000], Step [3400/4367], Loss: 0.1068
2025-02-16 01:26:30,696 - Epoch [94/1000], Step [3500/4367], Loss: 0.1370
2025-02-16 01:27:05,295 - Epoch [94/1000], Step [3600/4367], Loss: 0.0550
2025-02-16 01:27:39,762 - Epoch [94/1000], Step [3700/4367], Loss: 0.0818
2025-02-16 01:28:14,373 - Epoch [94/1000], Step [3800/4367], Loss: 0.1139
2025-02-16 01:28:48,623 - Epoch [94/1000], Step [3900/4367], Loss: 0.1629
2025-02-16 01:29:23,121 - Epoch [94/1000], Step [4000/4367], Loss: 0.3623
2025-02-16 01:29:57,537 - Epoch [94/1000], Step [4100/4367], Loss: 0.0832
2025-02-16 01:30:31,927 - Epoch [94/1000], Step [4200/4367], Loss: 0.0635
2025-02-16 01:31:06,657 - Epoch [94/1000], Step [4300/4367], Loss: 0.1837
2025-02-16 01:31:39,891 - Epoch [94/1000], Validation Step [100/1090], Val Loss: 0.0001
2025-02-16 01:31:49,083 - Epoch [94/1000], Validation Step [200/1090], Val Loss: 0.0002
2025-02-16 01:31:58,418 - Epoch [94/1000], Validation Step [300/1090], Val Loss: 0.2518
2025-02-16 01:32:08,088 - Epoch [94/1000], Validation Step [400/1090], Val Loss: 0.0351
2025-02-16 01:32:17,202 - Epoch [94/1000], Validation Step [500/1090], Val Loss: 0.3981
2025-02-16 01:32:26,737 - Epoch [94/1000], Validation Step [600/1090], Val Loss: 0.1192
2025-02-16 01:32:36,307 - Epoch [94/1000], Validation Step [700/1090], Val Loss: 0.1249
2025-02-16 01:32:45,085 - Epoch [94/1000], Validation Step [800/1090], Val Loss: 0.0040
2025-02-16 01:32:53,656 - Epoch [94/1000], Validation Step [900/1090], Val Loss: 0.0039
2025-02-16 01:33:02,918 - Epoch [94/1000], Validation Step [1000/1090], Val Loss: 0.0001
2025-02-16 01:33:11,498 - Epoch 94/1000, Train Loss: 0.1312, Val Loss: 0.1397, Accuracy: 94.88%
2025-02-16 01:33:11,898 - Model saved at /data2/personal/sungjin/korean_dialects/checkpoint_real/checkpoint_epoch_94.pth
2025-02-16 01:33:48,152 - Epoch [95/1000], Step [100/4367], Loss: 0.3200
2025-02-16 01:34:23,013 - Epoch [95/1000], Step [200/4367], Loss: 0.1886
2025-02-16 01:34:57,639 - Epoch [95/1000], Step [300/4367], Loss: 0.1058
2025-02-16 01:35:32,435 - Epoch [95/1000], Step [400/4367], Loss: 0.1722
2025-02-16 01:36:07,077 - Epoch [95/1000], Step [500/4367], Loss: 0.1559
2025-02-16 01:36:41,787 - Epoch [95/1000], Step [600/4367], Loss: 0.0279
2025-02-16 01:37:16,403 - Epoch [95/1000], Step [700/4367], Loss: 0.3959
2025-02-16 01:37:51,010 - Epoch [95/1000], Step [800/4367], Loss: 0.1259
2025-02-16 01:38:25,667 - Epoch [95/1000], Step [900/4367], Loss: 0.0152
2025-02-16 01:39:00,069 - Epoch [95/1000], Step [1000/4367], Loss: 0.1114
2025-02-16 01:39:34,964 - Epoch [95/1000], Step [1100/4367], Loss: 0.0294
2025-02-16 01:40:09,298 - Epoch [95/1000], Step [1200/4367], Loss: 0.1462
2025-02-16 01:40:43,754 - Epoch [95/1000], Step [1300/4367], Loss: 0.0775
2025-02-16 01:41:18,572 - Epoch [95/1000], Step [1400/4367], Loss: 0.0499
2025-02-16 01:41:53,288 - Epoch [95/1000], Step [1500/4367], Loss: 0.1132
2025-02-16 01:42:28,274 - Epoch [95/1000], Step [1600/4367], Loss: 0.1587
2025-02-16 01:43:03,006 - Epoch [95/1000], Step [1700/4367], Loss: 0.2258
2025-02-16 01:43:37,832 - Epoch [95/1000], Step [1800/4367], Loss: 0.1997
2025-02-16 01:44:12,933 - Epoch [95/1000], Step [1900/4367], Loss: 0.0955
2025-02-16 01:44:47,904 - Epoch [95/1000], Step [2000/4367], Loss: 0.1018
2025-02-16 01:45:22,513 - Epoch [95/1000], Step [2100/4367], Loss: 0.0460
2025-02-16 01:45:56,915 - Epoch [95/1000], Step [2200/4367], Loss: 0.0572
2025-02-16 01:46:31,594 - Epoch [95/1000], Step [2300/4367], Loss: 0.0703
2025-02-16 01:47:05,911 - Epoch [95/1000], Step [2400/4367], Loss: 0.0313
2025-02-16 01:47:40,646 - Epoch [95/1000], Step [2500/4367], Loss: 0.0914
2025-02-16 01:48:14,899 - Epoch [95/1000], Step [2600/4367], Loss: 0.0321
2025-02-16 01:48:49,223 - Epoch [95/1000], Step [2700/4367], Loss: 0.0610
2025-02-16 01:49:23,964 - Epoch [95/1000], Step [2800/4367], Loss: 0.0517
2025-02-16 01:49:58,794 - Epoch [95/1000], Step [2900/4367], Loss: 0.0780
2025-02-16 01:50:33,719 - Epoch [95/1000], Step [3000/4367], Loss: 0.1451
2025-02-16 01:51:08,325 - Epoch [95/1000], Step [3100/4367], Loss: 0.1577
2025-02-16 01:51:43,065 - Epoch [95/1000], Step [3200/4367], Loss: 0.1384
2025-02-16 01:52:18,192 - Epoch [95/1000], Step [3300/4367], Loss: 0.0441
2025-02-16 01:52:53,284 - Epoch [95/1000], Step [3400/4367], Loss: 0.0890
2025-02-16 01:53:27,949 - Epoch [95/1000], Step [3500/4367], Loss: 0.0909
2025-02-16 01:54:02,544 - Epoch [95/1000], Step [3600/4367], Loss: 0.2067
2025-02-16 01:54:37,106 - Epoch [95/1000], Step [3700/4367], Loss: 0.0921
2025-02-16 01:55:12,154 - Epoch [95/1000], Step [3800/4367], Loss: 0.0416
2025-02-16 01:55:46,420 - Epoch [95/1000], Step [3900/4367], Loss: 0.2301
2025-02-16 01:56:21,279 - Epoch [95/1000], Step [4000/4367], Loss: 0.2112
2025-02-16 01:56:56,613 - Epoch [95/1000], Step [4100/4367], Loss: 0.1902
2025-02-16 01:57:31,140 - Epoch [95/1000], Step [4200/4367], Loss: 0.3694
2025-02-16 01:58:05,239 - Epoch [95/1000], Step [4300/4367], Loss: 0.0802
2025-02-16 01:58:37,879 - Epoch [95/1000], Validation Step [100/1090], Val Loss: 0.0003
2025-02-16 01:58:47,016 - Epoch [95/1000], Validation Step [200/1090], Val Loss: 0.0010
2025-02-16 01:58:56,297 - Epoch [95/1000], Validation Step [300/1090], Val Loss: 0.2517
2025-02-16 01:59:05,900 - Epoch [95/1000], Validation Step [400/1090], Val Loss: 0.0499
2025-02-16 01:59:14,954 - Epoch [95/1000], Validation Step [500/1090], Val Loss: 0.4171
2025-02-16 01:59:24,435 - Epoch [95/1000], Validation Step [600/1090], Val Loss: 0.1142
2025-02-16 01:59:33,950 - Epoch [95/1000], Validation Step [700/1090], Val Loss: 0.1103
2025-02-16 01:59:42,687 - Epoch [95/1000], Validation Step [800/1090], Val Loss: 0.0063
2025-02-16 01:59:51,194 - Epoch [95/1000], Validation Step [900/1090], Val Loss: 0.0063
2025-02-16 02:00:00,389 - Epoch [95/1000], Validation Step [1000/1090], Val Loss: 0.0000
2025-02-16 02:00:08,941 - Epoch 95/1000, Train Loss: 0.1317, Val Loss: 0.1407, Accuracy: 94.87%
2025-02-16 02:00:44,716 - Epoch [96/1000], Step [100/4367], Loss: 0.0633
2025-02-16 02:01:19,476 - Epoch [96/1000], Step [200/4367], Loss: 0.1080
2025-02-16 02:01:54,105 - Epoch [96/1000], Step [300/4367], Loss: 0.2123
2025-02-16 02:02:29,016 - Epoch [96/1000], Step [400/4367], Loss: 0.2010
2025-02-16 02:03:03,512 - Epoch [96/1000], Step [500/4367], Loss: 0.0403
2025-02-16 02:03:38,245 - Epoch [96/1000], Step [600/4367], Loss: 0.1332
2025-02-16 02:04:12,438 - Epoch [96/1000], Step [700/4367], Loss: 0.0918
2025-02-16 02:04:47,594 - Epoch [96/1000], Step [800/4367], Loss: 0.1445
2025-02-16 02:05:22,743 - Epoch [96/1000], Step [900/4367], Loss: 0.2155
2025-02-16 02:05:57,787 - Epoch [96/1000], Step [1000/4367], Loss: 0.1911
2025-02-16 02:06:32,489 - Epoch [96/1000], Step [1100/4367], Loss: 0.0194
2025-02-16 02:07:07,192 - Epoch [96/1000], Step [1200/4367], Loss: 0.0342
2025-02-16 02:07:42,341 - Epoch [96/1000], Step [1300/4367], Loss: 0.2407
2025-02-16 02:08:16,606 - Epoch [96/1000], Step [1400/4367], Loss: 0.1346
2025-02-16 02:08:51,292 - Epoch [96/1000], Step [1500/4367], Loss: 0.1123
2025-02-16 02:09:25,887 - Epoch [96/1000], Step [1600/4367], Loss: 0.0457
2025-02-16 02:10:00,654 - Epoch [96/1000], Step [1700/4367], Loss: 0.0293
2025-02-16 02:10:35,363 - Epoch [96/1000], Step [1800/4367], Loss: 0.0421
2025-02-16 02:11:10,099 - Epoch [96/1000], Step [1900/4367], Loss: 0.2237
2025-02-16 02:11:44,874 - Epoch [96/1000], Step [2000/4367], Loss: 0.1741
2025-02-16 02:12:19,234 - Epoch [96/1000], Step [2100/4367], Loss: 0.1091
2025-02-16 02:12:53,519 - Epoch [96/1000], Step [2200/4367], Loss: 0.0595
2025-02-16 02:13:28,659 - Epoch [96/1000], Step [2300/4367], Loss: 0.1687
2025-02-16 02:14:03,295 - Epoch [96/1000], Step [2400/4367], Loss: 0.2094
2025-02-16 02:14:37,833 - Epoch [96/1000], Step [2500/4367], Loss: 0.0772
2025-02-16 02:15:12,277 - Epoch [96/1000], Step [2600/4367], Loss: 0.1727
2025-02-16 02:15:47,149 - Epoch [96/1000], Step [2700/4367], Loss: 0.1105
2025-02-16 02:16:21,563 - Epoch [96/1000], Step [2800/4367], Loss: 0.1092
2025-02-16 02:16:56,368 - Epoch [96/1000], Step [2900/4367], Loss: 0.0615
2025-02-16 02:17:30,741 - Epoch [96/1000], Step [3000/4367], Loss: 0.1148
2025-02-16 02:18:05,432 - Epoch [96/1000], Step [3100/4367], Loss: 0.2236
2025-02-16 02:18:40,346 - Epoch [96/1000], Step [3200/4367], Loss: 0.1084
2025-02-16 02:19:15,179 - Epoch [96/1000], Step [3300/4367], Loss: 0.1322
2025-02-16 02:19:49,959 - Epoch [96/1000], Step [3400/4367], Loss: 0.0974
2025-02-16 02:20:24,336 - Epoch [96/1000], Step [3500/4367], Loss: 0.1799
2025-02-16 02:20:58,572 - Epoch [96/1000], Step [3600/4367], Loss: 0.0202
2025-02-16 02:21:32,951 - Epoch [96/1000], Step [3700/4367], Loss: 0.3010
2025-02-16 02:22:07,924 - Epoch [96/1000], Step [3800/4367], Loss: 0.0457
2025-02-16 02:22:42,705 - Epoch [96/1000], Step [3900/4367], Loss: 0.0401
2025-02-16 02:23:17,412 - Epoch [96/1000], Step [4000/4367], Loss: 0.1314
2025-02-16 02:23:51,979 - Epoch [96/1000], Step [4100/4367], Loss: 0.0884
2025-02-16 02:24:26,802 - Epoch [96/1000], Step [4200/4367], Loss: 0.0930
2025-02-16 02:25:01,501 - Epoch [96/1000], Step [4300/4367], Loss: 0.0461
2025-02-16 02:25:34,551 - Epoch [96/1000], Validation Step [100/1090], Val Loss: 0.0001
2025-02-16 02:25:43,763 - Epoch [96/1000], Validation Step [200/1090], Val Loss: 0.0002
2025-02-16 02:25:53,104 - Epoch [96/1000], Validation Step [300/1090], Val Loss: 0.2624
2025-02-16 02:26:02,771 - Epoch [96/1000], Validation Step [400/1090], Val Loss: 0.0542
2025-02-16 02:26:11,880 - Epoch [96/1000], Validation Step [500/1090], Val Loss: 0.4607
2025-02-16 02:26:21,416 - Epoch [96/1000], Validation Step [600/1090], Val Loss: 0.1452
2025-02-16 02:26:30,984 - Epoch [96/1000], Validation Step [700/1090], Val Loss: 0.1254
2025-02-16 02:26:39,773 - Epoch [96/1000], Validation Step [800/1090], Val Loss: 0.0023
2025-02-16 02:26:48,331 - Epoch [96/1000], Validation Step [900/1090], Val Loss: 0.0023
2025-02-16 02:26:57,591 - Epoch [96/1000], Validation Step [1000/1090], Val Loss: 0.0000
2025-02-16 02:27:06,201 - Epoch 96/1000, Train Loss: 0.1317, Val Loss: 0.1395, Accuracy: 94.85%
2025-02-16 02:27:06,691 - Model saved at /data2/personal/sungjin/korean_dialects/checkpoint_real/checkpoint_epoch_96.pth
2025-02-16 02:27:42,071 - Epoch [97/1000], Step [100/4367], Loss: 0.1474
2025-02-16 02:28:16,341 - Epoch [97/1000], Step [200/4367], Loss: 0.1262
2025-02-16 02:28:50,489 - Epoch [97/1000], Step [300/4367], Loss: 0.1209
2025-02-16 02:29:25,572 - Epoch [97/1000], Step [400/4367], Loss: 0.0520
2025-02-16 02:29:59,889 - Epoch [97/1000], Step [500/4367], Loss: 0.1025
2025-02-16 02:30:35,013 - Epoch [97/1000], Step [600/4367], Loss: 0.1992
2025-02-16 02:31:09,638 - Epoch [97/1000], Step [700/4367], Loss: 0.0590
2025-02-16 02:31:44,390 - Epoch [97/1000], Step [800/4367], Loss: 0.1449
2025-02-16 02:32:19,192 - Epoch [97/1000], Step [900/4367], Loss: 0.0278
2025-02-16 02:32:54,042 - Epoch [97/1000], Step [1000/4367], Loss: 0.0860
2025-02-16 02:33:29,220 - Epoch [97/1000], Step [1100/4367], Loss: 0.3410
2025-02-16 02:34:04,003 - Epoch [97/1000], Step [1200/4367], Loss: 0.1918
2025-02-16 02:34:38,863 - Epoch [97/1000], Step [1300/4367], Loss: 0.2442
2025-02-16 02:35:13,897 - Epoch [97/1000], Step [1400/4367], Loss: 0.0330
2025-02-16 02:35:48,599 - Epoch [97/1000], Step [1500/4367], Loss: 0.2171
2025-02-16 02:36:23,580 - Epoch [97/1000], Step [1600/4367], Loss: 0.1514
2025-02-16 02:36:58,165 - Epoch [97/1000], Step [1700/4367], Loss: 0.0320
2025-02-16 02:37:32,665 - Epoch [97/1000], Step [1800/4367], Loss: 0.0532
2025-02-16 02:38:07,527 - Epoch [97/1000], Step [1900/4367], Loss: 0.2372
2025-02-16 02:38:42,010 - Epoch [97/1000], Step [2000/4367], Loss: 0.1034
2025-02-16 02:39:16,630 - Epoch [97/1000], Step [2100/4367], Loss: 0.2042
2025-02-16 02:39:50,830 - Epoch [97/1000], Step [2200/4367], Loss: 0.0385
2025-02-16 02:40:25,547 - Epoch [97/1000], Step [2300/4367], Loss: 0.0921
2025-02-16 02:40:59,946 - Epoch [97/1000], Step [2400/4367], Loss: 0.2025
2025-02-16 02:41:34,609 - Epoch [97/1000], Step [2500/4367], Loss: 0.0750
2025-02-16 02:42:09,164 - Epoch [97/1000], Step [2600/4367], Loss: 0.1198
2025-02-16 02:42:43,648 - Epoch [97/1000], Step [2700/4367], Loss: 0.1452
2025-02-16 02:43:18,445 - Epoch [97/1000], Step [2800/4367], Loss: 0.1967
2025-02-16 02:43:53,347 - Epoch [97/1000], Step [2900/4367], Loss: 0.2185
2025-02-16 02:44:27,834 - Epoch [97/1000], Step [3000/4367], Loss: 0.1667
2025-02-16 02:45:02,624 - Epoch [97/1000], Step [3100/4367], Loss: 0.1400
2025-02-16 02:45:36,828 - Epoch [97/1000], Step [3200/4367], Loss: 0.1269
2025-02-16 02:46:11,435 - Epoch [97/1000], Step [3300/4367], Loss: 0.1708
2025-02-16 02:46:46,031 - Epoch [97/1000], Step [3400/4367], Loss: 0.0520
2025-02-16 02:47:20,316 - Epoch [97/1000], Step [3500/4367], Loss: 0.2070
2025-02-16 02:47:55,551 - Epoch [97/1000], Step [3600/4367], Loss: 0.0717
2025-02-16 02:48:30,431 - Epoch [97/1000], Step [3700/4367], Loss: 0.1832
2025-02-16 02:49:05,437 - Epoch [97/1000], Step [3800/4367], Loss: 0.0622
2025-02-16 02:49:40,343 - Epoch [97/1000], Step [3900/4367], Loss: 0.2442
2025-02-16 02:50:14,975 - Epoch [97/1000], Step [4000/4367], Loss: 0.1610
2025-02-16 02:50:49,783 - Epoch [97/1000], Step [4100/4367], Loss: 0.0455
2025-02-16 02:51:24,030 - Epoch [97/1000], Step [4200/4367], Loss: 0.1163
2025-02-16 02:51:58,566 - Epoch [97/1000], Step [4300/4367], Loss: 0.0732
2025-02-16 02:52:31,850 - Epoch [97/1000], Validation Step [100/1090], Val Loss: 0.0001
2025-02-16 02:52:41,033 - Epoch [97/1000], Validation Step [200/1090], Val Loss: 0.0002
2025-02-16 02:52:50,365 - Epoch [97/1000], Validation Step [300/1090], Val Loss: 0.2550
2025-02-16 02:53:00,022 - Epoch [97/1000], Validation Step [400/1090], Val Loss: 0.0487
2025-02-16 02:53:09,131 - Epoch [97/1000], Validation Step [500/1090], Val Loss: 0.4346
2025-02-16 02:53:18,661 - Epoch [97/1000], Validation Step [600/1090], Val Loss: 0.1322
2025-02-16 02:53:28,220 - Epoch [97/1000], Validation Step [700/1090], Val Loss: 0.1272
2025-02-16 02:53:37,004 - Epoch [97/1000], Validation Step [800/1090], Val Loss: 0.0038
2025-02-16 02:53:45,561 - Epoch [97/1000], Validation Step [900/1090], Val Loss: 0.0036
2025-02-16 02:53:54,807 - Epoch [97/1000], Validation Step [1000/1090], Val Loss: 0.0001
2025-02-16 02:54:03,423 - Epoch 97/1000, Train Loss: 0.1329, Val Loss: 0.1392, Accuracy: 94.86%
2025-02-16 02:54:38,896 - Epoch [98/1000], Step [100/4367], Loss: 0.0569
2025-02-16 02:55:13,537 - Epoch [98/1000], Step [200/4367], Loss: 0.1178
2025-02-16 02:55:48,113 - Epoch [98/1000], Step [300/4367], Loss: 0.2286
2025-02-16 02:56:22,776 - Epoch [98/1000], Step [400/4367], Loss: 0.1341
2025-02-16 02:56:57,620 - Epoch [98/1000], Step [500/4367], Loss: 0.1528
2025-02-16 02:57:32,144 - Epoch [98/1000], Step [600/4367], Loss: 0.2534
2025-02-16 02:58:06,781 - Epoch [98/1000], Step [700/4367], Loss: 0.1994
2025-02-16 02:58:41,515 - Epoch [98/1000], Step [800/4367], Loss: 0.0508
2025-02-16 02:59:16,415 - Epoch [98/1000], Step [900/4367], Loss: 0.0745
2025-02-16 02:59:51,390 - Epoch [98/1000], Step [1000/4367], Loss: 0.0758
2025-02-16 03:00:26,136 - Epoch [98/1000], Step [1100/4367], Loss: 0.1500
2025-02-16 03:01:00,855 - Epoch [98/1000], Step [1200/4367], Loss: 0.2174
2025-02-16 03:01:35,670 - Epoch [98/1000], Step [1300/4367], Loss: 0.3347
2025-02-16 03:02:10,740 - Epoch [98/1000], Step [1400/4367], Loss: 0.0922
2025-02-16 03:02:45,517 - Epoch [98/1000], Step [1500/4367], Loss: 0.1290
2025-02-16 03:03:20,271 - Epoch [98/1000], Step [1600/4367], Loss: 0.2415
2025-02-16 03:03:54,817 - Epoch [98/1000], Step [1700/4367], Loss: 0.1949
2025-02-16 03:04:29,462 - Epoch [98/1000], Step [1800/4367], Loss: 0.2435
2025-02-16 03:05:04,392 - Epoch [98/1000], Step [1900/4367], Loss: 0.0635
2025-02-16 03:05:39,402 - Epoch [98/1000], Step [2000/4367], Loss: 0.0758
2025-02-16 03:06:14,095 - Epoch [98/1000], Step [2100/4367], Loss: 0.1693
2025-02-16 03:06:49,154 - Epoch [98/1000], Step [2200/4367], Loss: 0.1817
2025-02-16 03:07:24,289 - Epoch [98/1000], Step [2300/4367], Loss: 0.0284
2025-02-16 03:07:58,947 - Epoch [98/1000], Step [2400/4367], Loss: 0.1185
2025-02-16 03:08:33,661 - Epoch [98/1000], Step [2500/4367], Loss: 0.0599
2025-02-16 03:09:07,893 - Epoch [98/1000], Step [2600/4367], Loss: 0.0950
2025-02-16 03:09:42,507 - Epoch [98/1000], Step [2700/4367], Loss: 0.0572
2025-02-16 03:10:17,509 - Epoch [98/1000], Step [2800/4367], Loss: 0.1942
2025-02-16 03:10:52,311 - Epoch [98/1000], Step [2900/4367], Loss: 0.2376
2025-02-16 03:11:27,154 - Epoch [98/1000], Step [3000/4367], Loss: 0.2395
2025-02-16 03:12:01,685 - Epoch [98/1000], Step [3100/4367], Loss: 0.1567
2025-02-16 03:12:36,173 - Epoch [98/1000], Step [3200/4367], Loss: 0.2391
2025-02-16 03:13:11,233 - Epoch [98/1000], Step [3300/4367], Loss: 0.0234
2025-02-16 03:13:45,965 - Epoch [98/1000], Step [3400/4367], Loss: 0.0384
2025-02-16 03:14:20,023 - Epoch [98/1000], Step [3500/4367], Loss: 0.2632
2025-02-16 03:14:54,486 - Epoch [98/1000], Step [3600/4367], Loss: 0.0618
2025-02-16 03:15:29,415 - Epoch [98/1000], Step [3700/4367], Loss: 0.0410
2025-02-16 03:16:03,950 - Epoch [98/1000], Step [3800/4367], Loss: 0.0418
2025-02-16 03:16:38,569 - Epoch [98/1000], Step [3900/4367], Loss: 0.3466
2025-02-16 03:17:13,857 - Epoch [98/1000], Step [4000/4367], Loss: 0.1638
2025-02-16 03:17:48,502 - Epoch [98/1000], Step [4100/4367], Loss: 0.0656
2025-02-16 03:18:23,200 - Epoch [98/1000], Step [4200/4367], Loss: 0.1287
2025-02-16 03:18:58,073 - Epoch [98/1000], Step [4300/4367], Loss: 0.1721
2025-02-16 03:19:31,139 - Epoch [98/1000], Validation Step [100/1090], Val Loss: 0.0001
2025-02-16 03:19:40,337 - Epoch [98/1000], Validation Step [200/1090], Val Loss: 0.0003
2025-02-16 03:19:49,684 - Epoch [98/1000], Validation Step [300/1090], Val Loss: 0.2515
2025-02-16 03:19:59,344 - Epoch [98/1000], Validation Step [400/1090], Val Loss: 0.0374
2025-02-16 03:20:08,453 - Epoch [98/1000], Validation Step [500/1090], Val Loss: 0.4276
2025-02-16 03:20:17,987 - Epoch [98/1000], Validation Step [600/1090], Val Loss: 0.1384
2025-02-16 03:20:27,560 - Epoch [98/1000], Validation Step [700/1090], Val Loss: 0.1185
2025-02-16 03:20:36,344 - Epoch [98/1000], Validation Step [800/1090], Val Loss: 0.0037
2025-02-16 03:20:44,911 - Epoch [98/1000], Validation Step [900/1090], Val Loss: 0.0037
2025-02-16 03:20:54,170 - Epoch [98/1000], Validation Step [1000/1090], Val Loss: 0.0001
2025-02-16 03:21:02,793 - Epoch 98/1000, Train Loss: 0.1326, Val Loss: 0.1395, Accuracy: 94.86%
2025-02-16 03:21:03,251 - Model saved at /data2/personal/sungjin/korean_dialects/checkpoint_real/checkpoint_epoch_98.pth
2025-02-16 03:21:39,436 - Epoch [99/1000], Step [100/4367], Loss: 0.0512
2025-02-16 03:22:14,066 - Epoch [99/1000], Step [200/4367], Loss: 0.2930
2025-02-16 03:22:49,421 - Epoch [99/1000], Step [300/4367], Loss: 0.1073
2025-02-16 03:23:24,208 - Epoch [99/1000], Step [400/4367], Loss: 0.2100
2025-02-16 03:23:58,790 - Epoch [99/1000], Step [500/4367], Loss: 0.1813
2025-02-16 03:24:33,276 - Epoch [99/1000], Step [600/4367], Loss: 0.0529
2025-02-16 03:25:07,719 - Epoch [99/1000], Step [700/4367], Loss: 0.0511
2025-02-16 03:25:42,342 - Epoch [99/1000], Step [800/4367], Loss: 0.1228
2025-02-16 03:26:17,027 - Epoch [99/1000], Step [900/4367], Loss: 0.0415
2025-02-16 03:26:51,715 - Epoch [99/1000], Step [1000/4367], Loss: 0.2032
2025-02-16 03:27:26,026 - Epoch [99/1000], Step [1100/4367], Loss: 0.1911
2025-02-16 03:28:00,599 - Epoch [99/1000], Step [1200/4367], Loss: 0.1117
2025-02-16 03:28:35,562 - Epoch [99/1000], Step [1300/4367], Loss: 0.2586
2025-02-16 03:29:10,452 - Epoch [99/1000], Step [1400/4367], Loss: 0.1281
2025-02-16 03:29:45,304 - Epoch [99/1000], Step [1500/4367], Loss: 0.2619
2025-02-16 03:30:20,145 - Epoch [99/1000], Step [1600/4367], Loss: 0.0880
2025-02-16 03:30:55,092 - Epoch [99/1000], Step [1700/4367], Loss: 0.1692
2025-02-16 03:31:29,726 - Epoch [99/1000], Step [1800/4367], Loss: 0.0436
2025-02-16 03:32:04,507 - Epoch [99/1000], Step [1900/4367], Loss: 0.2340
2025-02-16 03:32:39,408 - Epoch [99/1000], Step [2000/4367], Loss: 0.2259
2025-02-16 03:33:13,911 - Epoch [99/1000], Step [2100/4367], Loss: 0.1204
2025-02-16 03:33:48,427 - Epoch [99/1000], Step [2200/4367], Loss: 0.0732
2025-02-16 03:34:23,025 - Epoch [99/1000], Step [2300/4367], Loss: 0.2626
2025-02-16 03:34:58,091 - Epoch [99/1000], Step [2400/4367], Loss: 0.1129
2025-02-16 03:35:32,901 - Epoch [99/1000], Step [2500/4367], Loss: 0.2034
2025-02-16 03:36:07,524 - Epoch [99/1000], Step [2600/4367], Loss: 0.0858
2025-02-16 03:36:42,365 - Epoch [99/1000], Step [2700/4367], Loss: 0.1260
2025-02-16 03:37:17,396 - Epoch [99/1000], Step [2800/4367], Loss: 0.3843
2025-02-16 03:37:52,120 - Epoch [99/1000], Step [2900/4367], Loss: 0.1007
2025-02-16 03:38:26,454 - Epoch [99/1000], Step [3000/4367], Loss: 0.1305
2025-02-16 03:39:01,055 - Epoch [99/1000], Step [3100/4367], Loss: 0.1282
2025-02-16 03:39:35,547 - Epoch [99/1000], Step [3200/4367], Loss: 0.0248
2025-02-16 03:40:10,322 - Epoch [99/1000], Step [3300/4367], Loss: 0.3643
2025-02-16 03:40:44,875 - Epoch [99/1000], Step [3400/4367], Loss: 0.0370
2025-02-16 03:41:19,847 - Epoch [99/1000], Step [3500/4367], Loss: 0.1949
2025-02-16 03:41:54,252 - Epoch [99/1000], Step [3600/4367], Loss: 0.0400
2025-02-16 03:42:29,035 - Epoch [99/1000], Step [3700/4367], Loss: 0.1866
2025-02-16 03:43:03,888 - Epoch [99/1000], Step [3800/4367], Loss: 0.1344
2025-02-16 03:43:38,400 - Epoch [99/1000], Step [3900/4367], Loss: 0.1597
2025-02-16 03:44:12,796 - Epoch [99/1000], Step [4000/4367], Loss: 0.0279
2025-02-16 03:44:47,731 - Epoch [99/1000], Step [4100/4367], Loss: 0.0630
2025-02-16 03:45:22,719 - Epoch [99/1000], Step [4200/4367], Loss: 0.0429
2025-02-16 03:45:57,854 - Epoch [99/1000], Step [4300/4367], Loss: 0.1180
2025-02-16 03:46:31,205 - Epoch [99/1000], Validation Step [100/1090], Val Loss: 0.0001
2025-02-16 03:46:40,403 - Epoch [99/1000], Validation Step [200/1090], Val Loss: 0.0002
2025-02-16 03:46:49,757 - Epoch [99/1000], Validation Step [300/1090], Val Loss: 0.2652
2025-02-16 03:46:59,424 - Epoch [99/1000], Validation Step [400/1090], Val Loss: 0.0415
2025-02-16 03:47:08,542 - Epoch [99/1000], Validation Step [500/1090], Val Loss: 0.4100
2025-02-16 03:47:18,076 - Epoch [99/1000], Validation Step [600/1090], Val Loss: 0.1157
2025-02-16 03:47:27,648 - Epoch [99/1000], Validation Step [700/1090], Val Loss: 0.1279
2025-02-16 03:47:36,437 - Epoch [99/1000], Validation Step [800/1090], Val Loss: 0.0046
2025-02-16 03:47:45,009 - Epoch [99/1000], Validation Step [900/1090], Val Loss: 0.0046
2025-02-16 03:47:54,267 - Epoch [99/1000], Validation Step [1000/1090], Val Loss: 0.0001
2025-02-16 03:48:02,895 - Epoch 99/1000, Train Loss: 0.1325, Val Loss: 0.1397, Accuracy: 94.88%
2025-02-16 03:48:38,844 - Epoch [100/1000], Step [100/4367], Loss: 0.0980
2025-02-16 03:49:13,314 - Epoch [100/1000], Step [200/4367], Loss: 0.0640
2025-02-16 03:49:48,137 - Epoch [100/1000], Step [300/4367], Loss: 0.0595
2025-02-16 03:50:22,752 - Epoch [100/1000], Step [400/4367], Loss: 0.1832
2025-02-16 03:50:57,385 - Epoch [100/1000], Step [500/4367], Loss: 0.0559
2025-02-16 03:51:31,908 - Epoch [100/1000], Step [600/4367], Loss: 0.1449
2025-02-16 03:52:06,696 - Epoch [100/1000], Step [700/4367], Loss: 0.1066
2025-02-16 03:52:41,868 - Epoch [100/1000], Step [800/4367], Loss: 0.1073
2025-02-16 03:53:16,326 - Epoch [100/1000], Step [900/4367], Loss: 0.2080
2025-02-16 03:53:51,216 - Epoch [100/1000], Step [1000/4367], Loss: 0.1322
2025-02-16 03:54:25,907 - Epoch [100/1000], Step [1100/4367], Loss: 0.0709
2025-02-16 03:55:01,008 - Epoch [100/1000], Step [1200/4367], Loss: 0.0656
2025-02-16 03:55:35,855 - Epoch [100/1000], Step [1300/4367], Loss: 0.0297
2025-02-16 03:56:10,556 - Epoch [100/1000], Step [1400/4367], Loss: 0.1681
2025-02-16 03:56:45,678 - Epoch [100/1000], Step [1500/4367], Loss: 0.1239
2025-02-16 03:57:20,572 - Epoch [100/1000], Step [1600/4367], Loss: 0.0766
2025-02-16 03:57:55,139 - Epoch [100/1000], Step [1700/4367], Loss: 0.0899
2025-02-16 03:58:29,869 - Epoch [100/1000], Step [1800/4367], Loss: 0.1685
2025-02-16 03:59:05,280 - Epoch [100/1000], Step [1900/4367], Loss: 0.1458
2025-02-16 03:59:39,800 - Epoch [100/1000], Step [2000/4367], Loss: 0.2383
2025-02-16 04:00:14,627 - Epoch [100/1000], Step [2100/4367], Loss: 0.2232
2025-02-16 04:00:49,380 - Epoch [100/1000], Step [2200/4367], Loss: 0.1597
2025-02-16 04:01:24,087 - Epoch [100/1000], Step [2300/4367], Loss: 0.2700
2025-02-16 04:01:58,770 - Epoch [100/1000], Step [2400/4367], Loss: 0.1573
2025-02-16 04:02:33,076 - Epoch [100/1000], Step [2500/4367], Loss: 0.0439
2025-02-16 04:03:08,092 - Epoch [100/1000], Step [2600/4367], Loss: 0.0635
2025-02-16 04:03:43,029 - Epoch [100/1000], Step [2700/4367], Loss: 0.2476
2025-02-16 04:04:17,552 - Epoch [100/1000], Step [2800/4367], Loss: 0.0501
2025-02-16 04:04:52,180 - Epoch [100/1000], Step [2900/4367], Loss: 0.1152
2025-02-16 04:05:27,337 - Epoch [100/1000], Step [3000/4367], Loss: 0.1568
2025-02-16 04:06:01,978 - Epoch [100/1000], Step [3100/4367], Loss: 0.0483
2025-02-16 04:06:36,571 - Epoch [100/1000], Step [3200/4367], Loss: 0.1464
2025-02-16 04:07:11,578 - Epoch [100/1000], Step [3300/4367], Loss: 0.1824
2025-02-16 04:07:46,009 - Epoch [100/1000], Step [3400/4367], Loss: 0.0526
2025-02-16 04:08:20,724 - Epoch [100/1000], Step [3500/4367], Loss: 0.0843
2025-02-16 04:08:56,020 - Epoch [100/1000], Step [3600/4367], Loss: 0.0344
2025-02-16 04:09:30,727 - Epoch [100/1000], Step [3700/4367], Loss: 0.1358
2025-02-16 04:10:05,225 - Epoch [100/1000], Step [3800/4367], Loss: 0.0618
2025-02-16 04:10:39,861 - Epoch [100/1000], Step [3900/4367], Loss: 0.0699
2025-02-16 04:11:14,651 - Epoch [100/1000], Step [4000/4367], Loss: 0.2830
2025-02-16 04:11:49,108 - Epoch [100/1000], Step [4100/4367], Loss: 0.0766
2025-02-16 04:12:23,763 - Epoch [100/1000], Step [4200/4367], Loss: 0.2795
2025-02-16 04:12:58,713 - Epoch [100/1000], Step [4300/4367], Loss: 0.0335
2025-02-16 04:13:31,896 - Epoch [100/1000], Validation Step [100/1090], Val Loss: 0.0002
2025-02-16 04:13:41,086 - Epoch [100/1000], Validation Step [200/1090], Val Loss: 0.0006
2025-02-16 04:13:50,425 - Epoch [100/1000], Validation Step [300/1090], Val Loss: 0.2529
2025-02-16 04:14:00,092 - Epoch [100/1000], Validation Step [400/1090], Val Loss: 0.0535
2025-02-16 04:14:09,206 - Epoch [100/1000], Validation Step [500/1090], Val Loss: 0.4532
2025-02-16 04:14:18,736 - Epoch [100/1000], Validation Step [600/1090], Val Loss: 0.1165
2025-02-16 04:14:28,307 - Epoch [100/1000], Validation Step [700/1090], Val Loss: 0.1150
2025-02-16 04:14:37,097 - Epoch [100/1000], Validation Step [800/1090], Val Loss: 0.0057
2025-02-16 04:14:45,664 - Epoch [100/1000], Validation Step [900/1090], Val Loss: 0.0050
2025-02-16 04:14:54,914 - Epoch [100/1000], Validation Step [1000/1090], Val Loss: 0.0002
2025-02-16 04:15:03,531 - Epoch 100/1000, Train Loss: 0.1324, Val Loss: 0.1427, Accuracy: 94.80%
2025-02-16 04:15:03,944 - Model saved at /data2/personal/sungjin/korean_dialects/checkpoint_real/checkpoint_epoch_100.pth
2025-02-16 04:15:40,225 - Epoch [101/1000], Step [100/4367], Loss: 0.1188
2025-02-16 04:16:14,823 - Epoch [101/1000], Step [200/4367], Loss: 0.0636
2025-02-16 04:16:49,624 - Epoch [101/1000], Step [300/4367], Loss: 0.2048
2025-02-16 04:17:24,383 - Epoch [101/1000], Step [400/4367], Loss: 0.1725
2025-02-16 04:17:58,757 - Epoch [101/1000], Step [500/4367], Loss: 0.0895
2025-02-16 04:18:33,786 - Epoch [101/1000], Step [600/4367], Loss: 0.0292
2025-02-16 04:19:08,438 - Epoch [101/1000], Step [700/4367], Loss: 0.1278
2025-02-16 04:19:42,974 - Epoch [101/1000], Step [800/4367], Loss: 0.0303
2025-02-16 04:20:17,614 - Epoch [101/1000], Step [900/4367], Loss: 0.1656
2025-02-16 04:20:52,362 - Epoch [101/1000], Step [1000/4367], Loss: 0.0507
2025-02-16 04:21:27,979 - Epoch [101/1000], Step [1100/4367], Loss: 0.0261
2025-02-16 04:22:02,693 - Epoch [101/1000], Step [1200/4367], Loss: 0.0806
2025-02-16 04:22:37,803 - Epoch [101/1000], Step [1300/4367], Loss: 0.1550
2025-02-16 04:23:12,349 - Epoch [101/1000], Step [1400/4367], Loss: 0.2340
2025-02-16 04:23:46,981 - Epoch [101/1000], Step [1500/4367], Loss: 0.0427
2025-02-16 04:24:21,590 - Epoch [101/1000], Step [1600/4367], Loss: 0.0743
2025-02-16 04:24:56,326 - Epoch [101/1000], Step [1700/4367], Loss: 0.7122
2025-02-16 04:25:30,759 - Epoch [101/1000], Step [1800/4367], Loss: 0.1345
2025-02-16 04:26:05,719 - Epoch [101/1000], Step [1900/4367], Loss: 0.1720
2025-02-16 04:26:40,317 - Epoch [101/1000], Step [2000/4367], Loss: 0.0833
2025-02-16 04:27:15,193 - Epoch [101/1000], Step [2100/4367], Loss: 0.1608
2025-02-16 04:27:49,761 - Epoch [101/1000], Step [2200/4367], Loss: 0.0432
2025-02-16 04:28:24,879 - Epoch [101/1000], Step [2300/4367], Loss: 0.1625
2025-02-16 04:28:59,864 - Epoch [101/1000], Step [2400/4367], Loss: 0.0786
2025-02-16 04:29:34,203 - Epoch [101/1000], Step [2500/4367], Loss: 0.0216
2025-02-16 04:30:08,791 - Epoch [101/1000], Step [2600/4367], Loss: 0.1104
2025-02-16 04:30:43,062 - Epoch [101/1000], Step [2700/4367], Loss: 0.1722
2025-02-16 04:31:17,377 - Epoch [101/1000], Step [2800/4367], Loss: 0.2012
2025-02-16 04:31:51,602 - Epoch [101/1000], Step [2900/4367], Loss: 0.2090
2025-02-16 04:32:26,263 - Epoch [101/1000], Step [3000/4367], Loss: 0.0565
2025-02-16 04:33:00,846 - Epoch [101/1000], Step [3100/4367], Loss: 0.0955
2025-02-16 04:33:35,669 - Epoch [101/1000], Step [3200/4367], Loss: 0.1831
2025-02-16 04:34:10,186 - Epoch [101/1000], Step [3300/4367], Loss: 0.1926
2025-02-16 04:34:44,935 - Epoch [101/1000], Step [3400/4367], Loss: 0.2007
2025-02-16 04:35:19,814 - Epoch [101/1000], Step [3500/4367], Loss: 0.1520
2025-02-16 04:35:54,041 - Epoch [101/1000], Step [3600/4367], Loss: 0.0469
2025-02-16 04:36:28,441 - Epoch [101/1000], Step [3700/4367], Loss: 0.0375
2025-02-16 04:37:03,040 - Epoch [101/1000], Step [3800/4367], Loss: 0.0331
2025-02-16 04:37:37,940 - Epoch [101/1000], Step [3900/4367], Loss: 0.1949
2025-02-16 04:38:13,098 - Epoch [101/1000], Step [4000/4367], Loss: 0.0706
2025-02-16 04:38:48,419 - Epoch [101/1000], Step [4100/4367], Loss: 0.1915
2025-02-16 04:39:23,224 - Epoch [101/1000], Step [4200/4367], Loss: 0.1307
2025-02-16 04:39:57,963 - Epoch [101/1000], Step [4300/4367], Loss: 0.1780
2025-02-16 04:40:31,052 - Epoch [101/1000], Validation Step [100/1090], Val Loss: 0.0003
2025-02-16 04:40:40,268 - Epoch [101/1000], Validation Step [200/1090], Val Loss: 0.0005
2025-02-16 04:40:49,624 - Epoch [101/1000], Validation Step [300/1090], Val Loss: 0.2565
2025-02-16 04:40:59,302 - Epoch [101/1000], Validation Step [400/1090], Val Loss: 0.0411
2025-02-16 04:41:08,418 - Epoch [101/1000], Validation Step [500/1090], Val Loss: 0.4025
2025-02-16 04:41:17,959 - Epoch [101/1000], Validation Step [600/1090], Val Loss: 0.1266
2025-02-16 04:41:27,534 - Epoch [101/1000], Validation Step [700/1090], Val Loss: 0.1162
2025-02-16 04:41:36,332 - Epoch [101/1000], Validation Step [800/1090], Val Loss: 0.0044
2025-02-16 04:41:44,905 - Epoch [101/1000], Validation Step [900/1090], Val Loss: 0.0047
2025-02-16 04:41:54,178 - Epoch [101/1000], Validation Step [1000/1090], Val Loss: 0.0000
2025-02-16 04:42:02,803 - Epoch 101/1000, Train Loss: 0.1330, Val Loss: 0.1410, Accuracy: 94.84%
2025-02-16 04:42:38,621 - Epoch [102/1000], Step [100/4367], Loss: 0.0766
2025-02-16 04:43:13,040 - Epoch [102/1000], Step [200/4367], Loss: 0.1361
2025-02-16 04:43:47,869 - Epoch [102/1000], Step [300/4367], Loss: 0.2510
2025-02-16 04:44:22,609 - Epoch [102/1000], Step [400/4367], Loss: 0.1976
2025-02-16 04:44:57,554 - Epoch [102/1000], Step [500/4367], Loss: 0.1385
2025-02-16 04:45:32,328 - Epoch [102/1000], Step [600/4367], Loss: 0.0914
2025-02-16 04:46:07,227 - Epoch [102/1000], Step [700/4367], Loss: 0.1152
2025-02-16 04:46:42,081 - Epoch [102/1000], Step [800/4367], Loss: 0.1085
2025-02-16 04:47:16,547 - Epoch [102/1000], Step [900/4367], Loss: 0.2034
2025-02-16 04:47:51,290 - Epoch [102/1000], Step [1000/4367], Loss: 0.0757
2025-02-16 04:48:25,290 - Epoch [102/1000], Step [1100/4367], Loss: 0.0256
2025-02-16 04:48:59,656 - Epoch [102/1000], Step [1200/4367], Loss: 0.0488
2025-02-16 04:49:34,637 - Epoch [102/1000], Step [1300/4367], Loss: 0.1164
2025-02-16 04:50:09,725 - Epoch [102/1000], Step [1400/4367], Loss: 0.0984
2025-02-16 04:50:44,522 - Epoch [102/1000], Step [1500/4367], Loss: 0.1021
2025-02-16 04:51:19,469 - Epoch [102/1000], Step [1600/4367], Loss: 0.1052
2025-02-16 04:51:54,302 - Epoch [102/1000], Step [1700/4367], Loss: 0.3089
2025-02-16 04:52:28,966 - Epoch [102/1000], Step [1800/4367], Loss: 0.1004
2025-02-16 04:53:03,530 - Epoch [102/1000], Step [1900/4367], Loss: 0.3722
2025-02-16 04:53:38,424 - Epoch [102/1000], Step [2000/4367], Loss: 0.2762
2025-02-16 04:54:13,032 - Epoch [102/1000], Step [2100/4367], Loss: 0.2097
2025-02-16 04:54:47,679 - Epoch [102/1000], Step [2200/4367], Loss: 0.0647
2025-02-16 04:55:22,708 - Epoch [102/1000], Step [2300/4367], Loss: 0.1826
2025-02-16 04:55:57,490 - Epoch [102/1000], Step [2400/4367], Loss: 0.0343
2025-02-16 04:56:31,700 - Epoch [102/1000], Step [2500/4367], Loss: 0.0938
2025-02-16 04:57:06,112 - Epoch [102/1000], Step [2600/4367], Loss: 0.2050
2025-02-16 04:57:40,469 - Epoch [102/1000], Step [2700/4367], Loss: 0.1409
2025-02-16 04:58:14,830 - Epoch [102/1000], Step [2800/4367], Loss: 0.1537
2025-02-16 04:58:49,612 - Epoch [102/1000], Step [2900/4367], Loss: 0.2071
2025-02-16 04:59:24,559 - Epoch [102/1000], Step [3000/4367], Loss: 0.1164
2025-02-16 04:59:59,155 - Epoch [102/1000], Step [3100/4367], Loss: 0.3334
2025-02-16 05:00:33,335 - Epoch [102/1000], Step [3200/4367], Loss: 0.1232
2025-02-16 05:01:07,912 - Epoch [102/1000], Step [3300/4367], Loss: 0.0843
2025-02-16 05:01:43,302 - Epoch [102/1000], Step [3400/4367], Loss: 0.0410
2025-02-16 05:02:18,246 - Epoch [102/1000], Step [3500/4367], Loss: 0.1275
2025-02-16 05:02:52,659 - Epoch [102/1000], Step [3600/4367], Loss: 0.1084
2025-02-16 05:03:27,293 - Epoch [102/1000], Step [3700/4367], Loss: 0.3429
2025-02-16 05:04:02,368 - Epoch [102/1000], Step [3800/4367], Loss: 0.1604
2025-02-16 05:04:37,244 - Epoch [102/1000], Step [3900/4367], Loss: 0.0603
2025-02-16 05:05:11,730 - Epoch [102/1000], Step [4000/4367], Loss: 0.1661
2025-02-16 05:05:46,477 - Epoch [102/1000], Step [4100/4367], Loss: 0.0955
2025-02-16 05:06:20,983 - Epoch [102/1000], Step [4200/4367], Loss: 0.0542
2025-02-16 05:06:55,699 - Epoch [102/1000], Step [4300/4367], Loss: 0.1458
2025-02-16 05:07:28,574 - Epoch [102/1000], Validation Step [100/1090], Val Loss: 0.0002
2025-02-16 05:07:37,761 - Epoch [102/1000], Validation Step [200/1090], Val Loss: 0.0006
2025-02-16 05:07:47,105 - Epoch [102/1000], Validation Step [300/1090], Val Loss: 0.2471
2025-02-16 05:07:56,769 - Epoch [102/1000], Validation Step [400/1090], Val Loss: 0.0443
2025-02-16 05:08:05,868 - Epoch [102/1000], Validation Step [500/1090], Val Loss: 0.4032
2025-02-16 05:08:15,393 - Epoch [102/1000], Validation Step [600/1090], Val Loss: 0.1270
2025-02-16 05:08:24,953 - Epoch [102/1000], Validation Step [700/1090], Val Loss: 0.1234
2025-02-16 05:08:33,733 - Epoch [102/1000], Validation Step [800/1090], Val Loss: 0.0052
2025-02-16 05:08:42,286 - Epoch [102/1000], Validation Step [900/1090], Val Loss: 0.0049
2025-02-16 05:08:51,529 - Epoch [102/1000], Validation Step [1000/1090], Val Loss: 0.0001
2025-02-16 05:09:00,144 - Epoch 102/1000, Train Loss: 0.1311, Val Loss: 0.1385, Accuracy: 94.88%
2025-02-16 05:09:00,548 - Model saved at /data2/personal/sungjin/korean_dialects/checkpoint_real/checkpoint_epoch_102.pth
2025-02-16 05:09:36,465 - Epoch [103/1000], Step [100/4367], Loss: 0.0929
2025-02-16 05:10:11,239 - Epoch [103/1000], Step [200/4367], Loss: 0.1433
2025-02-16 05:10:46,329 - Epoch [103/1000], Step [300/4367], Loss: 0.0365
2025-02-16 05:11:20,667 - Epoch [103/1000], Step [400/4367], Loss: 0.0597
2025-02-16 05:11:55,134 - Epoch [103/1000], Step [500/4367], Loss: 0.1462
2025-02-16 05:12:30,171 - Epoch [103/1000], Step [600/4367], Loss: 0.1228
2025-02-16 05:13:04,867 - Epoch [103/1000], Step [700/4367], Loss: 0.2347
2025-02-16 05:13:39,381 - Epoch [103/1000], Step [800/4367], Loss: 0.2347
2025-02-16 05:14:13,823 - Epoch [103/1000], Step [900/4367], Loss: 0.1038
2025-02-16 05:14:48,754 - Epoch [103/1000], Step [1000/4367], Loss: 0.0535
2025-02-16 05:15:22,788 - Epoch [103/1000], Step [1100/4367], Loss: 0.0553
2025-02-16 05:15:57,061 - Epoch [103/1000], Step [1200/4367], Loss: 0.0939
2025-02-16 05:16:31,692 - Epoch [103/1000], Step [1300/4367], Loss: 0.1719
2025-02-16 05:17:06,660 - Epoch [103/1000], Step [1400/4367], Loss: 0.0908
2025-02-16 05:17:41,412 - Epoch [103/1000], Step [1500/4367], Loss: 0.0619
2025-02-16 05:18:16,438 - Epoch [103/1000], Step [1600/4367], Loss: 0.1828
2025-02-16 05:18:51,250 - Epoch [103/1000], Step [1700/4367], Loss: 0.3511
2025-02-16 05:19:25,916 - Epoch [103/1000], Step [1800/4367], Loss: 0.0526
2025-02-16 05:20:01,110 - Epoch [103/1000], Step [1900/4367], Loss: 0.1091
2025-02-16 05:20:35,722 - Epoch [103/1000], Step [2000/4367], Loss: 0.0346
2025-02-16 05:21:10,531 - Epoch [103/1000], Step [2100/4367], Loss: 0.0723
2025-02-16 05:21:45,393 - Epoch [103/1000], Step [2200/4367], Loss: 0.0809
2025-02-16 05:22:19,903 - Epoch [103/1000], Step [2300/4367], Loss: 0.0409
2025-02-16 05:22:54,866 - Epoch [103/1000], Step [2400/4367], Loss: 0.0847
2025-02-16 05:23:29,508 - Epoch [103/1000], Step [2500/4367], Loss: 0.0795
2025-02-16 05:24:03,910 - Epoch [103/1000], Step [2600/4367], Loss: 0.0361
2025-02-16 05:24:38,314 - Epoch [103/1000], Step [2700/4367], Loss: 0.0596
2025-02-16 05:25:12,997 - Epoch [103/1000], Step [2800/4367], Loss: 0.0810
2025-02-16 05:25:48,019 - Epoch [103/1000], Step [2900/4367], Loss: 0.1720
2025-02-16 05:26:22,638 - Epoch [103/1000], Step [3000/4367], Loss: 0.0868
2025-02-16 05:26:57,802 - Epoch [103/1000], Step [3100/4367], Loss: 0.1524
2025-02-16 05:27:32,665 - Epoch [103/1000], Step [3200/4367], Loss: 0.0465
2025-02-16 05:28:07,591 - Epoch [103/1000], Step [3300/4367], Loss: 0.0545
2025-02-16 05:28:42,154 - Epoch [103/1000], Step [3400/4367], Loss: 0.0602
2025-02-16 05:29:16,676 - Epoch [103/1000], Step [3500/4367], Loss: 0.0253
2025-02-16 05:29:51,300 - Epoch [103/1000], Step [3600/4367], Loss: 0.0550
2025-02-16 05:30:25,963 - Epoch [103/1000], Step [3700/4367], Loss: 0.1352
2025-02-16 05:31:00,654 - Epoch [103/1000], Step [3800/4367], Loss: 0.1356
2025-02-16 05:31:35,080 - Epoch [103/1000], Step [3900/4367], Loss: 0.0863
2025-02-16 05:32:09,806 - Epoch [103/1000], Step [4000/4367], Loss: 0.1625
2025-02-16 05:32:44,420 - Epoch [103/1000], Step [4100/4367], Loss: 0.1790
2025-02-16 05:33:19,015 - Epoch [103/1000], Step [4200/4367], Loss: 0.2928
2025-02-16 05:33:53,456 - Epoch [103/1000], Step [4300/4367], Loss: 0.0955
2025-02-16 05:34:26,637 - Epoch [103/1000], Validation Step [100/1090], Val Loss: 0.0001
2025-02-16 05:34:35,846 - Epoch [103/1000], Validation Step [200/1090], Val Loss: 0.0004
2025-02-16 05:34:45,195 - Epoch [103/1000], Validation Step [300/1090], Val Loss: 0.2587
2025-02-16 05:34:54,869 - Epoch [103/1000], Validation Step [400/1090], Val Loss: 0.0667
2025-02-16 05:35:03,979 - Epoch [103/1000], Validation Step [500/1090], Val Loss: 0.5346
2025-02-16 05:35:13,523 - Epoch [103/1000], Validation Step [600/1090], Val Loss: 0.1508
2025-02-16 05:35:23,097 - Epoch [103/1000], Validation Step [700/1090], Val Loss: 0.1260
2025-02-16 05:35:31,893 - Epoch [103/1000], Validation Step [800/1090], Val Loss: 0.0022
2025-02-16 05:35:40,464 - Epoch [103/1000], Validation Step [900/1090], Val Loss: 0.0020
2025-02-16 05:35:49,733 - Epoch [103/1000], Validation Step [1000/1090], Val Loss: 0.0003
2025-02-16 05:35:58,357 - Epoch 103/1000, Train Loss: 0.1317, Val Loss: 0.1456, Accuracy: 94.69%
2025-02-16 05:36:34,038 - Epoch [104/1000], Step [100/4367], Loss: 0.1274
2025-02-16 05:37:08,518 - Epoch [104/1000], Step [200/4367], Loss: 0.1560
2025-02-16 05:37:43,380 - Epoch [104/1000], Step [300/4367], Loss: 0.1659
2025-02-16 05:38:18,277 - Epoch [104/1000], Step [400/4367], Loss: 0.1958
2025-02-16 05:38:53,400 - Epoch [104/1000], Step [500/4367], Loss: 0.2108
2025-02-16 05:39:27,676 - Epoch [104/1000], Step [600/4367], Loss: 0.0955
2025-02-16 05:40:02,598 - Epoch [104/1000], Step [700/4367], Loss: 0.1251
2025-02-16 05:40:37,089 - Epoch [104/1000], Step [800/4367], Loss: 0.1787
2025-02-16 05:41:12,064 - Epoch [104/1000], Step [900/4367], Loss: 0.0402
2025-02-16 05:41:46,838 - Epoch [104/1000], Step [1000/4367], Loss: 0.1311
2025-02-16 05:42:21,644 - Epoch [104/1000], Step [1100/4367], Loss: 0.1249
2025-02-16 05:42:56,423 - Epoch [104/1000], Step [1200/4367], Loss: 0.0109
2025-02-16 05:43:31,174 - Epoch [104/1000], Step [1300/4367], Loss: 0.0640
2025-02-16 05:44:05,994 - Epoch [104/1000], Step [1400/4367], Loss: 0.1910
2025-02-16 05:44:40,679 - Epoch [104/1000], Step [1500/4367], Loss: 0.0836
2025-02-16 05:45:15,520 - Epoch [104/1000], Step [1600/4367], Loss: 0.2930
2025-02-16 05:45:50,466 - Epoch [104/1000], Step [1700/4367], Loss: 0.0542
2025-02-16 05:46:25,229 - Epoch [104/1000], Step [1800/4367], Loss: 0.1976
2025-02-16 05:47:00,142 - Epoch [104/1000], Step [1900/4367], Loss: 0.2045
2025-02-16 05:47:34,969 - Epoch [104/1000], Step [2000/4367], Loss: 0.0999
2025-02-16 05:48:09,483 - Epoch [104/1000], Step [2100/4367], Loss: 0.1671
2025-02-16 05:48:44,197 - Epoch [104/1000], Step [2200/4367], Loss: 0.1494
2025-02-16 05:49:18,498 - Epoch [104/1000], Step [2300/4367], Loss: 0.1504
2025-02-16 05:49:53,184 - Epoch [104/1000], Step [2400/4367], Loss: 0.1600
2025-02-16 05:50:27,779 - Epoch [104/1000], Step [2500/4367], Loss: 0.2361
2025-02-16 05:51:02,660 - Epoch [104/1000], Step [2600/4367], Loss: 0.2492
2025-02-16 05:51:37,433 - Epoch [104/1000], Step [2700/4367], Loss: 0.1734
2025-02-16 05:52:12,645 - Epoch [104/1000], Step [2800/4367], Loss: 0.0345
2025-02-16 05:52:47,713 - Epoch [104/1000], Step [2900/4367], Loss: 0.6485
2025-02-16 05:53:22,134 - Epoch [104/1000], Step [3000/4367], Loss: 0.2972
2025-02-16 05:53:56,673 - Epoch [104/1000], Step [3100/4367], Loss: 0.0433
2025-02-16 05:54:31,261 - Epoch [104/1000], Step [3200/4367], Loss: 0.2872
2025-02-16 05:55:05,664 - Epoch [104/1000], Step [3300/4367], Loss: 0.4906
2025-02-16 05:55:39,977 - Epoch [104/1000], Step [3400/4367], Loss: 0.1310
2025-02-16 05:56:14,330 - Epoch [104/1000], Step [3500/4367], Loss: 0.1571
2025-02-16 05:56:49,377 - Epoch [104/1000], Step [3600/4367], Loss: 0.1414
2025-02-16 05:57:24,339 - Epoch [104/1000], Step [3700/4367], Loss: 0.2181
2025-02-16 05:57:59,111 - Epoch [104/1000], Step [3800/4367], Loss: 0.0312
2025-02-16 05:58:33,710 - Epoch [104/1000], Step [3900/4367], Loss: 0.0912
2025-02-16 05:59:08,194 - Epoch [104/1000], Step [4000/4367], Loss: 0.0550
2025-02-16 05:59:43,121 - Epoch [104/1000], Step [4100/4367], Loss: 0.1287
2025-02-16 06:00:18,068 - Epoch [104/1000], Step [4200/4367], Loss: 0.3420
2025-02-16 06:00:52,616 - Epoch [104/1000], Step [4300/4367], Loss: 0.0526
2025-02-16 06:01:25,571 - Epoch [104/1000], Validation Step [100/1090], Val Loss: 0.0001
2025-02-16 06:01:34,783 - Epoch [104/1000], Validation Step [200/1090], Val Loss: 0.0001
2025-02-16 06:01:44,131 - Epoch [104/1000], Validation Step [300/1090], Val Loss: 0.2598
2025-02-16 06:01:53,800 - Epoch [104/1000], Validation Step [400/1090], Val Loss: 0.0430
2025-02-16 06:02:02,917 - Epoch [104/1000], Validation Step [500/1090], Val Loss: 0.4269
2025-02-16 06:02:12,457 - Epoch [104/1000], Validation Step [600/1090], Val Loss: 0.1305
2025-02-16 06:02:22,036 - Epoch [104/1000], Validation Step [700/1090], Val Loss: 0.1192
2025-02-16 06:02:30,829 - Epoch [104/1000], Validation Step [800/1090], Val Loss: 0.0045
2025-02-16 06:02:39,401 - Epoch [104/1000], Validation Step [900/1090], Val Loss: 0.0031
2025-02-16 06:02:48,666 - Epoch [104/1000], Validation Step [1000/1090], Val Loss: 0.0000
2025-02-16 06:02:57,297 - Epoch 104/1000, Train Loss: 0.1318, Val Loss: 0.1388, Accuracy: 94.92%
2025-02-16 06:02:57,689 - Model saved at /data2/personal/sungjin/korean_dialects/checkpoint_real/checkpoint_epoch_104.pth
2025-02-16 06:03:33,540 - Epoch [105/1000], Step [100/4367], Loss: 0.1412
2025-02-16 06:04:07,995 - Epoch [105/1000], Step [200/4367], Loss: 0.3037
2025-02-16 06:04:43,132 - Epoch [105/1000], Step [300/4367], Loss: 0.1214
2025-02-16 06:05:18,050 - Epoch [105/1000], Step [400/4367], Loss: 0.0880
2025-02-16 06:05:52,911 - Epoch [105/1000], Step [500/4367], Loss: 0.0969
2025-02-16 06:06:27,745 - Epoch [105/1000], Step [600/4367], Loss: 0.1099
2025-02-16 06:07:02,714 - Epoch [105/1000], Step [700/4367], Loss: 0.1562
2025-02-16 06:07:37,447 - Epoch [105/1000], Step [800/4367], Loss: 0.1251
2025-02-16 06:08:12,219 - Epoch [105/1000], Step [900/4367], Loss: 0.1245
2025-02-16 06:08:47,013 - Epoch [105/1000], Step [1000/4367], Loss: 0.0978
2025-02-16 06:09:20,954 - Epoch [105/1000], Step [1100/4367], Loss: 0.0947
2025-02-16 06:09:55,520 - Epoch [105/1000], Step [1200/4367], Loss: 0.4397
2025-02-16 06:10:30,134 - Epoch [105/1000], Step [1300/4367], Loss: 0.2541
2025-02-16 06:11:05,233 - Epoch [105/1000], Step [1400/4367], Loss: 0.1007
2025-02-16 06:11:39,833 - Epoch [105/1000], Step [1500/4367], Loss: 0.1022
2025-02-16 06:12:14,344 - Epoch [105/1000], Step [1600/4367], Loss: 0.0555
2025-02-16 06:12:49,206 - Epoch [105/1000], Step [1700/4367], Loss: 0.2270
2025-02-16 06:13:23,528 - Epoch [105/1000], Step [1800/4367], Loss: 0.2018
2025-02-16 06:13:58,048 - Epoch [105/1000], Step [1900/4367], Loss: 0.2750
2025-02-16 06:14:32,735 - Epoch [105/1000], Step [2000/4367], Loss: 0.0635
2025-02-16 06:15:07,404 - Epoch [105/1000], Step [2100/4367], Loss: 0.1427
2025-02-16 06:15:41,948 - Epoch [105/1000], Step [2200/4367], Loss: 0.0916
2025-02-16 06:16:16,742 - Epoch [105/1000], Step [2300/4367], Loss: 0.0292
2025-02-16 06:16:51,565 - Epoch [105/1000], Step [2400/4367], Loss: 0.1324
2025-02-16 06:17:26,285 - Epoch [105/1000], Step [2500/4367], Loss: 0.0735
2025-02-16 06:18:00,816 - Epoch [105/1000], Step [2600/4367], Loss: 0.3684
2025-02-16 06:18:35,540 - Epoch [105/1000], Step [2700/4367], Loss: 0.0545
2025-02-16 06:19:09,425 - Epoch [105/1000], Step [2800/4367], Loss: 0.1833
2025-02-16 06:19:43,805 - Epoch [105/1000], Step [2900/4367], Loss: 0.1308
2025-02-16 06:20:18,775 - Epoch [105/1000], Step [3000/4367], Loss: 0.1443
2025-02-16 06:20:53,454 - Epoch [105/1000], Step [3100/4367], Loss: 0.1946
2025-02-16 06:21:27,738 - Epoch [105/1000], Step [3200/4367], Loss: 0.1077
2025-02-16 06:22:03,038 - Epoch [105/1000], Step [3300/4367], Loss: 0.4182
2025-02-16 06:22:38,147 - Epoch [105/1000], Step [3400/4367], Loss: 0.0268
2025-02-16 06:23:12,406 - Epoch [105/1000], Step [3500/4367], Loss: 0.2955
2025-02-16 06:23:47,003 - Epoch [105/1000], Step [3600/4367], Loss: 0.1312
2025-02-16 06:24:21,618 - Epoch [105/1000], Step [3700/4367], Loss: 0.1080
2025-02-16 06:24:56,813 - Epoch [105/1000], Step [3800/4367], Loss: 0.0596
2025-02-16 06:25:31,870 - Epoch [105/1000], Step [3900/4367], Loss: 0.2048
2025-02-16 06:26:06,372 - Epoch [105/1000], Step [4000/4367], Loss: 0.1222
2025-02-16 06:26:40,391 - Epoch [105/1000], Step [4100/4367], Loss: 0.1449
2025-02-16 06:27:15,224 - Epoch [105/1000], Step [4200/4367], Loss: 0.1748
2025-02-16 06:27:49,844 - Epoch [105/1000], Step [4300/4367], Loss: 0.1662
2025-02-16 06:28:22,949 - Epoch [105/1000], Validation Step [100/1090], Val Loss: 0.0001
2025-02-16 06:28:32,091 - Epoch [105/1000], Validation Step [200/1090], Val Loss: 0.0001
2025-02-16 06:28:41,382 - Epoch [105/1000], Validation Step [300/1090], Val Loss: 0.2360
2025-02-16 06:28:50,992 - Epoch [105/1000], Validation Step [400/1090], Val Loss: 0.0436
2025-02-16 06:29:00,043 - Epoch [105/1000], Validation Step [500/1090], Val Loss: 0.3831
2025-02-16 06:29:09,531 - Epoch [105/1000], Validation Step [600/1090], Val Loss: 0.1419
2025-02-16 06:29:19,085 - Epoch [105/1000], Validation Step [700/1090], Val Loss: 0.1389
2025-02-16 06:29:27,868 - Epoch [105/1000], Validation Step [800/1090], Val Loss: 0.0054
2025-02-16 06:29:36,424 - Epoch [105/1000], Validation Step [900/1090], Val Loss: 0.0050
2025-02-16 06:29:45,672 - Epoch [105/1000], Validation Step [1000/1090], Val Loss: 0.0000
2025-02-16 06:29:54,282 - Epoch 105/1000, Train Loss: 0.1319, Val Loss: 0.1393, Accuracy: 94.92%
2025-02-16 06:30:30,496 - Epoch [106/1000], Step [100/4367], Loss: 0.0887
2025-02-16 06:31:05,379 - Epoch [106/1000], Step [200/4367], Loss: 0.2792
2025-02-16 06:31:39,777 - Epoch [106/1000], Step [300/4367], Loss: 0.0943
2025-02-16 06:32:14,471 - Epoch [106/1000], Step [400/4367], Loss: 0.0639
2025-02-16 06:32:48,734 - Epoch [106/1000], Step [500/4367], Loss: 0.1286
2025-02-16 06:33:22,865 - Epoch [106/1000], Step [600/4367], Loss: 0.1268
2025-02-16 06:33:57,949 - Epoch [106/1000], Step [700/4367], Loss: 0.3568
2025-02-16 06:34:32,310 - Epoch [106/1000], Step [800/4367], Loss: 0.2173
2025-02-16 06:35:07,208 - Epoch [106/1000], Step [900/4367], Loss: 0.1135
2025-02-16 06:35:41,622 - Epoch [106/1000], Step [1000/4367], Loss: 0.2096
2025-02-16 06:36:16,282 - Epoch [106/1000], Step [1100/4367], Loss: 0.0859
2025-02-16 06:36:50,636 - Epoch [106/1000], Step [1200/4367], Loss: 0.0543
2025-02-16 06:37:25,310 - Epoch [106/1000], Step [1300/4367], Loss: 0.2483
2025-02-16 06:38:00,110 - Epoch [106/1000], Step [1400/4367], Loss: 0.1351
2025-02-16 06:38:34,604 - Epoch [106/1000], Step [1500/4367], Loss: 0.1272
2025-02-16 06:39:09,438 - Epoch [106/1000], Step [1600/4367], Loss: 0.1288
2025-02-16 06:39:43,547 - Epoch [106/1000], Step [1700/4367], Loss: 0.3326
2025-02-16 06:40:18,139 - Epoch [106/1000], Step [1800/4367], Loss: 0.1506
2025-02-16 06:40:52,843 - Epoch [106/1000], Step [1900/4367], Loss: 0.1553
2025-02-16 06:41:27,352 - Epoch [106/1000], Step [2000/4367], Loss: 0.1347
2025-02-16 06:42:01,690 - Epoch [106/1000], Step [2100/4367], Loss: 0.2575
2025-02-16 06:42:36,458 - Epoch [106/1000], Step [2200/4367], Loss: 0.0658
2025-02-16 06:43:11,128 - Epoch [106/1000], Step [2300/4367], Loss: 0.1510
2025-02-16 06:43:45,934 - Epoch [106/1000], Step [2400/4367], Loss: 0.1133
2025-02-16 06:44:20,652 - Epoch [106/1000], Step [2500/4367], Loss: 0.2392
2025-02-16 06:44:55,712 - Epoch [106/1000], Step [2600/4367], Loss: 0.0454
2025-02-16 06:45:30,302 - Epoch [106/1000], Step [2700/4367], Loss: 0.0886
2025-02-16 06:46:04,910 - Epoch [106/1000], Step [2800/4367], Loss: 0.1358
2025-02-16 06:46:39,819 - Epoch [106/1000], Step [2900/4367], Loss: 0.0701
2025-02-16 06:47:14,466 - Epoch [106/1000], Step [3000/4367], Loss: 0.1141
2025-02-16 06:47:49,405 - Epoch [106/1000], Step [3100/4367], Loss: 0.1608
2025-02-16 06:48:24,002 - Epoch [106/1000], Step [3200/4367], Loss: 0.1855
2025-02-16 06:48:58,369 - Epoch [106/1000], Step [3300/4367], Loss: 0.0494
2025-02-16 06:49:32,861 - Epoch [106/1000], Step [3400/4367], Loss: 0.1806
2025-02-16 06:50:07,156 - Epoch [106/1000], Step [3500/4367], Loss: 0.1443
2025-02-16 06:50:41,531 - Epoch [106/1000], Step [3600/4367], Loss: 0.1689
2025-02-16 06:51:15,762 - Epoch [106/1000], Step [3700/4367], Loss: 0.2066
2025-02-16 06:51:50,874 - Epoch [106/1000], Step [3800/4367], Loss: 0.2393
2025-02-16 06:52:25,838 - Epoch [106/1000], Step [3900/4367], Loss: 0.0916
2025-02-16 06:53:00,741 - Epoch [106/1000], Step [4000/4367], Loss: 0.1713
2025-02-16 06:53:35,300 - Epoch [106/1000], Step [4100/4367], Loss: 0.0892
2025-02-16 06:54:09,492 - Epoch [106/1000], Step [4200/4367], Loss: 0.1534
2025-02-16 06:54:44,136 - Epoch [106/1000], Step [4300/4367], Loss: 0.1096
2025-02-16 06:55:16,824 - Epoch [106/1000], Validation Step [100/1090], Val Loss: 0.0000
2025-02-16 06:55:26,021 - Epoch [106/1000], Validation Step [200/1090], Val Loss: 0.0001
2025-02-16 06:55:35,375 - Epoch [106/1000], Validation Step [300/1090], Val Loss: 0.2451
2025-02-16 06:55:45,039 - Epoch [106/1000], Validation Step [400/1090], Val Loss: 0.0439
2025-02-16 06:55:54,151 - Epoch [106/1000], Validation Step [500/1090], Val Loss: 0.4227
2025-02-16 06:56:03,684 - Epoch [106/1000], Validation Step [600/1090], Val Loss: 0.1344
2025-02-16 06:56:13,257 - Epoch [106/1000], Validation Step [700/1090], Val Loss: 0.1250
2025-02-16 06:56:22,043 - Epoch [106/1000], Validation Step [800/1090], Val Loss: 0.0040
2025-02-16 06:56:30,608 - Epoch [106/1000], Validation Step [900/1090], Val Loss: 0.0034
2025-02-16 06:56:39,862 - Epoch [106/1000], Validation Step [1000/1090], Val Loss: 0.0003
2025-02-16 06:56:48,478 - Epoch 106/1000, Train Loss: 0.1321, Val Loss: 0.1390, Accuracy: 94.87%
2025-02-16 06:56:48,995 - Model saved at /data2/personal/sungjin/korean_dialects/checkpoint_real/checkpoint_epoch_106.pth
2025-02-16 06:57:25,148 - Epoch [107/1000], Step [100/4367], Loss: 0.1782
2025-02-16 06:57:59,905 - Epoch [107/1000], Step [200/4367], Loss: 0.3007
2025-02-16 06:58:34,965 - Epoch [107/1000], Step [300/4367], Loss: 0.0452
2025-02-16 06:59:09,763 - Epoch [107/1000], Step [400/4367], Loss: 0.0626
2025-02-16 06:59:44,245 - Epoch [107/1000], Step [500/4367], Loss: 0.0761
2025-02-16 07:00:18,978 - Epoch [107/1000], Step [600/4367], Loss: 0.3552
2025-02-16 07:00:53,627 - Epoch [107/1000], Step [700/4367], Loss: 0.0862
2025-02-16 07:01:28,372 - Epoch [107/1000], Step [800/4367], Loss: 0.1261
2025-02-16 07:02:02,315 - Epoch [107/1000], Step [900/4367], Loss: 0.1851
2025-02-16 07:02:36,917 - Epoch [107/1000], Step [1000/4367], Loss: 0.0858
2025-02-16 07:03:11,629 - Epoch [107/1000], Step [1100/4367], Loss: 0.1653
2025-02-16 07:03:46,299 - Epoch [107/1000], Step [1200/4367], Loss: 0.0180
2025-02-16 07:04:20,402 - Epoch [107/1000], Step [1300/4367], Loss: 0.1279
2025-02-16 07:04:55,107 - Epoch [107/1000], Step [1400/4367], Loss: 0.0889
2025-02-16 07:05:29,563 - Epoch [107/1000], Step [1500/4367], Loss: 0.1084
2025-02-16 07:06:04,539 - Epoch [107/1000], Step [1600/4367], Loss: 0.1400
2025-02-16 07:06:39,453 - Epoch [107/1000], Step [1700/4367], Loss: 0.0749
2025-02-16 07:07:14,077 - Epoch [107/1000], Step [1800/4367], Loss: 0.0621
2025-02-16 07:07:49,138 - Epoch [107/1000], Step [1900/4367], Loss: 0.1559
2025-02-16 07:08:23,874 - Epoch [107/1000], Step [2000/4367], Loss: 0.2997
2025-02-16 07:08:58,909 - Epoch [107/1000], Step [2100/4367], Loss: 0.1408
2025-02-16 07:09:33,803 - Epoch [107/1000], Step [2200/4367], Loss: 0.0194
2025-02-16 07:10:07,782 - Epoch [107/1000], Step [2300/4367], Loss: 0.0889
2025-02-16 07:10:42,164 - Epoch [107/1000], Step [2400/4367], Loss: 0.1124
2025-02-16 07:11:17,036 - Epoch [107/1000], Step [2500/4367], Loss: 0.1296
2025-02-16 07:11:52,117 - Epoch [107/1000], Step [2600/4367], Loss: 0.1509
2025-02-16 07:12:26,877 - Epoch [107/1000], Step [2700/4367], Loss: 0.0270
2025-02-16 07:13:02,460 - Epoch [107/1000], Step [2800/4367], Loss: 0.0424
2025-02-16 07:13:36,626 - Epoch [107/1000], Step [2900/4367], Loss: 0.2623
2025-02-16 07:14:11,244 - Epoch [107/1000], Step [3000/4367], Loss: 0.1051
2025-02-16 07:14:45,633 - Epoch [107/1000], Step [3100/4367], Loss: 0.1088
2025-02-16 07:15:20,036 - Epoch [107/1000], Step [3200/4367], Loss: 0.1114
2025-02-16 07:15:54,928 - Epoch [107/1000], Step [3300/4367], Loss: 0.0366
2025-02-16 07:16:29,957 - Epoch [107/1000], Step [3400/4367], Loss: 0.0788
2025-02-16 07:17:04,478 - Epoch [107/1000], Step [3500/4367], Loss: 0.0763
2025-02-16 07:17:39,230 - Epoch [107/1000], Step [3600/4367], Loss: 0.0780
2025-02-16 07:18:14,101 - Epoch [107/1000], Step [3700/4367], Loss: 0.1290
2025-02-16 07:18:49,055 - Epoch [107/1000], Step [3800/4367], Loss: 0.1589
2025-02-16 07:19:24,157 - Epoch [107/1000], Step [3900/4367], Loss: 0.1558
2025-02-16 07:19:58,609 - Epoch [107/1000], Step [4000/4367], Loss: 0.2897
2025-02-16 07:20:33,700 - Epoch [107/1000], Step [4100/4367], Loss: 0.0446
2025-02-16 07:21:07,926 - Epoch [107/1000], Step [4200/4367], Loss: 0.0611
2025-02-16 07:21:42,577 - Epoch [107/1000], Step [4300/4367], Loss: 0.1353
2025-02-16 07:22:15,477 - Epoch [107/1000], Validation Step [100/1090], Val Loss: 0.0002
2025-02-16 07:22:24,687 - Epoch [107/1000], Validation Step [200/1090], Val Loss: 0.0005
2025-02-16 07:22:34,033 - Epoch [107/1000], Validation Step [300/1090], Val Loss: 0.2236
2025-02-16 07:22:43,706 - Epoch [107/1000], Validation Step [400/1090], Val Loss: 0.0608
2025-02-16 07:22:52,819 - Epoch [107/1000], Validation Step [500/1090], Val Loss: 0.4663
2025-02-16 07:23:02,360 - Epoch [107/1000], Validation Step [600/1090], Val Loss: 0.1734
2025-02-16 07:23:11,936 - Epoch [107/1000], Validation Step [700/1090], Val Loss: 0.1501
2025-02-16 07:23:20,722 - Epoch [107/1000], Validation Step [800/1090], Val Loss: 0.0037
2025-02-16 07:23:29,291 - Epoch [107/1000], Validation Step [900/1090], Val Loss: 0.0032
2025-02-16 07:23:38,555 - Epoch [107/1000], Validation Step [1000/1090], Val Loss: 0.0001
2025-02-16 07:23:47,173 - Epoch 107/1000, Train Loss: 0.1313, Val Loss: 0.1409, Accuracy: 94.83%
2025-02-16 07:24:22,737 - Epoch [108/1000], Step [100/4367], Loss: 0.0570
2025-02-16 07:24:57,386 - Epoch [108/1000], Step [200/4367], Loss: 0.2120
2025-02-16 07:25:32,490 - Epoch [108/1000], Step [300/4367], Loss: 0.2457
2025-02-16 07:26:07,169 - Epoch [108/1000], Step [400/4367], Loss: 0.0680
2025-02-16 07:26:42,135 - Epoch [108/1000], Step [500/4367], Loss: 0.1858
2025-02-16 07:27:16,352 - Epoch [108/1000], Step [600/4367], Loss: 0.1981
2025-02-16 07:27:51,471 - Epoch [108/1000], Step [700/4367], Loss: 0.0961
2025-02-16 07:28:25,978 - Epoch [108/1000], Step [800/4367], Loss: 0.0700
2025-02-16 07:29:00,683 - Epoch [108/1000], Step [900/4367], Loss: 0.1787
2025-02-16 07:29:35,832 - Epoch [108/1000], Step [1000/4367], Loss: 0.1479
2025-02-16 07:30:10,584 - Epoch [108/1000], Step [1100/4367], Loss: 0.1875
2025-02-16 07:30:44,999 - Epoch [108/1000], Step [1200/4367], Loss: 0.0570
2025-02-16 07:31:19,748 - Epoch [108/1000], Step [1300/4367], Loss: 0.0972
2025-02-16 07:31:54,148 - Epoch [108/1000], Step [1400/4367], Loss: 0.1506
2025-02-16 07:32:28,738 - Epoch [108/1000], Step [1500/4367], Loss: 0.0363
2025-02-16 07:33:03,608 - Epoch [108/1000], Step [1600/4367], Loss: 0.2238
2025-02-16 07:33:38,462 - Epoch [108/1000], Step [1700/4367], Loss: 0.1278
2025-02-16 07:34:12,819 - Epoch [108/1000], Step [1800/4367], Loss: 0.2627
2025-02-16 07:34:47,818 - Epoch [108/1000], Step [1900/4367], Loss: 0.1222
2025-02-16 07:35:22,368 - Epoch [108/1000], Step [2000/4367], Loss: 0.1659
2025-02-16 07:35:57,007 - Epoch [108/1000], Step [2100/4367], Loss: 0.0915
2025-02-16 07:36:31,658 - Epoch [108/1000], Step [2200/4367], Loss: 0.0860
2025-02-16 07:37:06,727 - Epoch [108/1000], Step [2300/4367], Loss: 0.1233
2025-02-16 07:37:41,740 - Epoch [108/1000], Step [2400/4367], Loss: 0.0971
2025-02-16 07:38:16,659 - Epoch [108/1000], Step [2500/4367], Loss: 0.1194
2025-02-16 07:38:50,688 - Epoch [108/1000], Step [2600/4367], Loss: 0.2420
2025-02-16 07:39:25,352 - Epoch [108/1000], Step [2700/4367], Loss: 0.1628
2025-02-16 07:39:59,736 - Epoch [108/1000], Step [2800/4367], Loss: 0.0918
2025-02-16 07:40:34,431 - Epoch [108/1000], Step [2900/4367], Loss: 0.0562
2025-02-16 07:41:09,166 - Epoch [108/1000], Step [3000/4367], Loss: 0.0490
2025-02-16 07:41:43,859 - Epoch [108/1000], Step [3100/4367], Loss: 0.1106
2025-02-16 07:42:18,765 - Epoch [108/1000], Step [3200/4367], Loss: 0.1223
2025-02-16 07:42:53,316 - Epoch [108/1000], Step [3300/4367], Loss: 0.1164
2025-02-16 07:43:28,145 - Epoch [108/1000], Step [3400/4367], Loss: 0.1185
2025-02-16 07:44:03,074 - Epoch [108/1000], Step [3500/4367], Loss: 0.0585
2025-02-16 07:44:37,760 - Epoch [108/1000], Step [3600/4367], Loss: 0.1335
2025-02-16 07:45:12,412 - Epoch [108/1000], Step [3700/4367], Loss: 0.0582
2025-02-16 07:45:47,743 - Epoch [108/1000], Step [3800/4367], Loss: 0.1309
2025-02-16 07:46:21,944 - Epoch [108/1000], Step [3900/4367], Loss: 0.1374
2025-02-16 07:46:56,776 - Epoch [108/1000], Step [4000/4367], Loss: 0.1228
2025-02-16 07:47:31,854 - Epoch [108/1000], Step [4100/4367], Loss: 0.1421
2025-02-16 07:48:06,693 - Epoch [108/1000], Step [4200/4367], Loss: 0.0677
2025-02-16 07:48:41,425 - Epoch [108/1000], Step [4300/4367], Loss: 0.1817
2025-02-16 07:49:14,098 - Epoch [108/1000], Validation Step [100/1090], Val Loss: 0.0001
2025-02-16 07:49:23,304 - Epoch [108/1000], Validation Step [200/1090], Val Loss: 0.0002
2025-02-16 07:49:32,654 - Epoch [108/1000], Validation Step [300/1090], Val Loss: 0.2316
2025-02-16 07:49:42,330 - Epoch [108/1000], Validation Step [400/1090], Val Loss: 0.0405
2025-02-16 07:49:51,445 - Epoch [108/1000], Validation Step [500/1090], Val Loss: 0.4123
2025-02-16 07:50:00,977 - Epoch [108/1000], Validation Step [600/1090], Val Loss: 0.1337
2025-02-16 07:50:10,541 - Epoch [108/1000], Validation Step [700/1090], Val Loss: 0.1275
2025-02-16 07:50:19,334 - Epoch [108/1000], Validation Step [800/1090], Val Loss: 0.0051
2025-02-16 07:50:27,897 - Epoch [108/1000], Validation Step [900/1090], Val Loss: 0.0049
2025-02-16 07:50:37,163 - Epoch [108/1000], Validation Step [1000/1090], Val Loss: 0.0000
2025-02-16 07:50:45,796 - Epoch 108/1000, Train Loss: 0.1322, Val Loss: 0.1398, Accuracy: 94.85%
2025-02-16 07:50:46,232 - Model saved at /data2/personal/sungjin/korean_dialects/checkpoint_real/checkpoint_epoch_108.pth
2025-02-16 07:51:21,971 - Epoch [109/1000], Step [100/4367], Loss: 0.2258
2025-02-16 07:51:56,957 - Epoch [109/1000], Step [200/4367], Loss: 0.1076
2025-02-16 07:52:31,742 - Epoch [109/1000], Step [300/4367], Loss: 0.1133
2025-02-16 07:53:06,360 - Epoch [109/1000], Step [400/4367], Loss: 0.0434
2025-02-16 07:53:40,829 - Epoch [109/1000], Step [500/4367], Loss: 0.2492
2025-02-16 07:54:15,659 - Epoch [109/1000], Step [600/4367], Loss: 0.0900
2025-02-16 07:54:50,113 - Epoch [109/1000], Step [700/4367], Loss: 0.1576
2025-02-16 07:55:25,022 - Epoch [109/1000], Step [800/4367], Loss: 0.3325
2025-02-16 07:55:59,654 - Epoch [109/1000], Step [900/4367], Loss: 0.1905
2025-02-16 07:56:34,641 - Epoch [109/1000], Step [1000/4367], Loss: 0.0407
2025-02-16 07:57:09,375 - Epoch [109/1000], Step [1100/4367], Loss: 0.3362
2025-02-16 07:57:44,381 - Epoch [109/1000], Step [1200/4367], Loss: 0.1313
2025-02-16 07:58:18,953 - Epoch [109/1000], Step [1300/4367], Loss: 0.1629
2025-02-16 07:58:53,695 - Epoch [109/1000], Step [1400/4367], Loss: 0.2347
2025-02-16 07:59:28,468 - Epoch [109/1000], Step [1500/4367], Loss: 0.3359
2025-02-16 08:00:03,046 - Epoch [109/1000], Step [1600/4367], Loss: 0.0604
2025-02-16 08:00:37,475 - Epoch [109/1000], Step [1700/4367], Loss: 0.1928
2025-02-16 08:01:11,584 - Epoch [109/1000], Step [1800/4367], Loss: 0.0519
2025-02-16 08:01:46,597 - Epoch [109/1000], Step [1900/4367], Loss: 0.1025
2025-02-16 08:02:21,115 - Epoch [109/1000], Step [2000/4367], Loss: 0.1127
2025-02-16 08:02:55,453 - Epoch [109/1000], Step [2100/4367], Loss: 0.0913
2025-02-16 08:03:30,092 - Epoch [109/1000], Step [2200/4367], Loss: 0.1866
2025-02-16 08:04:05,300 - Epoch [109/1000], Step [2300/4367], Loss: 0.2620
2025-02-16 08:04:40,203 - Epoch [109/1000], Step [2400/4367], Loss: 0.2064
2025-02-16 08:05:15,222 - Epoch [109/1000], Step [2500/4367], Loss: 0.1572
2025-02-16 08:05:49,797 - Epoch [109/1000], Step [2600/4367], Loss: 0.1296
2025-02-16 08:06:24,643 - Epoch [109/1000], Step [2700/4367], Loss: 0.1980
2025-02-16 08:06:59,074 - Epoch [109/1000], Step [2800/4367], Loss: 0.1204
2025-02-16 08:07:33,753 - Epoch [109/1000], Step [2900/4367], Loss: 0.0893
2025-02-16 08:08:08,440 - Epoch [109/1000], Step [3000/4367], Loss: 0.1116
2025-02-16 08:08:43,274 - Epoch [109/1000], Step [3100/4367], Loss: 0.0969
2025-02-16 08:09:18,177 - Epoch [109/1000], Step [3200/4367], Loss: 0.0691
2025-02-16 08:09:52,715 - Epoch [109/1000], Step [3300/4367], Loss: 0.0761
2025-02-16 08:10:27,346 - Epoch [109/1000], Step [3400/4367], Loss: 0.0755
2025-02-16 08:11:01,939 - Epoch [109/1000], Step [3500/4367], Loss: 0.1362
2025-02-16 08:11:36,628 - Epoch [109/1000], Step [3600/4367], Loss: 0.1881
2025-02-16 08:12:11,020 - Epoch [109/1000], Step [3700/4367], Loss: 0.2713
2025-02-16 08:12:45,354 - Epoch [109/1000], Step [3800/4367], Loss: 0.0180
2025-02-16 08:13:20,159 - Epoch [109/1000], Step [3900/4367], Loss: 0.1385
2025-02-16 08:13:54,526 - Epoch [109/1000], Step [4000/4367], Loss: 0.0582
2025-02-16 08:14:29,110 - Epoch [109/1000], Step [4100/4367], Loss: 0.1812
2025-02-16 08:15:03,914 - Epoch [109/1000], Step [4200/4367], Loss: 0.0465
2025-02-16 08:15:38,700 - Epoch [109/1000], Step [4300/4367], Loss: 0.0884
2025-02-16 08:16:11,466 - Epoch [109/1000], Validation Step [100/1090], Val Loss: 0.0000
2025-02-16 08:16:20,662 - Epoch [109/1000], Validation Step [200/1090], Val Loss: 0.0000
2025-02-16 08:16:30,005 - Epoch [109/1000], Validation Step [300/1090], Val Loss: 0.2901
2025-02-16 08:16:39,674 - Epoch [109/1000], Validation Step [400/1090], Val Loss: 0.0422
2025-02-16 08:16:48,781 - Epoch [109/1000], Validation Step [500/1090], Val Loss: 0.4165
2025-02-16 08:16:58,319 - Epoch [109/1000], Validation Step [600/1090], Val Loss: 0.0798
2025-02-16 08:17:07,888 - Epoch [109/1000], Validation Step [700/1090], Val Loss: 0.0852
2025-02-16 08:17:16,680 - Epoch [109/1000], Validation Step [800/1090], Val Loss: 0.0047
2025-02-16 08:17:25,249 - Epoch [109/1000], Validation Step [900/1090], Val Loss: 0.0044
2025-02-16 08:17:34,507 - Epoch [109/1000], Validation Step [1000/1090], Val Loss: 0.0005
2025-02-16 08:17:43,119 - Epoch 109/1000, Train Loss: 0.1333, Val Loss: 0.1417, Accuracy: 94.81%
2025-02-16 08:18:18,549 - Epoch [110/1000], Step [100/4367], Loss: 0.1125
2025-02-16 08:18:53,025 - Epoch [110/1000], Step [200/4367], Loss: 0.0565
2025-02-16 08:19:27,533 - Epoch [110/1000], Step [300/4367], Loss: 0.0941
2025-02-16 08:20:02,253 - Epoch [110/1000], Step [400/4367], Loss: 0.0981
2025-02-16 08:20:36,787 - Epoch [110/1000], Step [500/4367], Loss: 0.0890
2025-02-16 08:21:11,109 - Epoch [110/1000], Step [600/4367], Loss: 0.1987
2025-02-16 08:21:46,002 - Epoch [110/1000], Step [700/4367], Loss: 0.1805
2025-02-16 08:22:20,745 - Epoch [110/1000], Step [800/4367], Loss: 0.0145
2025-02-16 08:22:55,645 - Epoch [110/1000], Step [900/4367], Loss: 0.1747
2025-02-16 08:23:30,389 - Epoch [110/1000], Step [1000/4367], Loss: 0.1045
2025-02-16 08:24:04,719 - Epoch [110/1000], Step [1100/4367], Loss: 0.2401
2025-02-16 08:24:39,447 - Epoch [110/1000], Step [1200/4367], Loss: 0.0796
2025-02-16 08:25:13,967 - Epoch [110/1000], Step [1300/4367], Loss: 0.0574
2025-02-16 08:25:48,797 - Epoch [110/1000], Step [1400/4367], Loss: 0.1793
2025-02-16 08:26:23,570 - Epoch [110/1000], Step [1500/4367], Loss: 0.0768
2025-02-16 08:26:58,080 - Epoch [110/1000], Step [1600/4367], Loss: 0.0281
2025-02-16 08:27:33,126 - Epoch [110/1000], Step [1700/4367], Loss: 0.1935
2025-02-16 08:28:07,641 - Epoch [110/1000], Step [1800/4367], Loss: 0.1004
2025-02-16 08:28:42,625 - Epoch [110/1000], Step [1900/4367], Loss: 0.0566
2025-02-16 08:29:17,273 - Epoch [110/1000], Step [2000/4367], Loss: 0.0430
2025-02-16 08:29:51,744 - Epoch [110/1000], Step [2100/4367], Loss: 0.0917
2025-02-16 08:30:26,473 - Epoch [110/1000], Step [2200/4367], Loss: 0.0810
2025-02-16 08:31:01,127 - Epoch [110/1000], Step [2300/4367], Loss: 0.0464
2025-02-16 08:31:36,081 - Epoch [110/1000], Step [2400/4367], Loss: 0.2257
2025-02-16 08:32:11,013 - Epoch [110/1000], Step [2500/4367], Loss: 0.0665
2025-02-16 08:32:45,720 - Epoch [110/1000], Step [2600/4367], Loss: 0.2719
2025-02-16 08:33:20,538 - Epoch [110/1000], Step [2700/4367], Loss: 0.1281
2025-02-16 08:33:55,222 - Epoch [110/1000], Step [2800/4367], Loss: 0.0378
2025-02-16 08:34:30,508 - Epoch [110/1000], Step [2900/4367], Loss: 0.0234
2025-02-16 08:35:05,778 - Epoch [110/1000], Step [3000/4367], Loss: 0.1262
2025-02-16 08:35:40,280 - Epoch [110/1000], Step [3100/4367], Loss: 0.1494
2025-02-16 08:36:14,779 - Epoch [110/1000], Step [3200/4367], Loss: 0.1833
2025-02-16 08:36:49,649 - Epoch [110/1000], Step [3300/4367], Loss: 0.1610
2025-02-16 08:37:24,125 - Epoch [110/1000], Step [3400/4367], Loss: 0.1153
2025-02-16 08:37:58,840 - Epoch [110/1000], Step [3500/4367], Loss: 0.1640
2025-02-16 08:38:33,343 - Epoch [110/1000], Step [3600/4367], Loss: 0.2694
2025-02-16 08:39:08,238 - Epoch [110/1000], Step [3700/4367], Loss: 0.0570
2025-02-16 08:39:43,081 - Epoch [110/1000], Step [3800/4367], Loss: 0.2662
2025-02-16 08:40:17,762 - Epoch [110/1000], Step [3900/4367], Loss: 0.0667
2025-02-16 08:40:52,227 - Epoch [110/1000], Step [4000/4367], Loss: 0.1424
2025-02-16 08:41:26,991 - Epoch [110/1000], Step [4100/4367], Loss: 0.1951
2025-02-16 08:42:01,999 - Epoch [110/1000], Step [4200/4367], Loss: 0.2365
2025-02-16 08:42:37,058 - Epoch [110/1000], Step [4300/4367], Loss: 0.1234
2025-02-16 08:43:09,877 - Epoch [110/1000], Validation Step [100/1090], Val Loss: 0.0003
2025-02-16 08:43:19,075 - Epoch [110/1000], Validation Step [200/1090], Val Loss: 0.0005
2025-02-16 08:43:28,431 - Epoch [110/1000], Validation Step [300/1090], Val Loss: 0.2464
2025-02-16 08:43:38,101 - Epoch [110/1000], Validation Step [400/1090], Val Loss: 0.0469
2025-02-16 08:43:47,225 - Epoch [110/1000], Validation Step [500/1090], Val Loss: 0.4090
2025-02-16 08:43:56,766 - Epoch [110/1000], Validation Step [600/1090], Val Loss: 0.1265
2025-02-16 08:44:06,343 - Epoch [110/1000], Validation Step [700/1090], Val Loss: 0.1270
2025-02-16 08:44:15,134 - Epoch [110/1000], Validation Step [800/1090], Val Loss: 0.0051
2025-02-16 08:44:23,701 - Epoch [110/1000], Validation Step [900/1090], Val Loss: 0.0047
2025-02-16 08:44:32,961 - Epoch [110/1000], Validation Step [1000/1090], Val Loss: 0.0001
2025-02-16 08:44:41,601 - Epoch 110/1000, Train Loss: 0.1326, Val Loss: 0.1375, Accuracy: 94.93%
2025-02-16 08:44:42,037 - Model saved at /data2/personal/sungjin/korean_dialects/checkpoint_real/checkpoint_epoch_110.pth
2025-02-16 08:45:17,892 - Epoch [111/1000], Step [100/4367], Loss: 0.3468
2025-02-16 08:45:52,486 - Epoch [111/1000], Step [200/4367], Loss: 0.0734
2025-02-16 08:46:27,144 - Epoch [111/1000], Step [300/4367], Loss: 0.0401
2025-02-16 08:47:01,979 - Epoch [111/1000], Step [400/4367], Loss: 0.1568
2025-02-16 08:47:37,119 - Epoch [111/1000], Step [500/4367], Loss: 0.1146
2025-02-16 08:48:11,729 - Epoch [111/1000], Step [600/4367], Loss: 0.1194
2025-02-16 08:48:46,499 - Epoch [111/1000], Step [700/4367], Loss: 0.1995
2025-02-16 08:49:20,890 - Epoch [111/1000], Step [800/4367], Loss: 0.0543
2025-02-16 08:49:56,416 - Epoch [111/1000], Step [900/4367], Loss: 0.0601
2025-02-16 08:50:31,395 - Epoch [111/1000], Step [1000/4367], Loss: 0.2607
2025-02-16 08:51:05,693 - Epoch [111/1000], Step [1100/4367], Loss: 0.1476
2025-02-16 08:51:40,273 - Epoch [111/1000], Step [1200/4367], Loss: 0.2544
2025-02-16 08:52:14,424 - Epoch [111/1000], Step [1300/4367], Loss: 0.1600
2025-02-16 08:52:48,825 - Epoch [111/1000], Step [1400/4367], Loss: 0.2546
2025-02-16 08:53:24,262 - Epoch [111/1000], Step [1500/4367], Loss: 0.1547
2025-02-16 08:53:59,183 - Epoch [111/1000], Step [1600/4367], Loss: 0.2978
2025-02-16 08:54:34,086 - Epoch [111/1000], Step [1700/4367], Loss: 0.0840
2025-02-16 08:55:08,369 - Epoch [111/1000], Step [1800/4367], Loss: 0.0715
2025-02-16 08:55:42,852 - Epoch [111/1000], Step [1900/4367], Loss: 0.0754
2025-02-16 08:56:17,275 - Epoch [111/1000], Step [2000/4367], Loss: 0.1573
2025-02-16 08:56:52,253 - Epoch [111/1000], Step [2100/4367], Loss: 0.1084
2025-02-16 08:57:26,718 - Epoch [111/1000], Step [2200/4367], Loss: 0.0400
2025-02-16 08:58:01,285 - Epoch [111/1000], Step [2300/4367], Loss: 0.2818
2025-02-16 08:58:35,843 - Epoch [111/1000], Step [2400/4367], Loss: 0.2089
2025-02-16 08:59:11,231 - Epoch [111/1000], Step [2500/4367], Loss: 0.1065
2025-02-16 08:59:46,067 - Epoch [111/1000], Step [2600/4367], Loss: 0.2200
2025-02-16 09:00:20,838 - Epoch [111/1000], Step [2700/4367], Loss: 0.2401
2025-02-16 09:00:55,147 - Epoch [111/1000], Step [2800/4367], Loss: 0.1086
2025-02-16 09:01:29,913 - Epoch [111/1000], Step [2900/4367], Loss: 0.0895
2025-02-16 09:02:04,528 - Epoch [111/1000], Step [3000/4367], Loss: 0.0953
2025-02-16 09:02:38,785 - Epoch [111/1000], Step [3100/4367], Loss: 0.1898
2025-02-16 09:03:13,590 - Epoch [111/1000], Step [3200/4367], Loss: 0.3078
2025-02-16 09:03:48,316 - Epoch [111/1000], Step [3300/4367], Loss: 0.1854
2025-02-16 09:04:23,352 - Epoch [111/1000], Step [3400/4367], Loss: 0.1984
2025-02-16 09:04:58,087 - Epoch [111/1000], Step [3500/4367], Loss: 0.0408
2025-02-16 09:05:32,425 - Epoch [111/1000], Step [3600/4367], Loss: 0.0688
2025-02-16 09:06:07,476 - Epoch [111/1000], Step [3700/4367], Loss: 0.0751
2025-02-16 09:06:42,110 - Epoch [111/1000], Step [3800/4367], Loss: 0.0287
2025-02-16 09:07:16,350 - Epoch [111/1000], Step [3900/4367], Loss: 0.1761
2025-02-16 09:07:51,160 - Epoch [111/1000], Step [4000/4367], Loss: 0.1584
2025-02-16 09:08:25,780 - Epoch [111/1000], Step [4100/4367], Loss: 0.0490
2025-02-16 09:09:00,663 - Epoch [111/1000], Step [4200/4367], Loss: 0.0778
2025-02-16 09:09:35,513 - Epoch [111/1000], Step [4300/4367], Loss: 0.1369
2025-02-16 09:10:08,384 - Epoch [111/1000], Validation Step [100/1090], Val Loss: 0.0001
2025-02-16 09:10:17,578 - Epoch [111/1000], Validation Step [200/1090], Val Loss: 0.0004
2025-02-16 09:10:26,923 - Epoch [111/1000], Validation Step [300/1090], Val Loss: 0.2431
2025-02-16 09:10:36,596 - Epoch [111/1000], Validation Step [400/1090], Val Loss: 0.0402
2025-02-16 09:10:45,705 - Epoch [111/1000], Validation Step [500/1090], Val Loss: 0.4575
2025-02-16 09:10:55,236 - Epoch [111/1000], Validation Step [600/1090], Val Loss: 0.1467
2025-02-16 09:11:04,807 - Epoch [111/1000], Validation Step [700/1090], Val Loss: 0.1350
2025-02-16 09:11:13,592 - Epoch [111/1000], Validation Step [800/1090], Val Loss: 0.0029
2025-02-16 09:11:22,154 - Epoch [111/1000], Validation Step [900/1090], Val Loss: 0.0022
2025-02-16 09:11:31,407 - Epoch [111/1000], Validation Step [1000/1090], Val Loss: 0.0001
2025-02-16 09:11:40,041 - Epoch 111/1000, Train Loss: 0.1310, Val Loss: 0.1407, Accuracy: 94.84%
2025-02-16 09:12:15,386 - Epoch [112/1000], Step [100/4367], Loss: 0.1997
2025-02-16 09:12:49,967 - Epoch [112/1000], Step [200/4367], Loss: 0.1146
2025-02-16 09:13:24,626 - Epoch [112/1000], Step [300/4367], Loss: 0.1285
2025-02-16 09:13:59,566 - Epoch [112/1000], Step [400/4367], Loss: 0.0403
2025-02-16 09:14:34,302 - Epoch [112/1000], Step [500/4367], Loss: 0.1587
2025-02-16 09:15:09,306 - Epoch [112/1000], Step [600/4367], Loss: 0.1752
2025-02-16 09:15:44,139 - Epoch [112/1000], Step [700/4367], Loss: 0.1196
2025-02-16 09:16:18,582 - Epoch [112/1000], Step [800/4367], Loss: 0.1035
2025-02-16 09:16:53,615 - Epoch [112/1000], Step [900/4367], Loss: 0.0700
2025-02-16 09:17:28,201 - Epoch [112/1000], Step [1000/4367], Loss: 0.1002
2025-02-16 09:18:03,271 - Epoch [112/1000], Step [1100/4367], Loss: 0.1256
2025-02-16 09:18:37,629 - Epoch [112/1000], Step [1200/4367], Loss: 0.0323
2025-02-16 09:19:12,162 - Epoch [112/1000], Step [1300/4367], Loss: 0.1760
2025-02-16 09:19:46,841 - Epoch [112/1000], Step [1400/4367], Loss: 0.1007
2025-02-16 09:20:21,796 - Epoch [112/1000], Step [1500/4367], Loss: 0.1871
2025-02-16 09:20:56,741 - Epoch [112/1000], Step [1600/4367], Loss: 0.0710
2025-02-16 09:21:31,838 - Epoch [112/1000], Step [1700/4367], Loss: 0.1581
2025-02-16 09:22:06,099 - Epoch [112/1000], Step [1800/4367], Loss: 0.3240
2025-02-16 09:22:41,057 - Epoch [112/1000], Step [1900/4367], Loss: 0.1374
2025-02-16 09:23:16,093 - Epoch [112/1000], Step [2000/4367], Loss: 0.1799
2025-02-16 09:23:50,720 - Epoch [112/1000], Step [2100/4367], Loss: 0.0718
2025-02-16 09:24:24,911 - Epoch [112/1000], Step [2200/4367], Loss: 0.0957
2025-02-16 09:24:59,738 - Epoch [112/1000], Step [2300/4367], Loss: 0.1711
2025-02-16 09:25:33,896 - Epoch [112/1000], Step [2400/4367], Loss: 0.1908
2025-02-16 09:26:08,579 - Epoch [112/1000], Step [2500/4367], Loss: 0.1303
2025-02-16 09:26:43,329 - Epoch [112/1000], Step [2600/4367], Loss: 0.1877
2025-02-16 09:27:18,163 - Epoch [112/1000], Step [2700/4367], Loss: 0.1425
2025-02-16 09:27:52,636 - Epoch [112/1000], Step [2800/4367], Loss: 0.1131
2025-02-16 09:28:27,184 - Epoch [112/1000], Step [2900/4367], Loss: 0.0592
2025-02-16 09:29:02,246 - Epoch [112/1000], Step [3000/4367], Loss: 0.1956
2025-02-16 09:29:37,649 - Epoch [112/1000], Step [3100/4367], Loss: 0.1684
2025-02-16 09:30:12,623 - Epoch [112/1000], Step [3200/4367], Loss: 0.1789
2025-02-16 09:30:47,481 - Epoch [112/1000], Step [3300/4367], Loss: 0.0985
2025-02-16 09:31:22,242 - Epoch [112/1000], Step [3400/4367], Loss: 0.0407
2025-02-16 09:31:56,643 - Epoch [112/1000], Step [3500/4367], Loss: 0.0848
2025-02-16 09:32:31,391 - Epoch [112/1000], Step [3600/4367], Loss: 0.2376
2025-02-16 09:33:06,359 - Epoch [112/1000], Step [3700/4367], Loss: 0.0738
2025-02-16 09:33:41,074 - Epoch [112/1000], Step [3800/4367], Loss: 0.2038
2025-02-16 09:34:15,764 - Epoch [112/1000], Step [3900/4367], Loss: 0.4607
2025-02-16 09:34:50,667 - Epoch [112/1000], Step [4000/4367], Loss: 0.0388
2025-02-16 09:35:25,096 - Epoch [112/1000], Step [4100/4367], Loss: 0.1479
2025-02-16 09:35:59,608 - Epoch [112/1000], Step [4200/4367], Loss: 0.0599
2025-02-16 09:36:34,357 - Epoch [112/1000], Step [4300/4367], Loss: 0.2333
2025-02-16 09:37:07,531 - Epoch [112/1000], Validation Step [100/1090], Val Loss: 0.0001
2025-02-16 09:37:16,744 - Epoch [112/1000], Validation Step [200/1090], Val Loss: 0.0002
2025-02-16 09:37:26,089 - Epoch [112/1000], Validation Step [300/1090], Val Loss: 0.2473
2025-02-16 09:37:35,750 - Epoch [112/1000], Validation Step [400/1090], Val Loss: 0.0534
2025-02-16 09:37:44,860 - Epoch [112/1000], Validation Step [500/1090], Val Loss: 0.4791
2025-02-16 09:37:54,386 - Epoch [112/1000], Validation Step [600/1090], Val Loss: 0.1210
2025-02-16 09:38:03,949 - Epoch [112/1000], Validation Step [700/1090], Val Loss: 0.1084
2025-02-16 09:38:12,729 - Epoch [112/1000], Validation Step [800/1090], Val Loss: 0.0038
2025-02-16 09:38:21,294 - Epoch [112/1000], Validation Step [900/1090], Val Loss: 0.0031
2025-02-16 09:38:30,551 - Epoch [112/1000], Validation Step [1000/1090], Val Loss: 0.0001
2025-02-16 09:38:39,171 - Epoch 112/1000, Train Loss: 0.1312, Val Loss: 0.1385, Accuracy: 94.90%
2025-02-16 09:38:39,608 - Model saved at /data2/personal/sungjin/korean_dialects/checkpoint_real/checkpoint_epoch_112.pth
2025-02-16 09:39:15,235 - Epoch [113/1000], Step [100/4367], Loss: 0.2638
2025-02-16 09:39:50,014 - Epoch [113/1000], Step [200/4367], Loss: 0.0979
2025-02-16 09:40:24,712 - Epoch [113/1000], Step [300/4367], Loss: 0.1088
2025-02-16 09:40:59,523 - Epoch [113/1000], Step [400/4367], Loss: 0.1697
2025-02-16 09:41:34,684 - Epoch [113/1000], Step [500/4367], Loss: 0.0660
2025-02-16 09:42:08,951 - Epoch [113/1000], Step [600/4367], Loss: 0.1112
2025-02-16 09:42:43,479 - Epoch [113/1000], Step [700/4367], Loss: 0.0785
2025-02-16 09:43:18,234 - Epoch [113/1000], Step [800/4367], Loss: 0.0751
2025-02-16 09:43:52,751 - Epoch [113/1000], Step [900/4367], Loss: 0.1772
2025-02-16 09:44:27,852 - Epoch [113/1000], Step [1000/4367], Loss: 0.0762
2025-02-16 09:45:01,961 - Epoch [113/1000], Step [1100/4367], Loss: 0.2183
2025-02-16 09:45:36,661 - Epoch [113/1000], Step [1200/4367], Loss: 0.3206
2025-02-16 09:46:11,554 - Epoch [113/1000], Step [1300/4367], Loss: 0.0647
2025-02-16 09:46:45,815 - Epoch [113/1000], Step [1400/4367], Loss: 0.0652
2025-02-16 09:47:20,520 - Epoch [113/1000], Step [1500/4367], Loss: 0.3712
2025-02-16 09:47:55,195 - Epoch [113/1000], Step [1600/4367], Loss: 0.0803
2025-02-16 09:48:29,735 - Epoch [113/1000], Step [1700/4367], Loss: 0.1616
2025-02-16 09:49:04,536 - Epoch [113/1000], Step [1800/4367], Loss: 0.3526
2025-02-16 09:49:39,318 - Epoch [113/1000], Step [1900/4367], Loss: 0.1329
2025-02-16 09:50:14,023 - Epoch [113/1000], Step [2000/4367], Loss: 0.1403
2025-02-16 09:50:48,817 - Epoch [113/1000], Step [2100/4367], Loss: 0.2633
2025-02-16 09:51:23,863 - Epoch [113/1000], Step [2200/4367], Loss: 0.0310
2025-02-16 09:51:58,273 - Epoch [113/1000], Step [2300/4367], Loss: 0.0433
2025-02-16 09:52:33,126 - Epoch [113/1000], Step [2400/4367], Loss: 0.1434
2025-02-16 09:53:08,043 - Epoch [113/1000], Step [2500/4367], Loss: 0.0652
2025-02-16 09:53:43,094 - Epoch [113/1000], Step [2600/4367], Loss: 0.0539
2025-02-16 09:54:17,487 - Epoch [113/1000], Step [2700/4367], Loss: 0.0352
2025-02-16 09:54:52,029 - Epoch [113/1000], Step [2800/4367], Loss: 0.1008
2025-02-16 09:55:26,220 - Epoch [113/1000], Step [2900/4367], Loss: 0.0763
2025-02-16 09:56:01,183 - Epoch [113/1000], Step [3000/4367], Loss: 0.0839
2025-02-16 09:56:35,839 - Epoch [113/1000], Step [3100/4367], Loss: 0.1000
2025-02-16 09:57:10,305 - Epoch [113/1000], Step [3200/4367], Loss: 0.1012
2025-02-16 09:57:44,790 - Epoch [113/1000], Step [3300/4367], Loss: 0.0559
2025-02-16 09:58:20,092 - Epoch [113/1000], Step [3400/4367], Loss: 0.0181
2025-02-16 09:58:54,586 - Epoch [113/1000], Step [3500/4367], Loss: 0.3010
2025-02-16 09:59:29,452 - Epoch [113/1000], Step [3600/4367], Loss: 0.0275
2025-02-16 10:00:03,897 - Epoch [113/1000], Step [3700/4367], Loss: 0.1636
2025-02-16 10:00:38,229 - Epoch [113/1000], Step [3800/4367], Loss: 0.1911
2025-02-16 10:01:12,057 - Epoch [113/1000], Step [3900/4367], Loss: 0.1173
2025-02-16 10:01:46,751 - Epoch [113/1000], Step [4000/4367], Loss: 0.3772
2025-02-16 10:02:21,716 - Epoch [113/1000], Step [4100/4367], Loss: 0.0214
2025-02-16 10:02:56,740 - Epoch [113/1000], Step [4200/4367], Loss: 0.2543
2025-02-16 10:03:31,349 - Epoch [113/1000], Step [4300/4367], Loss: 0.2033
2025-02-16 10:04:04,117 - Epoch [113/1000], Validation Step [100/1090], Val Loss: 0.0000
2025-02-16 10:04:13,319 - Epoch [113/1000], Validation Step [200/1090], Val Loss: 0.0001
2025-02-16 10:04:22,665 - Epoch [113/1000], Validation Step [300/1090], Val Loss: 0.2588
2025-02-16 10:04:32,336 - Epoch [113/1000], Validation Step [400/1090], Val Loss: 0.0409
2025-02-16 10:04:41,459 - Epoch [113/1000], Validation Step [500/1090], Val Loss: 0.4533
2025-02-16 10:04:50,992 - Epoch [113/1000], Validation Step [600/1090], Val Loss: 0.1096
2025-02-16 10:05:00,564 - Epoch [113/1000], Validation Step [700/1090], Val Loss: 0.0935
2025-02-16 10:05:09,365 - Epoch [113/1000], Validation Step [800/1090], Val Loss: 0.0032
2025-02-16 10:05:17,936 - Epoch [113/1000], Validation Step [900/1090], Val Loss: 0.0032
2025-02-16 10:05:27,198 - Epoch [113/1000], Validation Step [1000/1090], Val Loss: 0.0000
2025-02-16 10:05:35,829 - Epoch 113/1000, Train Loss: 0.1316, Val Loss: 0.1398, Accuracy: 94.89%
2025-02-16 10:06:11,661 - Epoch [114/1000], Step [100/4367], Loss: 0.2801
2025-02-16 10:06:46,251 - Epoch [114/1000], Step [200/4367], Loss: 0.2432
2025-02-16 10:07:20,816 - Epoch [114/1000], Step [300/4367], Loss: 0.1197
2025-02-16 10:07:55,856 - Epoch [114/1000], Step [400/4367], Loss: 0.3065
2025-02-16 10:08:30,776 - Epoch [114/1000], Step [500/4367], Loss: 0.0573
2025-02-16 10:09:05,776 - Epoch [114/1000], Step [600/4367], Loss: 0.2685
2025-02-16 10:09:40,722 - Epoch [114/1000], Step [700/4367], Loss: 0.1943
2025-02-16 10:10:15,310 - Epoch [114/1000], Step [800/4367], Loss: 0.0726
2025-02-16 10:10:50,283 - Epoch [114/1000], Step [900/4367], Loss: 0.0794
2025-02-16 10:11:24,838 - Epoch [114/1000], Step [1000/4367], Loss: 0.1494
2025-02-16 10:11:59,636 - Epoch [114/1000], Step [1100/4367], Loss: 0.0115
2025-02-16 10:12:34,458 - Epoch [114/1000], Step [1200/4367], Loss: 0.0573
2025-02-16 10:13:09,675 - Epoch [114/1000], Step [1300/4367], Loss: 0.0267
2025-02-16 10:13:45,054 - Epoch [114/1000], Step [1400/4367], Loss: 0.0335
2025-02-16 10:14:19,342 - Epoch [114/1000], Step [1500/4367], Loss: 0.0757
2025-02-16 10:14:54,367 - Epoch [114/1000], Step [1600/4367], Loss: 0.1403
2025-02-16 10:15:29,094 - Epoch [114/1000], Step [1700/4367], Loss: 0.0917
2025-02-16 10:16:03,492 - Epoch [114/1000], Step [1800/4367], Loss: 0.1902
2025-02-16 10:16:38,470 - Epoch [114/1000], Step [1900/4367], Loss: 0.1197
2025-02-16 10:17:13,258 - Epoch [114/1000], Step [2000/4367], Loss: 0.0907
2025-02-16 10:17:47,589 - Epoch [114/1000], Step [2100/4367], Loss: 0.3563
2025-02-16 10:18:22,156 - Epoch [114/1000], Step [2200/4367], Loss: 0.0438
2025-02-16 10:18:57,159 - Epoch [114/1000], Step [2300/4367], Loss: 0.2307
2025-02-16 10:19:31,846 - Epoch [114/1000], Step [2400/4367], Loss: 0.0854
2025-02-16 10:20:06,549 - Epoch [114/1000], Step [2500/4367], Loss: 0.1811
2025-02-16 10:20:41,459 - Epoch [114/1000], Step [2600/4367], Loss: 0.1184
2025-02-16 10:21:16,130 - Epoch [114/1000], Step [2700/4367], Loss: 0.3110
2025-02-16 10:21:50,744 - Epoch [114/1000], Step [2800/4367], Loss: 0.1333
2025-02-16 10:22:25,331 - Epoch [114/1000], Step [2900/4367], Loss: 0.0962
2025-02-16 10:23:00,227 - Epoch [114/1000], Step [3000/4367], Loss: 0.1161
2025-02-16 10:23:34,789 - Epoch [114/1000], Step [3100/4367], Loss: 0.0544
2025-02-16 10:24:09,745 - Epoch [114/1000], Step [3200/4367], Loss: 0.1989
2025-02-16 10:24:44,122 - Epoch [114/1000], Step [3300/4367], Loss: 0.0983
2025-02-16 10:25:18,555 - Epoch [114/1000], Step [3400/4367], Loss: 0.0534
2025-02-16 10:25:53,444 - Epoch [114/1000], Step [3500/4367], Loss: 0.0415
2025-02-16 10:26:28,533 - Epoch [114/1000], Step [3600/4367], Loss: 0.0279
2025-02-16 10:27:03,216 - Epoch [114/1000], Step [3700/4367], Loss: 0.0542
2025-02-16 10:27:37,837 - Epoch [114/1000], Step [3800/4367], Loss: 0.0518
2025-02-16 10:28:12,572 - Epoch [114/1000], Step [3900/4367], Loss: 0.2809
2025-02-16 10:28:46,754 - Epoch [114/1000], Step [4000/4367], Loss: 0.3145
2025-02-16 10:29:21,497 - Epoch [114/1000], Step [4100/4367], Loss: 0.1149
2025-02-16 10:29:56,159 - Epoch [114/1000], Step [4200/4367], Loss: 0.0910
2025-02-16 10:30:30,204 - Epoch [114/1000], Step [4300/4367], Loss: 0.0856
2025-02-16 10:31:03,242 - Epoch [114/1000], Validation Step [100/1090], Val Loss: 0.0001
2025-02-16 10:31:12,443 - Epoch [114/1000], Validation Step [200/1090], Val Loss: 0.0001
2025-02-16 10:31:21,779 - Epoch [114/1000], Validation Step [300/1090], Val Loss: 0.2501
2025-02-16 10:31:31,447 - Epoch [114/1000], Validation Step [400/1090], Val Loss: 0.0563
2025-02-16 10:31:40,551 - Epoch [114/1000], Validation Step [500/1090], Val Loss: 0.4696
2025-02-16 10:31:50,086 - Epoch [114/1000], Validation Step [600/1090], Val Loss: 0.1155
2025-02-16 10:31:59,641 - Epoch [114/1000], Validation Step [700/1090], Val Loss: 0.1157
2025-02-16 10:32:08,423 - Epoch [114/1000], Validation Step [800/1090], Val Loss: 0.0034
2025-02-16 10:32:16,981 - Epoch [114/1000], Validation Step [900/1090], Val Loss: 0.0029
2025-02-16 10:32:26,243 - Epoch [114/1000], Validation Step [1000/1090], Val Loss: 0.0002
2025-02-16 10:32:34,859 - Epoch 114/1000, Train Loss: 0.1335, Val Loss: 0.1387, Accuracy: 94.89%
2025-02-16 10:32:35,266 - Model saved at /data2/personal/sungjin/korean_dialects/checkpoint_real/checkpoint_epoch_114.pth
2025-02-16 10:33:11,112 - Epoch [115/1000], Step [100/4367], Loss: 0.0406
2025-02-16 10:33:45,803 - Epoch [115/1000], Step [200/4367], Loss: 0.0468
2025-02-16 10:34:20,517 - Epoch [115/1000], Step [300/4367], Loss: 0.1031
2025-02-16 10:34:55,352 - Epoch [115/1000], Step [400/4367], Loss: 0.0402
2025-02-16 10:35:29,928 - Epoch [115/1000], Step [500/4367], Loss: 0.0632
2025-02-16 10:36:04,467 - Epoch [115/1000], Step [600/4367], Loss: 0.1044
2025-02-16 10:36:39,202 - Epoch [115/1000], Step [700/4367], Loss: 0.0794
2025-02-16 10:37:13,982 - Epoch [115/1000], Step [800/4367], Loss: 0.2184
2025-02-16 10:37:48,546 - Epoch [115/1000], Step [900/4367], Loss: 0.1843
2025-02-16 10:38:22,664 - Epoch [115/1000], Step [1000/4367], Loss: 0.3069
2025-02-16 10:38:57,535 - Epoch [115/1000], Step [1100/4367], Loss: 0.0833
2025-02-16 10:39:32,418 - Epoch [115/1000], Step [1200/4367], Loss: 0.0883
2025-02-16 10:40:06,908 - Epoch [115/1000], Step [1300/4367], Loss: 0.0636
2025-02-16 10:40:41,590 - Epoch [115/1000], Step [1400/4367], Loss: 0.1957
2025-02-16 10:41:15,850 - Epoch [115/1000], Step [1500/4367], Loss: 0.1127
2025-02-16 10:41:50,920 - Epoch [115/1000], Step [1600/4367], Loss: 0.0504
2025-02-16 10:42:25,530 - Epoch [115/1000], Step [1700/4367], Loss: 0.0888
2025-02-16 10:43:00,149 - Epoch [115/1000], Step [1800/4367], Loss: 0.1455
2025-02-16 10:43:35,035 - Epoch [115/1000], Step [1900/4367], Loss: 0.1395
2025-02-16 10:44:09,479 - Epoch [115/1000], Step [2000/4367], Loss: 0.1332
2025-02-16 10:44:44,321 - Epoch [115/1000], Step [2100/4367], Loss: 0.2030
2025-02-16 10:45:19,434 - Epoch [115/1000], Step [2200/4367], Loss: 0.3561
2025-02-16 10:45:53,957 - Epoch [115/1000], Step [2300/4367], Loss: 0.2927
2025-02-16 10:46:28,539 - Epoch [115/1000], Step [2400/4367], Loss: 0.0609
2025-02-16 10:47:03,789 - Epoch [115/1000], Step [2500/4367], Loss: 0.0336
2025-02-16 10:47:38,658 - Epoch [115/1000], Step [2600/4367], Loss: 0.1502
2025-02-16 10:48:13,512 - Epoch [115/1000], Step [2700/4367], Loss: 0.1443
2025-02-16 10:48:48,367 - Epoch [115/1000], Step [2800/4367], Loss: 0.1088
2025-02-16 10:49:23,143 - Epoch [115/1000], Step [2900/4367], Loss: 0.2484
2025-02-16 10:49:57,503 - Epoch [115/1000], Step [3000/4367], Loss: 0.1274
2025-02-16 10:50:32,358 - Epoch [115/1000], Step [3100/4367], Loss: 0.5982
2025-02-16 10:51:07,018 - Epoch [115/1000], Step [3200/4367], Loss: 0.0153
2025-02-16 10:51:41,826 - Epoch [115/1000], Step [3300/4367], Loss: 0.3406
2025-02-16 10:52:16,149 - Epoch [115/1000], Step [3400/4367], Loss: 0.1101
2025-02-16 10:52:50,741 - Epoch [115/1000], Step [3500/4367], Loss: 0.1133
2025-02-16 10:53:25,840 - Epoch [115/1000], Step [3600/4367], Loss: 0.1046
2025-02-16 10:54:00,598 - Epoch [115/1000], Step [3700/4367], Loss: 0.0718
2025-02-16 10:54:35,150 - Epoch [115/1000], Step [3800/4367], Loss: 0.1143
2025-02-16 10:55:10,136 - Epoch [115/1000], Step [3900/4367], Loss: 0.2426
2025-02-16 10:55:44,640 - Epoch [115/1000], Step [4000/4367], Loss: 0.0152
2025-02-16 10:56:19,395 - Epoch [115/1000], Step [4100/4367], Loss: 0.0592
2025-02-16 10:56:54,167 - Epoch [115/1000], Step [4200/4367], Loss: 0.1296
2025-02-16 10:57:29,068 - Epoch [115/1000], Step [4300/4367], Loss: 0.1186
2025-02-16 10:58:02,162 - Epoch [115/1000], Validation Step [100/1090], Val Loss: 0.0002
2025-02-16 10:58:11,352 - Epoch [115/1000], Validation Step [200/1090], Val Loss: 0.0007
2025-02-16 10:58:20,700 - Epoch [115/1000], Validation Step [300/1090], Val Loss: 0.2606
2025-02-16 10:58:30,371 - Epoch [115/1000], Validation Step [400/1090], Val Loss: 0.1062
2025-02-16 10:58:39,477 - Epoch [115/1000], Validation Step [500/1090], Val Loss: 0.6721
2025-02-16 10:58:49,004 - Epoch [115/1000], Validation Step [600/1090], Val Loss: 0.1304
2025-02-16 10:58:58,582 - Epoch [115/1000], Validation Step [700/1090], Val Loss: 0.1213
2025-02-16 10:59:07,361 - Epoch [115/1000], Validation Step [800/1090], Val Loss: 0.0010
2025-02-16 10:59:15,924 - Epoch [115/1000], Validation Step [900/1090], Val Loss: 0.0011
2025-02-16 10:59:25,183 - Epoch [115/1000], Validation Step [1000/1090], Val Loss: 0.0000
2025-02-16 10:59:33,800 - Epoch 115/1000, Train Loss: 0.1333, Val Loss: 0.1511, Accuracy: 94.59%
2025-02-16 11:00:09,756 - Epoch [116/1000], Step [100/4367], Loss: 0.0541
2025-02-16 11:00:44,855 - Epoch [116/1000], Step [200/4367], Loss: 0.2223
2025-02-16 11:01:19,735 - Epoch [116/1000], Step [300/4367], Loss: 0.0428
2025-02-16 11:01:54,242 - Epoch [116/1000], Step [400/4367], Loss: 0.0994
2025-02-16 11:02:29,209 - Epoch [116/1000], Step [500/4367], Loss: 0.1093
2025-02-16 11:03:03,801 - Epoch [116/1000], Step [600/4367], Loss: 0.0633
2025-02-16 11:03:38,366 - Epoch [116/1000], Step [700/4367], Loss: 0.2286
2025-02-16 11:04:13,089 - Epoch [116/1000], Step [800/4367], Loss: 0.2171
2025-02-16 11:04:47,746 - Epoch [116/1000], Step [900/4367], Loss: 0.1790
2025-02-16 11:05:22,722 - Epoch [116/1000], Step [1000/4367], Loss: 0.1278
2025-02-16 11:05:57,160 - Epoch [116/1000], Step [1100/4367], Loss: 0.2435
2025-02-16 11:06:31,423 - Epoch [116/1000], Step [1200/4367], Loss: 0.0850
2025-02-16 11:07:06,262 - Epoch [116/1000], Step [1300/4367], Loss: 0.0921
2025-02-16 11:07:41,057 - Epoch [116/1000], Step [1400/4367], Loss: 0.1697
2025-02-16 11:08:15,637 - Epoch [116/1000], Step [1500/4367], Loss: 0.1592
2025-02-16 11:08:50,106 - Epoch [116/1000], Step [1600/4367], Loss: 0.1415
2025-02-16 11:09:24,891 - Epoch [116/1000], Step [1700/4367], Loss: 0.1676
2025-02-16 11:09:59,930 - Epoch [116/1000], Step [1800/4367], Loss: 0.1994
2025-02-16 11:10:34,456 - Epoch [116/1000], Step [1900/4367], Loss: 0.1569
2025-02-16 11:11:09,301 - Epoch [116/1000], Step [2000/4367], Loss: 0.1298
2025-02-16 11:11:43,744 - Epoch [116/1000], Step [2100/4367], Loss: 0.1812
2025-02-16 11:12:18,452 - Epoch [116/1000], Step [2200/4367], Loss: 0.1535
2025-02-16 11:12:53,243 - Epoch [116/1000], Step [2300/4367], Loss: 0.1958
2025-02-16 11:13:27,541 - Epoch [116/1000], Step [2400/4367], Loss: 0.1497
2025-02-16 11:14:02,064 - Epoch [116/1000], Step [2500/4367], Loss: 0.0810
2025-02-16 11:14:36,791 - Epoch [116/1000], Step [2600/4367], Loss: 0.0594
2025-02-16 11:15:11,743 - Epoch [116/1000], Step [2700/4367], Loss: 0.2547
2025-02-16 11:15:46,245 - Epoch [116/1000], Step [2800/4367], Loss: 0.0284
2025-02-16 11:16:20,764 - Epoch [116/1000], Step [2900/4367], Loss: 0.0727
2025-02-16 11:16:55,558 - Epoch [116/1000], Step [3000/4367], Loss: 0.2797
2025-02-16 11:17:30,065 - Epoch [116/1000], Step [3100/4367], Loss: 0.1307
2025-02-16 11:18:04,789 - Epoch [116/1000], Step [3200/4367], Loss: 0.1074
2025-02-16 11:18:39,229 - Epoch [116/1000], Step [3300/4367], Loss: 0.0538
2025-02-16 11:19:14,129 - Epoch [116/1000], Step [3400/4367], Loss: 0.1855
2025-02-16 11:19:48,853 - Epoch [116/1000], Step [3500/4367], Loss: 0.1283
2025-02-16 11:20:23,434 - Epoch [116/1000], Step [3600/4367], Loss: 0.0538
2025-02-16 11:20:58,069 - Epoch [116/1000], Step [3700/4367], Loss: 0.1288
2025-02-16 11:21:32,657 - Epoch [116/1000], Step [3800/4367], Loss: 0.3334
2025-02-16 11:22:08,010 - Epoch [116/1000], Step [3900/4367], Loss: 0.1979
2025-02-16 11:22:42,967 - Epoch [116/1000], Step [4000/4367], Loss: 0.1785
2025-02-16 11:23:17,380 - Epoch [116/1000], Step [4100/4367], Loss: 0.1366
2025-02-16 11:23:52,422 - Epoch [116/1000], Step [4200/4367], Loss: 0.1366
2025-02-16 11:24:27,144 - Epoch [116/1000], Step [4300/4367], Loss: 0.0718
2025-02-16 11:25:00,221 - Epoch [116/1000], Validation Step [100/1090], Val Loss: 0.0001
2025-02-16 11:25:09,421 - Epoch [116/1000], Validation Step [200/1090], Val Loss: 0.0002
2025-02-16 11:25:18,762 - Epoch [116/1000], Validation Step [300/1090], Val Loss: 0.2499
2025-02-16 11:25:28,422 - Epoch [116/1000], Validation Step [400/1090], Val Loss: 0.0491
2025-02-16 11:25:37,529 - Epoch [116/1000], Validation Step [500/1090], Val Loss: 0.4342
2025-02-16 11:25:47,055 - Epoch [116/1000], Validation Step [600/1090], Val Loss: 0.1300
2025-02-16 11:25:56,612 - Epoch [116/1000], Validation Step [700/1090], Val Loss: 0.1264
2025-02-16 11:26:05,390 - Epoch [116/1000], Validation Step [800/1090], Val Loss: 0.0052
2025-02-16 11:26:13,936 - Epoch [116/1000], Validation Step [900/1090], Val Loss: 0.0044
2025-02-16 11:26:23,194 - Epoch [116/1000], Validation Step [1000/1090], Val Loss: 0.0000
2025-02-16 11:26:31,821 - Epoch 116/1000, Train Loss: 0.1315, Val Loss: 0.1380, Accuracy: 94.98%
2025-02-16 11:26:32,253 - Model saved at /data2/personal/sungjin/korean_dialects/checkpoint_real/checkpoint_epoch_116.pth
2025-02-16 11:27:08,348 - Epoch [117/1000], Step [100/4367], Loss: 0.1642
2025-02-16 11:27:42,926 - Epoch [117/1000], Step [200/4367], Loss: 0.0549
2025-02-16 11:28:17,401 - Epoch [117/1000], Step [300/4367], Loss: 0.1862
2025-02-16 11:28:51,868 - Epoch [117/1000], Step [400/4367], Loss: 0.2751
2025-02-16 11:29:26,367 - Epoch [117/1000], Step [500/4367], Loss: 0.0278
2025-02-16 11:30:00,578 - Epoch [117/1000], Step [600/4367], Loss: 0.0765
2025-02-16 11:30:35,271 - Epoch [117/1000], Step [700/4367], Loss: 0.0507
2025-02-16 11:31:09,564 - Epoch [117/1000], Step [800/4367], Loss: 0.0683
2025-02-16 11:31:44,497 - Epoch [117/1000], Step [900/4367], Loss: 0.0798
2025-02-16 11:32:19,499 - Epoch [117/1000], Step [1000/4367], Loss: 0.1106
2025-02-16 11:32:54,075 - Epoch [117/1000], Step [1100/4367], Loss: 0.0474
2025-02-16 11:33:29,073 - Epoch [117/1000], Step [1200/4367], Loss: 0.0216
2025-02-16 11:34:03,923 - Epoch [117/1000], Step [1300/4367], Loss: 0.0775
2025-02-16 11:34:39,216 - Epoch [117/1000], Step [1400/4367], Loss: 0.1612
2025-02-16 11:35:14,458 - Epoch [117/1000], Step [1500/4367], Loss: 0.0331
2025-02-16 11:35:49,472 - Epoch [117/1000], Step [1600/4367], Loss: 0.1402
2025-02-16 11:36:24,112 - Epoch [117/1000], Step [1700/4367], Loss: 0.0771
2025-02-16 11:36:58,797 - Epoch [117/1000], Step [1800/4367], Loss: 0.0578
2025-02-16 11:37:33,523 - Epoch [117/1000], Step [1900/4367], Loss: 0.1076
2025-02-16 11:38:08,038 - Epoch [117/1000], Step [2000/4367], Loss: 0.0354
2025-02-16 11:38:42,706 - Epoch [117/1000], Step [2100/4367], Loss: 0.2995
2025-02-16 11:39:17,646 - Epoch [117/1000], Step [2200/4367], Loss: 0.1120
2025-02-16 11:39:52,509 - Epoch [117/1000], Step [2300/4367], Loss: 0.0715
2025-02-16 11:40:27,007 - Epoch [117/1000], Step [2400/4367], Loss: 0.1935
2025-02-16 11:41:02,309 - Epoch [117/1000], Step [2500/4367], Loss: 0.0711
2025-02-16 11:41:37,004 - Epoch [117/1000], Step [2600/4367], Loss: 0.1423
2025-02-16 11:42:12,105 - Epoch [117/1000], Step [2700/4367], Loss: 0.1084
2025-02-16 11:42:46,582 - Epoch [117/1000], Step [2800/4367], Loss: 0.0865
2025-02-16 11:43:20,955 - Epoch [117/1000], Step [2900/4367], Loss: 0.1167
2025-02-16 11:43:55,843 - Epoch [117/1000], Step [3000/4367], Loss: 0.0939
2025-02-16 11:44:30,901 - Epoch [117/1000], Step [3100/4367], Loss: 0.1186
2025-02-16 11:45:06,060 - Epoch [117/1000], Step [3200/4367], Loss: 0.0223
2025-02-16 11:45:40,690 - Epoch [117/1000], Step [3300/4367], Loss: 0.2498
2025-02-16 11:46:15,299 - Epoch [117/1000], Step [3400/4367], Loss: 0.2114
2025-02-16 11:46:49,899 - Epoch [117/1000], Step [3500/4367], Loss: 0.0903
2025-02-16 11:47:24,722 - Epoch [117/1000], Step [3600/4367], Loss: 0.0597
2025-02-16 11:47:58,814 - Epoch [117/1000], Step [3700/4367], Loss: 0.1264
2025-02-16 11:48:33,681 - Epoch [117/1000], Step [3800/4367], Loss: 0.1601
2025-02-16 11:49:08,590 - Epoch [117/1000], Step [3900/4367], Loss: 0.1147
2025-02-16 11:49:42,982 - Epoch [117/1000], Step [4000/4367], Loss: 0.2856
2025-02-16 11:50:17,250 - Epoch [117/1000], Step [4100/4367], Loss: 0.0989
2025-02-16 11:50:52,254 - Epoch [117/1000], Step [4200/4367], Loss: 0.1149
2025-02-16 11:51:27,002 - Epoch [117/1000], Step [4300/4367], Loss: 0.0691
2025-02-16 11:52:00,232 - Epoch [117/1000], Validation Step [100/1090], Val Loss: 0.0003
2025-02-16 11:52:09,433 - Epoch [117/1000], Validation Step [200/1090], Val Loss: 0.0004
2025-02-16 11:52:18,771 - Epoch [117/1000], Validation Step [300/1090], Val Loss: 0.2344
2025-02-16 11:52:28,437 - Epoch [117/1000], Validation Step [400/1090], Val Loss: 0.0372
2025-02-16 11:52:37,546 - Epoch [117/1000], Validation Step [500/1090], Val Loss: 0.3925
2025-02-16 11:52:47,080 - Epoch [117/1000], Validation Step [600/1090], Val Loss: 0.1188
2025-02-16 11:52:56,640 - Epoch [117/1000], Validation Step [700/1090], Val Loss: 0.1232
2025-02-16 11:53:05,423 - Epoch [117/1000], Validation Step [800/1090], Val Loss: 0.0050
2025-02-16 11:53:13,980 - Epoch [117/1000], Validation Step [900/1090], Val Loss: 0.0047
2025-02-16 11:53:23,232 - Epoch [117/1000], Validation Step [1000/1090], Val Loss: 0.0001
2025-02-16 11:53:31,873 - Epoch 117/1000, Train Loss: 0.1320, Val Loss: 0.1377, Accuracy: 94.87%
2025-02-16 11:54:07,820 - Epoch [118/1000], Step [100/4367], Loss: 0.1348
2025-02-16 11:54:42,672 - Epoch [118/1000], Step [200/4367], Loss: 0.2024
2025-02-16 11:55:17,285 - Epoch [118/1000], Step [300/4367], Loss: 0.1877
2025-02-16 11:55:52,127 - Epoch [118/1000], Step [400/4367], Loss: 0.1349
2025-02-16 11:56:26,338 - Epoch [118/1000], Step [500/4367], Loss: 0.0670
2025-02-16 11:57:01,238 - Epoch [118/1000], Step [600/4367], Loss: 0.2295
2025-02-16 11:57:36,148 - Epoch [118/1000], Step [700/4367], Loss: 0.1302
2025-02-16 11:58:10,422 - Epoch [118/1000], Step [800/4367], Loss: 0.0311
2025-02-16 11:58:45,079 - Epoch [118/1000], Step [900/4367], Loss: 0.2088
2025-02-16 11:59:19,820 - Epoch [118/1000], Step [1000/4367], Loss: 0.0375
2025-02-16 11:59:54,099 - Epoch [118/1000], Step [1100/4367], Loss: 0.0595
2025-02-16 12:00:28,542 - Epoch [118/1000], Step [1200/4367], Loss: 0.2094
2025-02-16 12:01:03,110 - Epoch [118/1000], Step [1300/4367], Loss: 0.1487
2025-02-16 12:01:37,766 - Epoch [118/1000], Step [1400/4367], Loss: 0.1080
2025-02-16 12:02:12,246 - Epoch [118/1000], Step [1500/4367], Loss: 0.0563
2025-02-16 12:02:46,826 - Epoch [118/1000], Step [1600/4367], Loss: 0.1006
2025-02-16 12:03:21,585 - Epoch [118/1000], Step [1700/4367], Loss: 0.1808
2025-02-16 12:03:56,888 - Epoch [118/1000], Step [1800/4367], Loss: 0.1135
2025-02-16 12:04:31,336 - Epoch [118/1000], Step [1900/4367], Loss: 0.1885
2025-02-16 12:05:05,784 - Epoch [118/1000], Step [2000/4367], Loss: 0.2914
2025-02-16 12:05:40,799 - Epoch [118/1000], Step [2100/4367], Loss: 0.0841
2025-02-16 12:06:15,134 - Epoch [118/1000], Step [2200/4367], Loss: 0.1296
2025-02-16 12:06:50,036 - Epoch [118/1000], Step [2300/4367], Loss: 0.1488
2025-02-16 12:07:24,716 - Epoch [118/1000], Step [2400/4367], Loss: 0.2333
2025-02-16 12:07:59,590 - Epoch [118/1000], Step [2500/4367], Loss: 0.0755
2025-02-16 12:08:34,415 - Epoch [118/1000], Step [2600/4367], Loss: 0.0752
2025-02-16 12:09:09,043 - Epoch [118/1000], Step [2700/4367], Loss: 0.1371
2025-02-16 12:09:44,163 - Epoch [118/1000], Step [2800/4367], Loss: 0.0445
2025-02-16 12:10:19,253 - Epoch [118/1000], Step [2900/4367], Loss: 0.0411
2025-02-16 12:10:54,365 - Epoch [118/1000], Step [3000/4367], Loss: 0.1461
2025-02-16 12:11:29,096 - Epoch [118/1000], Step [3100/4367], Loss: 0.1007
2025-02-16 12:12:03,883 - Epoch [118/1000], Step [3200/4367], Loss: 0.0153
2025-02-16 12:12:38,699 - Epoch [118/1000], Step [3300/4367], Loss: 0.2011
2025-02-16 12:13:13,565 - Epoch [118/1000], Step [3400/4367], Loss: 0.0171
2025-02-16 12:13:48,867 - Epoch [118/1000], Step [3500/4367], Loss: 0.4667
2025-02-16 12:14:23,375 - Epoch [118/1000], Step [3600/4367], Loss: 0.3015
2025-02-16 12:14:58,044 - Epoch [118/1000], Step [3700/4367], Loss: 0.1590
2025-02-16 12:15:32,287 - Epoch [118/1000], Step [3800/4367], Loss: 0.0570
2025-02-16 12:16:07,331 - Epoch [118/1000], Step [3900/4367], Loss: 0.0418
2025-02-16 12:16:42,493 - Epoch [118/1000], Step [4000/4367], Loss: 0.1690
2025-02-16 12:17:17,069 - Epoch [118/1000], Step [4100/4367], Loss: 0.0169
2025-02-16 12:17:51,994 - Epoch [118/1000], Step [4200/4367], Loss: 0.1119
2025-02-16 12:18:26,936 - Epoch [118/1000], Step [4300/4367], Loss: 0.1029
2025-02-16 12:19:00,286 - Epoch [118/1000], Validation Step [100/1090], Val Loss: 0.0003
2025-02-16 12:19:09,482 - Epoch [118/1000], Validation Step [200/1090], Val Loss: 0.0008
2025-02-16 12:19:18,784 - Epoch [118/1000], Validation Step [300/1090], Val Loss: 0.2285
2025-02-16 12:19:28,396 - Epoch [118/1000], Validation Step [400/1090], Val Loss: 0.0413
2025-02-16 12:19:37,462 - Epoch [118/1000], Validation Step [500/1090], Val Loss: 0.4532
2025-02-16 12:19:46,938 - Epoch [118/1000], Validation Step [600/1090], Val Loss: 0.1634
2025-02-16 12:19:56,457 - Epoch [118/1000], Validation Step [700/1090], Val Loss: 0.1460
2025-02-16 12:20:05,206 - Epoch [118/1000], Validation Step [800/1090], Val Loss: 0.0036
2025-02-16 12:20:13,737 - Epoch [118/1000], Validation Step [900/1090], Val Loss: 0.0032
2025-02-16 12:20:22,946 - Epoch [118/1000], Validation Step [1000/1090], Val Loss: 0.0000
2025-02-16 12:20:31,554 - Epoch 118/1000, Train Loss: 0.1319, Val Loss: 0.1408, Accuracy: 94.85%
2025-02-16 12:20:32,023 - Model saved at /data2/personal/sungjin/korean_dialects/checkpoint_real/checkpoint_epoch_118.pth
2025-02-16 12:21:08,351 - Epoch [119/1000], Step [100/4367], Loss: 0.2785
2025-02-16 12:21:42,912 - Epoch [119/1000], Step [200/4367], Loss: 0.0943
2025-02-16 12:22:17,972 - Epoch [119/1000], Step [300/4367], Loss: 0.2371
2025-02-16 12:22:52,242 - Epoch [119/1000], Step [400/4367], Loss: 0.1587
2025-02-16 12:23:27,156 - Epoch [119/1000], Step [500/4367], Loss: 0.2744
2025-02-16 12:24:02,114 - Epoch [119/1000], Step [600/4367], Loss: 0.0385
2025-02-16 12:24:36,622 - Epoch [119/1000], Step [700/4367], Loss: 0.1335
2025-02-16 12:25:11,419 - Epoch [119/1000], Step [800/4367], Loss: 0.1123
2025-02-16 12:25:46,549 - Epoch [119/1000], Step [900/4367], Loss: 0.2183
2025-02-16 12:26:21,263 - Epoch [119/1000], Step [1000/4367], Loss: 0.0744
2025-02-16 12:26:55,765 - Epoch [119/1000], Step [1100/4367], Loss: 0.0656
2025-02-16 12:27:30,221 - Epoch [119/1000], Step [1200/4367], Loss: 0.0778
2025-02-16 12:28:05,003 - Epoch [119/1000], Step [1300/4367], Loss: 0.2806
2025-02-16 12:28:39,462 - Epoch [119/1000], Step [1400/4367], Loss: 0.2759
2025-02-16 12:29:13,903 - Epoch [119/1000], Step [1500/4367], Loss: 0.0573
2025-02-16 12:29:48,622 - Epoch [119/1000], Step [1600/4367], Loss: 0.0347
2025-02-16 12:30:23,856 - Epoch [119/1000], Step [1700/4367], Loss: 0.0362
2025-02-16 12:30:58,523 - Epoch [119/1000], Step [1800/4367], Loss: 0.1497
2025-02-16 12:31:33,229 - Epoch [119/1000], Step [1900/4367], Loss: 0.1445
2025-02-16 12:32:07,822 - Epoch [119/1000], Step [2000/4367], Loss: 0.2075
2025-02-16 12:32:42,609 - Epoch [119/1000], Step [2100/4367], Loss: 0.1350
2025-02-16 12:33:17,274 - Epoch [119/1000], Step [2200/4367], Loss: 0.2825
2025-02-16 12:33:52,100 - Epoch [119/1000], Step [2300/4367], Loss: 0.2149
2025-02-16 12:34:26,802 - Epoch [119/1000], Step [2400/4367], Loss: 0.0843
2025-02-16 12:35:01,950 - Epoch [119/1000], Step [2500/4367], Loss: 0.0422
2025-02-16 12:35:36,893 - Epoch [119/1000], Step [2600/4367], Loss: 0.3865
2025-02-16 12:36:11,169 - Epoch [119/1000], Step [2700/4367], Loss: 0.2270
2025-02-16 12:36:45,761 - Epoch [119/1000], Step [2800/4367], Loss: 0.1840
2025-02-16 12:37:20,256 - Epoch [119/1000], Step [2900/4367], Loss: 0.1731
2025-02-16 12:37:54,903 - Epoch [119/1000], Step [3000/4367], Loss: 0.2216
2025-02-16 12:38:29,762 - Epoch [119/1000], Step [3100/4367], Loss: 0.0650
2025-02-16 12:39:04,262 - Epoch [119/1000], Step [3200/4367], Loss: 0.0522
2025-02-16 12:39:39,104 - Epoch [119/1000], Step [3300/4367], Loss: 0.0734
2025-02-16 12:40:13,880 - Epoch [119/1000], Step [3400/4367], Loss: 0.1770
2025-02-16 12:40:48,594 - Epoch [119/1000], Step [3500/4367], Loss: 0.0650
2025-02-16 12:41:23,550 - Epoch [119/1000], Step [3600/4367], Loss: 0.1087
2025-02-16 12:41:58,416 - Epoch [119/1000], Step [3700/4367], Loss: 0.2405
2025-02-16 12:42:32,636 - Epoch [119/1000], Step [3800/4367], Loss: 0.0494
2025-02-16 12:43:07,364 - Epoch [119/1000], Step [3900/4367], Loss: 0.2703
2025-02-16 12:43:42,656 - Epoch [119/1000], Step [4000/4367], Loss: 0.1908
2025-02-16 12:44:17,233 - Epoch [119/1000], Step [4100/4367], Loss: 0.2167
2025-02-16 12:44:52,151 - Epoch [119/1000], Step [4200/4367], Loss: 0.0501
2025-02-16 12:45:26,880 - Epoch [119/1000], Step [4300/4367], Loss: 0.2594
2025-02-16 12:46:00,421 - Epoch [119/1000], Validation Step [100/1090], Val Loss: 0.0002
2025-02-16 12:46:09,607 - Epoch [119/1000], Validation Step [200/1090], Val Loss: 0.0004
2025-02-16 12:46:18,985 - Epoch [119/1000], Validation Step [300/1090], Val Loss: 0.2449
2025-02-16 12:46:28,675 - Epoch [119/1000], Validation Step [400/1090], Val Loss: 0.0389
2025-02-16 12:46:37,840 - Epoch [119/1000], Validation Step [500/1090], Val Loss: 0.4040
2025-02-16 12:46:47,411 - Epoch [119/1000], Validation Step [600/1090], Val Loss: 0.1181
2025-02-16 12:46:57,038 - Epoch [119/1000], Validation Step [700/1090], Val Loss: 0.1168
2025-02-16 12:47:05,849 - Epoch [119/1000], Validation Step [800/1090], Val Loss: 0.0049
2025-02-16 12:47:14,440 - Epoch [119/1000], Validation Step [900/1090], Val Loss: 0.0044
2025-02-16 12:47:23,724 - Epoch [119/1000], Validation Step [1000/1090], Val Loss: 0.0000
2025-02-16 12:47:32,395 - Epoch 119/1000, Train Loss: 0.1319, Val Loss: 0.1394, Accuracy: 94.93%
2025-02-16 12:48:08,623 - Epoch [120/1000], Step [100/4367], Loss: 0.1017
2025-02-16 12:48:43,186 - Epoch [120/1000], Step [200/4367], Loss: 0.0247
2025-02-16 12:49:17,938 - Epoch [120/1000], Step [300/4367], Loss: 0.1393
2025-02-16 12:49:52,986 - Epoch [120/1000], Step [400/4367], Loss: 0.0401
2025-02-16 12:50:27,504 - Epoch [120/1000], Step [500/4367], Loss: 0.0754
2025-02-16 12:51:02,200 - Epoch [120/1000], Step [600/4367], Loss: 0.1070
2025-02-16 12:51:36,927 - Epoch [120/1000], Step [700/4367], Loss: 0.0820
2025-02-16 12:52:11,354 - Epoch [120/1000], Step [800/4367], Loss: 0.2217
2025-02-16 12:52:46,129 - Epoch [120/1000], Step [900/4367], Loss: 0.1525
2025-02-16 12:53:20,940 - Epoch [120/1000], Step [1000/4367], Loss: 0.0913
2025-02-16 12:53:55,155 - Epoch [120/1000], Step [1100/4367], Loss: 0.0335
2025-02-16 12:54:30,008 - Epoch [120/1000], Step [1200/4367], Loss: 0.0913
2025-02-16 12:55:04,429 - Epoch [120/1000], Step [1300/4367], Loss: 0.1027
2025-02-16 12:55:39,358 - Epoch [120/1000], Step [1400/4367], Loss: 0.1224
2025-02-16 12:56:14,268 - Epoch [120/1000], Step [1500/4367], Loss: 0.2144
2025-02-16 12:56:49,308 - Epoch [120/1000], Step [1600/4367], Loss: 0.0635
2025-02-16 12:57:24,508 - Epoch [120/1000], Step [1700/4367], Loss: 0.0809
2025-02-16 12:57:59,493 - Epoch [120/1000], Step [1800/4367], Loss: 0.1057
2025-02-16 12:58:34,320 - Epoch [120/1000], Step [1900/4367], Loss: 0.1152
2025-02-16 12:59:09,156 - Epoch [120/1000], Step [2000/4367], Loss: 0.1413
2025-02-16 12:59:44,054 - Epoch [120/1000], Step [2100/4367], Loss: 0.1735
2025-02-16 13:00:18,982 - Epoch [120/1000], Step [2200/4367], Loss: 0.3489
2025-02-16 13:00:54,398 - Epoch [120/1000], Step [2300/4367], Loss: 0.1137
2025-02-16 13:01:29,618 - Epoch [120/1000], Step [2400/4367], Loss: 0.1816
2025-02-16 13:02:04,324 - Epoch [120/1000], Step [2500/4367], Loss: 0.3394
2025-02-16 13:02:39,352 - Epoch [120/1000], Step [2600/4367], Loss: 0.1965
2025-02-16 13:03:14,767 - Epoch [120/1000], Step [2700/4367], Loss: 0.1717
2025-02-16 13:03:50,176 - Epoch [120/1000], Step [2800/4367], Loss: 0.1610
2025-02-16 13:04:25,008 - Epoch [120/1000], Step [2900/4367], Loss: 0.0789
2025-02-16 13:04:59,796 - Epoch [120/1000], Step [3000/4367], Loss: 0.1536
2025-02-16 13:05:34,668 - Epoch [120/1000], Step [3100/4367], Loss: 0.0086
2025-02-16 13:06:09,268 - Epoch [120/1000], Step [3200/4367], Loss: 0.0933
2025-02-16 13:06:44,712 - Epoch [120/1000], Step [3300/4367], Loss: 0.0623
2025-02-16 13:07:19,970 - Epoch [120/1000], Step [3400/4367], Loss: 0.0531
2025-02-16 13:07:54,767 - Epoch [120/1000], Step [3500/4367], Loss: 0.1624
2025-02-16 13:08:29,443 - Epoch [120/1000], Step [3600/4367], Loss: 0.0285
2025-02-16 13:09:04,506 - Epoch [120/1000], Step [3700/4367], Loss: 0.2886
2025-02-16 13:09:39,467 - Epoch [120/1000], Step [3800/4367], Loss: 0.2292
2025-02-16 13:10:14,572 - Epoch [120/1000], Step [3900/4367], Loss: 0.1970
2025-02-16 13:10:49,574 - Epoch [120/1000], Step [4000/4367], Loss: 0.1854
2025-02-16 13:11:24,336 - Epoch [120/1000], Step [4100/4367], Loss: 0.1507
2025-02-16 13:11:59,372 - Epoch [120/1000], Step [4200/4367], Loss: 0.1394
2025-02-16 13:12:34,336 - Epoch [120/1000], Step [4300/4367], Loss: 0.1010
2025-02-16 13:13:07,951 - Epoch [120/1000], Validation Step [100/1090], Val Loss: 0.0001
2025-02-16 13:13:17,263 - Epoch [120/1000], Validation Step [200/1090], Val Loss: 0.0001
2025-02-16 13:13:26,717 - Epoch [120/1000], Validation Step [300/1090], Val Loss: 0.2487
2025-02-16 13:13:36,480 - Epoch [120/1000], Validation Step [400/1090], Val Loss: 0.0504
2025-02-16 13:13:45,691 - Epoch [120/1000], Validation Step [500/1090], Val Loss: 0.4156
2025-02-16 13:13:55,313 - Epoch [120/1000], Validation Step [600/1090], Val Loss: 0.1477
2025-02-16 13:14:04,940 - Epoch [120/1000], Validation Step [700/1090], Val Loss: 0.1343
2025-02-16 13:14:13,816 - Epoch [120/1000], Validation Step [800/1090], Val Loss: 0.0035
2025-02-16 13:14:22,479 - Epoch [120/1000], Validation Step [900/1090], Val Loss: 0.0030
2025-02-16 13:14:31,844 - Epoch [120/1000], Validation Step [1000/1090], Val Loss: 0.0000
2025-02-16 13:14:40,562 - Epoch 120/1000, Train Loss: 0.1312, Val Loss: 0.1392, Accuracy: 94.92%
2025-02-16 13:14:41,072 - Model saved at /data2/personal/sungjin/korean_dialects/checkpoint_real/checkpoint_epoch_120.pth
2025-02-16 13:15:17,129 - Epoch [121/1000], Step [100/4367], Loss: 0.1904
2025-02-16 13:15:52,494 - Epoch [121/1000], Step [200/4367], Loss: 0.4188
2025-02-16 13:16:27,050 - Epoch [121/1000], Step [300/4367], Loss: 0.0979
2025-02-16 13:17:02,567 - Epoch [121/1000], Step [400/4367], Loss: 0.3145
2025-02-16 13:17:37,198 - Epoch [121/1000], Step [500/4367], Loss: 0.1532
2025-02-16 13:18:12,377 - Epoch [121/1000], Step [600/4367], Loss: 0.1644
2025-02-16 13:18:47,320 - Epoch [121/1000], Step [700/4367], Loss: 0.3031
2025-02-16 13:19:22,650 - Epoch [121/1000], Step [800/4367], Loss: 0.0516
2025-02-16 13:19:57,786 - Epoch [121/1000], Step [900/4367], Loss: 0.1492
2025-02-16 13:20:32,709 - Epoch [121/1000], Step [1000/4367], Loss: 0.1572
2025-02-16 13:21:07,875 - Epoch [121/1000], Step [1100/4367], Loss: 0.1387
2025-02-16 13:21:43,156 - Epoch [121/1000], Step [1200/4367], Loss: 0.1233
2025-02-16 13:22:18,100 - Epoch [121/1000], Step [1300/4367], Loss: 0.0759
2025-02-16 13:22:53,238 - Epoch [121/1000], Step [1400/4367], Loss: 0.2121
2025-02-16 13:23:28,276 - Epoch [121/1000], Step [1500/4367], Loss: 0.2968
2025-02-16 13:24:03,093 - Epoch [121/1000], Step [1600/4367], Loss: 0.1075
2025-02-16 13:24:37,661 - Epoch [121/1000], Step [1700/4367], Loss: 0.0308
2025-02-16 13:25:12,263 - Epoch [121/1000], Step [1800/4367], Loss: 0.2326
2025-02-16 13:25:47,037 - Epoch [121/1000], Step [1900/4367], Loss: 0.0640
2025-02-16 13:26:21,779 - Epoch [121/1000], Step [2000/4367], Loss: 0.0204
2025-02-16 13:26:56,635 - Epoch [121/1000], Step [2100/4367], Loss: 0.1052
2025-02-16 13:27:31,620 - Epoch [121/1000], Step [2200/4367], Loss: 0.1082
2025-02-16 13:28:05,894 - Epoch [121/1000], Step [2300/4367], Loss: 0.0647
2025-02-16 13:28:41,276 - Epoch [121/1000], Step [2400/4367], Loss: 0.0980
2025-02-16 13:29:16,433 - Epoch [121/1000], Step [2500/4367], Loss: 0.2059
2025-02-16 13:29:51,567 - Epoch [121/1000], Step [2600/4367], Loss: 0.2093
2025-02-16 13:30:26,580 - Epoch [121/1000], Step [2700/4367], Loss: 0.1132
2025-02-16 13:31:01,834 - Epoch [121/1000], Step [2800/4367], Loss: 0.1576
2025-02-16 13:31:37,015 - Epoch [121/1000], Step [2900/4367], Loss: 0.3456
2025-02-16 13:32:11,923 - Epoch [121/1000], Step [3000/4367], Loss: 0.1337
2025-02-16 13:32:46,591 - Epoch [121/1000], Step [3100/4367], Loss: 0.0159
2025-02-16 13:33:21,235 - Epoch [121/1000], Step [3200/4367], Loss: 0.1042
2025-02-16 13:33:56,295 - Epoch [121/1000], Step [3300/4367], Loss: 0.2843
2025-02-16 13:34:31,022 - Epoch [121/1000], Step [3400/4367], Loss: 0.1447
2025-02-16 13:35:05,874 - Epoch [121/1000], Step [3500/4367], Loss: 0.1425
2025-02-16 13:35:40,650 - Epoch [121/1000], Step [3600/4367], Loss: 0.0734
2025-02-16 13:36:15,560 - Epoch [121/1000], Step [3700/4367], Loss: 0.0525
2025-02-16 13:36:50,168 - Epoch [121/1000], Step [3800/4367], Loss: 0.1936
2025-02-16 13:37:25,264 - Epoch [121/1000], Step [3900/4367], Loss: 0.1479
2025-02-16 13:38:00,411 - Epoch [121/1000], Step [4000/4367], Loss: 0.1861
2025-02-16 13:38:35,340 - Epoch [121/1000], Step [4100/4367], Loss: 0.1060
2025-02-16 13:39:10,223 - Epoch [121/1000], Step [4200/4367], Loss: 0.0356
2025-02-16 13:39:45,578 - Epoch [121/1000], Step [4300/4367], Loss: 0.0751
2025-02-16 13:40:19,700 - Epoch [121/1000], Validation Step [100/1090], Val Loss: 0.0001
2025-02-16 13:40:29,018 - Epoch [121/1000], Validation Step [200/1090], Val Loss: 0.0004
2025-02-16 13:40:38,470 - Epoch [121/1000], Validation Step [300/1090], Val Loss: 0.2412
2025-02-16 13:40:48,259 - Epoch [121/1000], Validation Step [400/1090], Val Loss: 0.0545
2025-02-16 13:40:57,495 - Epoch [121/1000], Validation Step [500/1090], Val Loss: 0.4126
2025-02-16 13:41:07,152 - Epoch [121/1000], Validation Step [600/1090], Val Loss: 0.1605
2025-02-16 13:41:16,827 - Epoch [121/1000], Validation Step [700/1090], Val Loss: 0.1489
2025-02-16 13:41:25,732 - Epoch [121/1000], Validation Step [800/1090], Val Loss: 0.0040
2025-02-16 13:41:34,422 - Epoch [121/1000], Validation Step [900/1090], Val Loss: 0.0033
2025-02-16 13:41:43,777 - Epoch [121/1000], Validation Step [1000/1090], Val Loss: 0.0000
2025-02-16 13:41:52,515 - Epoch 121/1000, Train Loss: 0.1318, Val Loss: 0.1397, Accuracy: 94.89%
2025-02-16 13:42:29,252 - Epoch [122/1000], Step [100/4367], Loss: 0.0281
2025-02-16 13:43:04,823 - Epoch [122/1000], Step [200/4367], Loss: 0.1761
2025-02-16 13:43:40,016 - Epoch [122/1000], Step [300/4367], Loss: 0.2514
2025-02-16 13:44:14,979 - Epoch [122/1000], Step [400/4367], Loss: 0.0538
2025-02-16 13:44:50,008 - Epoch [122/1000], Step [500/4367], Loss: 0.1498
2025-02-16 13:45:25,048 - Epoch [122/1000], Step [600/4367], Loss: 0.1571
2025-02-16 13:45:59,711 - Epoch [122/1000], Step [700/4367], Loss: 0.1095
2025-02-16 13:46:34,621 - Epoch [122/1000], Step [800/4367], Loss: 0.1359
2025-02-16 13:47:09,788 - Epoch [122/1000], Step [900/4367], Loss: 0.1464
2025-02-16 13:47:44,855 - Epoch [122/1000], Step [1000/4367], Loss: 0.0935
2025-02-16 13:48:20,320 - Epoch [122/1000], Step [1100/4367], Loss: 0.0916
2025-02-16 13:48:54,821 - Epoch [122/1000], Step [1200/4367], Loss: 0.0461
2025-02-16 13:49:29,808 - Epoch [122/1000], Step [1300/4367], Loss: 0.0341
2025-02-16 13:50:04,959 - Epoch [122/1000], Step [1400/4367], Loss: 0.0403
2025-02-16 13:50:39,988 - Epoch [122/1000], Step [1500/4367], Loss: 0.1572
2025-02-16 13:51:15,178 - Epoch [122/1000], Step [1600/4367], Loss: 0.0225
2025-02-16 13:51:49,865 - Epoch [122/1000], Step [1700/4367], Loss: 0.1916
2025-02-16 13:52:24,271 - Epoch [122/1000], Step [1800/4367], Loss: 0.2588
2025-02-16 13:52:59,185 - Epoch [122/1000], Step [1900/4367], Loss: 0.0820
2025-02-16 13:53:33,880 - Epoch [122/1000], Step [2000/4367], Loss: 0.1216
2025-02-16 13:54:09,017 - Epoch [122/1000], Step [2100/4367], Loss: 0.2343
2025-02-16 13:54:43,762 - Epoch [122/1000], Step [2200/4367], Loss: 0.1264
2025-02-16 13:55:18,770 - Epoch [122/1000], Step [2300/4367], Loss: 0.2420
2025-02-16 13:55:53,892 - Epoch [122/1000], Step [2400/4367], Loss: 0.0782
2025-02-16 13:56:28,692 - Epoch [122/1000], Step [2500/4367], Loss: 0.2009
2025-02-16 13:57:03,458 - Epoch [122/1000], Step [2600/4367], Loss: 0.0971
2025-02-16 13:57:38,081 - Epoch [122/1000], Step [2700/4367], Loss: 0.1773
2025-02-16 13:58:12,907 - Epoch [122/1000], Step [2800/4367], Loss: 0.1565
2025-02-16 13:58:47,833 - Epoch [122/1000], Step [2900/4367], Loss: 0.0673
2025-02-16 13:59:22,873 - Epoch [122/1000], Step [3000/4367], Loss: 0.0997
2025-02-16 13:59:58,030 - Epoch [122/1000], Step [3100/4367], Loss: 0.0859
2025-02-16 14:00:32,389 - Epoch [122/1000], Step [3200/4367], Loss: 0.4556
2025-02-16 14:01:07,586 - Epoch [122/1000], Step [3300/4367], Loss: 0.0428
2025-02-16 14:01:42,069 - Epoch [122/1000], Step [3400/4367], Loss: 0.0837
2025-02-16 14:02:17,459 - Epoch [122/1000], Step [3500/4367], Loss: 0.0286
2025-02-16 14:02:52,707 - Epoch [122/1000], Step [3600/4367], Loss: 0.1041
2025-02-16 14:03:27,188 - Epoch [122/1000], Step [3700/4367], Loss: 0.1964
2025-02-16 14:04:02,273 - Epoch [122/1000], Step [3800/4367], Loss: 0.0778
2025-02-16 14:04:37,373 - Epoch [122/1000], Step [3900/4367], Loss: 0.1519
2025-02-16 14:05:12,339 - Epoch [122/1000], Step [4000/4367], Loss: 0.0818
2025-02-16 14:05:47,053 - Epoch [122/1000], Step [4100/4367], Loss: 0.0841
2025-02-16 14:06:22,096 - Epoch [122/1000], Step [4200/4367], Loss: 0.1891
2025-02-16 14:06:57,052 - Epoch [122/1000], Step [4300/4367], Loss: 0.1460
2025-02-16 14:07:30,784 - Epoch [122/1000], Validation Step [100/1090], Val Loss: 0.0000
2025-02-16 14:07:40,075 - Epoch [122/1000], Validation Step [200/1090], Val Loss: 0.0001
2025-02-16 14:07:49,494 - Epoch [122/1000], Validation Step [300/1090], Val Loss: 0.2989
2025-02-16 14:07:59,256 - Epoch [122/1000], Validation Step [400/1090], Val Loss: 0.0488
2025-02-16 14:08:08,454 - Epoch [122/1000], Validation Step [500/1090], Val Loss: 0.4195
2025-02-16 14:08:18,060 - Epoch [122/1000], Validation Step [600/1090], Val Loss: 0.0913
2025-02-16 14:08:27,706 - Epoch [122/1000], Validation Step [700/1090], Val Loss: 0.0856
2025-02-16 14:08:36,571 - Epoch [122/1000], Validation Step [800/1090], Val Loss: 0.0037
2025-02-16 14:08:45,206 - Epoch [122/1000], Validation Step [900/1090], Val Loss: 0.0030
2025-02-16 14:08:54,491 - Epoch [122/1000], Validation Step [1000/1090], Val Loss: 0.0003
2025-02-16 14:09:03,207 - Epoch 122/1000, Train Loss: 0.1322, Val Loss: 0.1432, Accuracy: 94.82%
2025-02-16 14:09:03,656 - Model saved at /data2/personal/sungjin/korean_dialects/checkpoint_real/checkpoint_epoch_122.pth
2025-02-16 14:09:39,889 - Epoch [123/1000], Step [100/4367], Loss: 0.1446
2025-02-16 14:10:14,801 - Epoch [123/1000], Step [200/4367], Loss: 0.2905
2025-02-16 14:10:49,446 - Epoch [123/1000], Step [300/4367], Loss: 0.3220
2025-02-16 14:11:24,687 - Epoch [123/1000], Step [400/4367], Loss: 0.1289
2025-02-16 14:11:59,747 - Epoch [123/1000], Step [500/4367], Loss: 0.1908
2025-02-16 14:12:34,526 - Epoch [123/1000], Step [600/4367], Loss: 0.0683
2025-02-16 14:13:09,882 - Epoch [123/1000], Step [700/4367], Loss: 0.2802
2025-02-16 14:13:44,859 - Epoch [123/1000], Step [800/4367], Loss: 0.1965
2025-02-16 14:14:19,993 - Epoch [123/1000], Step [900/4367], Loss: 0.1112
2025-02-16 14:14:55,207 - Epoch [123/1000], Step [1000/4367], Loss: 0.1529
2025-02-16 14:15:29,799 - Epoch [123/1000], Step [1100/4367], Loss: 0.2351
2025-02-16 14:16:04,814 - Epoch [123/1000], Step [1200/4367], Loss: 0.1162
2025-02-16 14:16:39,931 - Epoch [123/1000], Step [1300/4367], Loss: 0.2668
2025-02-16 14:17:14,844 - Epoch [123/1000], Step [1400/4367], Loss: 0.0399
2025-02-16 14:17:49,476 - Epoch [123/1000], Step [1500/4367], Loss: 0.0916
2025-02-16 14:18:23,911 - Epoch [123/1000], Step [1600/4367], Loss: 0.2907
