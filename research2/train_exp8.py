import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
import torch.optim as optim
from torch.utils.tensorboard import SummaryWriter
from datetime import datetime
from tqdm import tqdm
import os

exp = 'exp_8'
print("ğŸš€ TensorBoard ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„± ì¤‘...")
log_dir = os.path.join(f"/data2/personal/sungjin/research2/{exp}/runs", datetime.now().strftime("%Y-%m-%d_%H-%M-%S"))
writer = SummaryWriter(log_dir)
print(f"ğŸ“ ë¡œê·¸ ì €ì¥ ìœ„ì¹˜: {log_dir}")

# í•˜ì´í¼íŒŒë¼ë¯¸í„°
SEQ_LEN = 6
IMG_SIZE = 256
BATCH_SIZE = 8
EPOCHS = 40
LR = 1e-3
DEVICE = 'cuda:8'
print(f"ğŸ’» ì‹¤í–‰ ë””ë°”ì´ìŠ¤: {DEVICE}")

# ë°ì´í„° ë¡œë“œ
print("ğŸ“‚ NumPy ë°ì´í„° ë¡œë”© ì¤‘...")
X_train = np.load("/data2/personal/sungjin/research2/X_train_2.npy")  # (N, 12, 512, 512)
Y_train = np.load("/data2/personal/sungjin/research2/Y_train_2.npy")
X_test = np.load("/data2/personal/sungjin/research2/X_test_2.npy")
Y_test = np.load("/data2/personal/sungjin/research2/Y_test_2.npy")

print("âœ… NumPy ë°ì´í„° ë¡œë“œ ì™„ë£Œ!")

# ì°¨ì› ì¶”ê°€
print("ğŸ”„ ì±„ë„ ì°¨ì› ì¶”ê°€ ì¤‘...")
X_train = X_train[:, :, np.newaxis, :, :]
X_test = X_test[:, :, np.newaxis, :, :]
Y_train = Y_train[:, np.newaxis, :, :]
Y_test = Y_test[:, np.newaxis, :, :]

# Tensor ë³€í™˜
print("ğŸ§± TensorDataset ìƒì„± ì¤‘...")
train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32),
                              torch.tensor(Y_train, dtype=torch.float32))
test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32),
                             torch.tensor(Y_test, dtype=torch.float32))

print("ğŸ”„ DataLoader ìƒì„± ì¤‘...")
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)

# ëª¨ë¸ ì •ì˜
class ConvLSTMCell(nn.Module):
    def __init__(self, input_dim, hidden_dim, kernel_size):
        super().__init__()
        padding = kernel_size // 2
        self.hidden_dim = hidden_dim
        self.conv = nn.Conv2d(input_dim + hidden_dim, 4 * hidden_dim, kernel_size, padding=padding)

    def forward(self, x, h_prev, c_prev):
        combined = torch.cat([x, h_prev], dim=1)
        conv_out = self.conv(combined)
        cc_i, cc_f, cc_o, cc_g = torch.chunk(conv_out, 4, dim=1)
        i = torch.sigmoid(cc_i)
        f = torch.sigmoid(cc_f)
        o = torch.sigmoid(cc_o)
        g = torch.tanh(cc_g)
        c = f * c_prev + i * g
        h = o * torch.tanh(c)
        return h, c

class ConvLSTMNet(nn.Module):
    def __init__(self, input_dim=1, hidden_dim=32, kernel_size=3):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.cell = ConvLSTMCell(input_dim, hidden_dim, kernel_size)
        self.conv_out = nn.Conv2d(hidden_dim, 1, kernel_size=1)

    def forward(self, x):  # x: (B, T, C, H, W)
        B, T, C, H, W = x.size()
        h = torch.zeros(B, self.hidden_dim, H, W).to(x.device)
        c = torch.zeros(B, self.hidden_dim, H, W).to(x.device)

        for t in range(T):
            h, c = self.cell(x[:, t], h, c)

        return self.conv_out(h)

# í•™ìŠµ ì‹œì‘
print("ğŸ“¦ ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
model = ConvLSTMNet().to(DEVICE)

def dice_loss(pred, target, eps=1e-6):
    pred = torch.sigmoid(pred)  # Ensure sigmoid activation
    pred = pred.view(-1)
    target = target.view(-1)
    intersection = (pred * target).sum()
    union = pred.sum() + target.sum()
    dice = (2. * intersection + eps) / (union + eps)
    return 1 - dice

def bce_dice_loss(pred, target, alpha=0.1):
    bce = nn.functional.binary_cross_entropy_with_logits(pred, target)
    dice = dice_loss(pred, target)
    return alpha * bce + (1 - alpha) * dice

pos_weight_val = 1000
print(f"ğŸ“Š pos_weight: {pos_weight_val:.2f}")
pos_weight = torch.tensor([pos_weight_val], dtype=torch.float32).to(DEVICE)

# ì†ì‹¤ í•¨ìˆ˜ ì •ì˜
criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)

optimizer = optim.Adam(model.parameters(), lr=LR)

def accuracy_metric(pred, target, threshold=0.5):
    """
    target == 1ì¸ ë¶€ë¶„ì—ì„œ predë„ 1ë¡œ ë§ì¶˜ ë¹„ìœ¨ (Recall-like)
    """
    pred_bin = torch.sigmoid(pred) > threshold
    target_bin = target > 0.5

    # targetì´ 1ì¸ ìœ„ì¹˜ì—ì„œ predë„ 1ì¸ì§€ í™•ì¸
    true_positive = (pred_bin & target_bin).float().sum()
    total_positive = target_bin.float().sum()

    if total_positive == 0:
        return torch.tensor(1.0)  # targetì— 1ì´ ì—†ìœ¼ë©´ ì •í™•íˆ ì˜ˆì¸¡í–ˆë‹¤ê³  ê°„ì£¼

    return true_positive / total_positive

print("ğŸš€ í•™ìŠµ ì‹œì‘!")
for epoch in range(EPOCHS):
    print(f"\nğŸ“˜ Epoch {epoch+1}/{EPOCHS}")
    
    model.train()
    train_loss = 0.0
    train_acc = 0.0
    train_bar = tqdm(train_loader, desc="ğŸ› ï¸  Training", leave=False)
    for x_batch, y_batch in train_bar:
        x_batch, y_batch = x_batch.to(DEVICE), y_batch.to(DEVICE)
        optimizer.zero_grad()
        output = model(x_batch)
        loss = criterion(output, y_batch)
        loss.backward()
        optimizer.step()
        train_loss += loss.item()
        
        acc = accuracy_metric(output, y_batch)
        train_acc += acc.item()

        train_bar.set_postfix(loss=loss.item(), acc=acc.item())

    avg_train_loss = train_loss / len(train_loader)
    avg_train_acc = train_acc / len(train_loader)
    writer.add_scalar("Loss/Train", avg_train_loss, epoch)
    writer.add_scalar("Accuracy/Train", avg_train_acc, epoch)

    model.eval()
    val_loss = 0.0
    val_acc = 0.0
    val_bar = tqdm(test_loader, desc="ğŸ” Validation", leave=False)
    with torch.no_grad():
        for x_batch, y_batch in val_bar:
            x_batch, y_batch = x_batch.to(DEVICE), y_batch.to(DEVICE)
            output = model(x_batch)
            loss = criterion(output, y_batch)
            val_loss += loss.item()

            acc = accuracy_metric(output, y_batch)
            val_acc += acc.item()

            val_bar.set_postfix(loss=loss.item(), acc=acc.item())

    avg_val_loss = val_loss / len(test_loader)
    avg_val_acc = val_acc / len(test_loader)
    writer.add_scalar("Loss/Validation", avg_val_loss, epoch)
    writer.add_scalar("Accuracy/Validation", avg_val_acc, epoch)

    print(f"âœ… Epoch {epoch+1}/{EPOCHS} ì™„ë£Œ - "f"Train Loss: {avg_train_loss:.4f} | Train Acc: {avg_train_acc:.4f} | "f"Val Loss: {avg_val_loss:.4f} | Val Acc: {avg_val_acc:.4f}")


        # ì—í­ë³„ ëª¨ë¸ ì €ì¥
    torch.save(model.state_dict(), f"/data2/personal/sungjin/research2/{exp}/conv_lstm_epoch_{epoch+1:02d}.pt")

writer.close()
print("ğŸ“‰ TensorBoard ë¡œê·¸ ê¸°ë¡ ì™„ë£Œ.")
